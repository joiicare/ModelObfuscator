/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace wpjdeq {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

int8_t filter_r   aw[19200]={-58, 39, -11, 25, 11, -23, 4, -1, -18, -13, 38, -28, 50, 47, 81, -23, 1, 7, 4, 62, -59, -13, -33, 22, -13, 36, -10, -12, -8, -80, -47, -15, 34, -11, 23, -98, 4, -11, 1, -21, -56, -13, 7, 19, 8, -25, -42, -3, -20, 61, 4, 23, -2, -25, -5, 59, 29, 35, -11, 26, -16, -47, -82, -18, 24, 3, -46, 2, -7, 20, 11, 37, -63, -19, 2, -36, -36, 77, 9, 76, 7, 24, 7, 12, 12, 34, 49, -110, -5, -1, 9, 23, -10, 46, -54, -3, -16, -21, 13, 54, -4, 10, -20, -5, -12, 2, -27, 24, 6, -26, -26, -37, 81, 47, -13, 21, 0, 59, -37, -17, -37, 0, -14, 63, 8, 28, -4, -51, 4, -2, -99, 32, -1, -10, -29, 8, -2, -33, 7, 5, -120, 41, 3, 2, 7, 78, 3, -3, 4, 25, 46, 21, 19, 7, -47, -45, 127, 15, 66, 20, -38, -10, -3, 59, -26, -40, -6, 0, 61, 10, -4, 16, 19, -8, -5, -15, -5, 9, -12, 22, -2, 111, 27, 8, -27, 16, 3, 10, 7, 6, 57, -3, -26, -4, -20, 10, -40, 80, -41, 4, -6, 64, -58, -27, 7, -69, -22, 9, 0, -36, 32, 45, 1, 31, 8, -29, -31, -2, -14, 1, -45, 43, -1, -17, 11, 68, -13, 3, 19, -5, 107, 70, -65, 13, -16, -70, -47, -20, -70, 4, -55, 15, -33, 12, 31, -14, 22, 5, -13, -53, -17, 37, 3, 15, 46, -45, 54, 9, 0, 14, -22, 0, 77, -53, -6, -40, 14, -64, -1, -28, 30, 58, 20, -7, -14, -13, -23, -6, -30, -34, 35, 15, 10, -7, 48, -14, 9, 28, 69, 34, -34, -43, 93, 4, -26, 47, -51, 50, -49, 17, 41, 19, -17, -51, -71, 21, 15, -29, -20, -4, 16, -25, 21, 18, 58, -29, -18, 68, -26, -50, 14, 16, 8, -17, 7, -41, 34, 2, -2, 37, -16, -7, -48, 33, 27, 11, 45, 10, 9, 57, 43, -2, 5, -9, 21, 39, -31, 13, 19, -52, -36, 21, -56, -37, 20, 1, -21, 12, -67, 7, 21, 11, -20, 7, -28, 45, 5, 16, 38, -8, 63, 26, -10, 59, -51, -4, -2, 12, 13, -22, -61, -23, -62, -127, -41, 69, 20, -44, 54, -44, 57, 24, 45, 23, 43, 14, -2, -11, -36, -16, -36, -2, -1, 32, -21, -1, -35, -33, -58, 76, -2, -12, 6, 8, -6, 11, 11, -41, 8, -19, 18, 12, -34, 38, 51, 51, 48, 23, -44, 7, -46, 8, 25, 62, -35, 38, 21, 27, -10, 34, -8, -32, 24, 0, 41, -28, -39, 47, 5, -38, -20, -23, 41, 6, -5, 0, 50, -6, 11, 4, 111, -14, 37, -17, 24, 27, -19, -26, 88, -114, -19, -6, 9, -36, 28, -19, 37, 2, 0, 6, -26, 3, 5, -31, 34, 41, -38, 17, 18, -5, -31, 17, 27, -34, 55, 75, -15, 0, 0, 25, 16, -32, -2, -10, -12, 22, -1, -3, -51, -7, 32, 35, 15, 15, 22, 23, 73, -6, 18, 8, 24, -5, 16, 10, 59, 127, -5, -31, -42, -7, 39, -41, 10, 4, 15, -16, 9, -1, -22, -42, -1, -33, -17, -10, -60, 27, 28, 4, 25, -1, -28, -24, -47, 34, 19, 12, 24, 42, 10, 44, 36, 10, 1, 10, -40, 39, -2, 34, 10, 36, -49, 55, -28, -17, 20, -10, -75, 19, -26, -11, -3, -21, -3, 5, -70, 17, -26, -42, 0, 2, 16, 9, 58, -30, 24, 14, 45, 12, 16, 0, 0, -31, 46, -18, -44, 23, 18, -31, 24, 11, 4, 0, -26, 47, 38, -13, 42, -19, 22, -31, -23, 14, 67, -56, 6, 26, -45, -24, -10, -15, -24, -15, 16, -3, 32, -22, 24, -2, 17, -14, 53, -10, -18, 18, 42, 24, 30, 0, 58, 12, 5, 18, -32, 5, -36, -25, 24, -22, 30, 7, -26, -31, 3, 23, 17, 33, -19, 9, 39, 13, -6, 0, -29, 79, -18, 9, 13, 18, 1, 0, 8, -8, -35, -19, -44, -27, 16, 6, -17, 17, -20, 7, -32, 17, -18, 13, 62, -27, -25, -29, 6, 31, -54, -15, -64, -43, -18, -19, -20, -97, -1, 27, -13, -6, -3, -18, 3, -4, 19, -6, -21, -16, 9, 3, 3, -12, 10, 5, 1, -5, 8, -13, -1, 7, -22, 14, 4, 4, 14, 5, -12, 12, -14, -26, 12, -30, 4, 0, 18, 12, -14, -4, -24, 5, -1, 11, -9, 5, -8, 13, -1, 1, 1, 2, -3, 5, -37, 6, -9, -23, -22, 6, 8, -12, 1, 2, -2, -25, 8, -10, 33, 19, -68, 18, -28, 2, 30, 0, 2, 21, 7, 27, 4, -1, 5, 20, -14, -4, -4, 17, 10, 10, -11, 9, 10, 43, 9, -15, 24, 1, -4, -5, 57, -6, -1, -2, 10, -2, -10, -5, 4, 5, 5, -12, -11, 6, 21, 17, -2, 6, -19, 9, -1, -3, -16, -7, 8, -15, 2, 15, -17, 1, 12, 0, -18, -36, 2, 7, 16, -6, -2, -24, 5, -3, 11, 5, 13, -15, -9, 4, -127, 13, 3, -12, -28, 11, 4, 45, 1, -3, -9, 9, 3, -13, 0, 12, -19, -4, 4, -6, 21, 7, -13, -9, 9, -1, 7, 6, -6, -1, -9, 7, -7, 9, 1, 7, 1, -7, 4, -14, 5, -28, 9, -6, -2, -4, -9, 1, 8, 4, 5, 3, -1, 13, 3, -7, -10, -20, 25, 0, -4, 30, 21, 9, -19, 17, 6, 8, 1, -2, -2, -2, -9, -30, 2, -12, 6, 3, 5, -1, 1, -4, -13, 18, 5, 22, 3, -31, -2, 20, 6, -16, 1, 6, 16, 59, -16, 10, -4, -21, -56, -16, -40, 37, 45, 19, 3, 9, -54, -10, 104, -36, 63, 39, 34, 108, -34, -10, -44, -22, -5, -7, 21, 0, 0, -6, -10, 39, 12, -68, -5, 122, 84, 15, -127, -4, -20, -47, -16, 21, 67, -14, 23, 82, 37, -76, 32, -6, 3, 1, -33, -13, -104, -57, 5, 76, -29, 4, -59, 6, 57, 36, 86, -8, 38, -2, -59, 41, 4, 59, 18, 45, 81, 39, -34, 42, -8, 56, -66, 68, 43, -10, 23, -36, 32, 90, -18, 53, -51, 41, -48, -57, 56, -30, -55, 42, 35, -31, -35, -4, -24, -10, 43, 124, -21, -35, 20, 109, -79, -28, -58, 19, 0, 41, 32, 52, -4, -63, 9, 8, 43, 78, -74, 80, -79, -62, -6, -30, -23, -7, -52, -32, 0, 89, -24, -38, 75, 46, 1, -66, 18, -38, 60, 25, 14, -1, -16, -66, -23, 15, -28, -28, 5, -54, 50, -41, -27, 119, 106, -17, 30, -60, 19, 13, 0, -17, -23, 34, -24, -23, -101, -37, -27, 4, -55, 9, -5, 111, 13, -35, -24, -35, -15, 28, 82, 59, 54, 56, -51, 6, -73, 26, 48, -17, 5, -1, 23, -44, 96, 19, -15, 70, 77, -4, 28, -25, -39, 33, -34, 11, -11, -36, -27, -63, -5, -40, 13, -81, -1, 23, 26, -45, -82, -8, 53, 42, 27, 85, 23, -7, -83, -44, 66, 51, 8, 3, -29, 4, 10, -15, 56, 7, 1, 2, -6, 19, 28, -4, -5, -10, 13, 38, 32, 27, -33, -80, 9, 3, -10, 24, 29, -7, 3, -15, 7, 25, 20, 20, 16, 17, -15, -28, 24, -28, -4, 30, -23, -15, -21, 5, -21, 27, -10, 11, -32, -15, 12, 66, -35, 13, -3, 2, -112, -34, 17, 23, -8, 14, -12, 0, 22, -12, 8, 4, -19, 17, -25, 39, -9, -17, 17, -17, 1, -12, -13, 6, 4, -23, 20, 17, 7, -27, 10, 7, -2, -26, 46, -2, -29, 23, 24, 16, 21, 22, -18, 7, 10, -33, 64, 2, -5, -14, -5, -13, 10, 28, 13, 8, 36, -55, 8, -7, 17, -4, 50, 14, 17, 42, 28, 10, 12, -12, -36, -24, 35, 8, -16, -14, 30, -15, 23, -15, -6, 41, 13, 3, -88, 23, 17, 127, -30, 3, 18, 39, -43, -48, -13, -18, 0, -19, 68, -1, 15, -28, 16, -5, -83, -47, -4, 18, -1, -21, -43, 23, 9, 33, -33, 4, 42, 4, -47, -11, -67, 3, -13, 15, -19, 30, -2, -12, 5, 5, 75, 16, 34, 8, 7, -26, -11, 0, -3, -6, -1, -9, 18, 25, -9, 5, 6, -44, -18, -14, 37, 15, 15, 33, -23, -2, 28, 29, 25, -14, -25, 35, -2, 27, -17, -25, 6, -9, -2, -4, -54, 21, -7, 35, 13, -68, -26, -81, 6, -26, -106, 26, 7, -45, -13, -37, -9, 38, -64, 28, -9, 21, 31, 15, 31, -7, 10, -10, -5, -25, 24, -80, -84, -10, 6, 9, 30, 5, 69, -2, -17, 63, -16, 3, -30, -42, 37, 19, 17, -21, -17, -15, 29, 93, 12, -25, 56, -45, 40, -27, -2, -28, -1, 21, -1, 32, 56, 4, 34, 34, -19, 6, -5, 43, 13, -37, -56, -47, 5, -23, -32, 69, -29, -18, 59, 22, -15, 32, -9, -13, 18, -50, 5, 12, 20, -2, -9, 8, 1, 8, 12, -30, -25, 63, -19, 72, 25, 33, 23, 45, 11, 30, 14, 41, 10, -12, 51, -46, -16, -9, -2, 4, -25, 0, 5, 3, 2, 51, -62, 60, 54, -63, -17, -21, -43, 7, -34, 9, -19, -18, -13, -32, -73, 65, 3, 18, -88, 43, -38, -16, 15, 12, -4, 8, -21, 86, -12, 71, 12, -78, -1, 32, 84, 66, -11, -65, 30, -23, 39, 22, 4, 9, -13, -11, 18, -14, -44, -1, -39, 17, -127, 29, -43, 3, 14, 10, 24, -13, 10, -17, -18, -14, -3, -4, 3, 32, 15, 2, -12, -50, 37, 19, -8, -8, 75, -10, 43, -34, 31, 15, 33, -27, -14, 66, 23, -12, 18, 34, -1, -5, 54, 26, 43, -59, -3, 10, 3, -10, 1, 53, 8, 7, 5, -53, 11, 11, -16, 16, -34, -12, 25, 36, -23, 74, 19, 44, -12, -57, -51, -44, 1, 13, 8, 6, -12, 23, -34, -79, 8, -5, -39, 16, 26, -5, 34, 20, -20, 10, -1, 7, 3, -31, 35, -41, -9, 9, -5, -15, -47, 17, 19, -48, 6, 44, -6, -19, 34, 127, -10, 17, -32, -20, -27, -35, 25, -3, 14, -26, 10, -1, 10, -24, -27, -21, -29, -12, 50, -4, 7, -29, -10, -8, -11, 57, -41, 28, -8, 2, -2, -18, -63, 12, 20, 4, -55, -12, -13, 9, -12, 20, -16, 19, -7, -25, 5, 36, 9, -69, -46, 5, -4, 10, 45, 1, 15, 17, -1, 4, 10, 2, 8, -12, -2, 1, -33, 23, -13, -28, -4, 6, -14, 17, 20, -9, 76, 4, -6, -80, -7, -8, 108, 0, -37, 7, 3, 1, -7, 5, 18, -8, -14, 57, -9, 26, 1, -46, 15, 0, -31, 4, -6, 3, 24, 17, -12, -1, 1, 31, 96, 33, -20, -30, 12, -18, 4, 39, 30, 33, 12, -75, 7, 3, 71, -24, 71, -1, -86, -16, -14, 26, 36, -33, -5, -49, -7, -2, 41, -10, -4, 15, -16, 5, 2, -7, 5, 8, -8, -27, 8, 20, -17, 14, 31, 30, -10, -25, -15, -11, 42, 3, -3, -12, -2, -12, 7, -48, 14, 7, -7, -58, 35, 10, 27, 11, -4, 0, -10, -15, 28, -98, 22, -17, 23, -21, -7, 3, 9, -5, 5, -11, 17, 47, 38, -49, -4, -83, -17, -6, 8, 105, 10, -3, -10, 99, -120, 26, 25, -37, 19, -34, 61, -39, -33, 108, 120, 21, 48, 10, -40, -49, -15, 7, 8, 43, 19, 2, -23, -30, 24, -13, -32, -51, -2, 114, 14, 10, -22, 21, 40, 67, 25, -58, 39, 6, -45, 56, 24, -2, 10, 0, -35, 16, -14, 39, -12, -33, -25, -1, -29, -34, -84, -37, 13, 12, 14, -61, 18, -56, -26, 6, -46, -85, 48, 13, -124, 31, 4, -28, -17, -123, 44, -45, 13, 54, 4, 15, 50, -13, 34, -7, -64, -44, 40, 32, 43, -7, 62, 0, 5, 14, 54, -3, -44, 46, 61, 73, -63, -93, 50, 30, -52, 51, 38, 59, 7, -68, -11, -15, -12, 34, -103, -58, 87, 25, 64, -10, 51, 4, -16, 9, 89, -52, 25, 58, 30, -81, -29, -37, 10, 14, -17, -13, 26, 13, 49, -43, 14, -76, 22, 65, -29, 59, -18, 0, 12, -32, 30, -121, 101, -107, -31, -30, 19, 38, -17, 39, 94, 6, 26, 44, 21, -37, -66, 58, 10, -25, -57, -21, -36, 12, 89, -5, -39, -22, -28, -15, -6, 15, -24, -45, -25, 40, 1, 14, 8, 52, 28, 35, -57, -36, -78, 16, -55, 25, 16, 61, 10, -8, 32, -34, 4, 13, 38, -14, -7, 46, 47, 75, -24, 28, 58, 17, -4, 20, 40, 52, 5, 14, -127, -38, -37, 32, 51, 80, 7, -67, -11, -96, 41, 27, 41, -52, 18, 36, -47, -61, -7, 103, -8, 9, 16, 26, 19, 7, -10, 30, -5, 17, 33, 83, 94, -31, -32, -4, 9, 18, 16, -23, 12, -29, 17, 19, -31, 6, 10, -22, 23, -8, 33, -5, -30, 7, -26, 54, -13, -10, 46, -10, -45, -16, 5, -18, 12, -10, 34, -108, 2, -25, 9, 12, -56, 33, -12, 69, -55, 22, 11, -3, 24, 9, 2, -52, -67, 4, -11, -26, 2, -56, -6, -2, -3, 17, 26, -20, -45, 30, -4, -26, -11, 47, 51, 14, -20, -23, -1, -33, -19, -79, -13, -2, -22, -33, 1, -8, 61, -36, -102, -14, 16, -6, -3, -19, -4, 1, 6, -6, -39, 15, 0, 6, 94, -3, -16, 3, -4, -8, -20, 18, 7, 5, 78, 44, 30, 30, -2, -7, 26, 20, 11, -1, 1, -38, 22, -18, 85, 41, -22, -60, -35, -21, 8, 127, -4, -41, 14, -65, 16, 41, 39, 70, -5, -15, -12, 20, -69, -41, -19, 15, -33, -29, 44, 39, -11, 10, 24, 28, -10, -66, -17, 22, 38, -40, -7, 25, 14, -74, 3, 28, -2, 72, -97, 17, -42, -15, 7, 3, -30, 15, 2, -5, 26, 26, -2, 52, 1, 36, -17, 13, 5, 10, 26, -7, -23, -24, 32, -58, -8, -31, -12, -50, 10, -40, 2, 6, -11, -16, 37, 51, 49, 19, 66, 2, -24, -48, -9, -26, -73, 2, 8, 19, 88, 36, 18, -8, 18, -1, -14, -7, -1, -31, -22, 19, -29, 3, 21, -30, -6, 20, -1, -44, -47, -8, -31, -51, 28, 10, -35, 19, -36, -25, 62, 56, -31, -17, 26, -46, -61, -28, -19, 18, 23, 19, -11, 66, -44, -6, 9, -21, -4, -7, -21, -17, -90, 8, -68, -27, -58, 65, 41, 53, -50, -40, -19, -21, 42, -5, 28, -40, 17, -48, 17, -29, -51, 4, -22, -9, 3, 11, -16, 22, 74, -45, 96, 30, -46, 59, 2, -31, -18, 48, 59, 52, 37, -71, -31, -36, -68, -97, -57, 9, 42, 10, 6, -41, 32, 40, 20, -51, -33, -10, -20, 20, -12, 17, 32, -49, 17, 47, -40, 11, 77, 55, -16, 36, -4, 15, -20, -10, 25, -54, 47, -54, 60, -17, -27, -37, 36, 60, 79, 96, 47, -14, -21, 0, -31, -117, -16, -13, 9, 31, 35, -104, 101, 3, 33, 52, 35, 1, 73, 99, 13, -17, -11, 23, -24, -10, 16, 19, 73, -38, 68, -53, 0, -39, 27, 3, -32, 11, -49, -11, -47, -2, -19, 40, 27, 0, -25, 28, -38, -61, 51, 57, 67, 77, -48, 6, 18, 6, -16, 34, -108, 46, 35, -5, -2, 55, 23, 6, -32, 8, -36, 11, 32, -23, -1, 17, -5, -38, 8, 18, 16, 8, -4, 43, 25, -4, 1, -25, 7, 26, -31, 46, 42, 30, -32, 127, -15, 125, 18, 33, -83, 12, 1, 92, -10, 9, -20, -95, -22, -28, -59, -51, -51, 15, 15, -15, 24, -5, 34, -2, 10, 18, 8, -13, -21, -10, 11, -5, 29, 8, 64, -8, -1, -127, 5, 18, -56, 13, -60, 26, 24, 31, 45, -17, -28, 42, -24, -21, 46, 54, 17, 23, -5, -27, -13, 61, -50, 27, -3, -17, -5, 47, 27, -13, 12, 55, -10, 5, 20, 11, 10, -18, 37, -38, -17, 63, 11, 26, 31, 14, -10, -25, 0, -52, 47, -96, -9, -31, -23, 9, -19, -4, -40, -20, -5, -17, -2, -79, -3, -36, 20, -29, 19, -3, -18, 35, -22, -1, 6, 7, -5, 39, -34, -3, -32, -14, -50, -10, -6, 27, 45, -14, -5, 22, -8, 1, 12, -9, 25, 36, -34, -8, -38, 22, 10, -2, -9, 25, 58, 19, -35, 48, 16, 0, 41, -44, -7, 19, -23, 25, -27, 34, -1, 28, 0, -7, 14, -6, -40, -25, 3, 38, 20, -24, -5, 5, -6, 10, -37, 38, -27, 5, -3, -24, -3, -36, 32, -5, -2, 14, -6, 21, 15, -24, 17, -4, -17, -38, -21, -5, -76, -5, 15, 31, 2, -19, -4, 18, -12, -12, -20, 11, 5, -27, 28, -5, -36, -46, 20, -2, -39, -57, -49, -1, -15, 17, 70, -2, 11, 39, -30, -9, -20, 2, 28, 51, -26, 3, 118, -16, 94, -28, -55, 64, -11, 8, -49, -8, -2, -8, 51, -34, -28, 12, -46, 22, -4, -30, 53, 3, 17, 2, -7, 13, -27, -15, -7, -4, -15, -4, 5, 11, -41, 17, -7, 4, -12, -21, 8, -6, -9, -3, -10, -1, 4, 15, 0, 35, -71, -41, -6, -8, -32, 43, 25, -11, -17, 19, 0, -1, -29, 3, 27, -11, -2, -39, 14, -11, 8, -27, -44, 0, -10, -6, 6, 15, -19, 14, -19, -48, 2, -2, 3, -32, 40, 64, -11, -17, 20, 14, -25, -18, 12, -7, 3, 127, 2, 3, -14, -11, -7, 16, -10, -26, -7, -20, -15, 28, -4, -10, 7, -25, 17, -5, -2, -14, 1, 1, -5, -4, -10, 27, 1, -4, -12, -1, 1, -6, -3, -23, -3, 6, -1, -4, -28, 43, 18, 10, -16, 9, -18, -4, 23, 13, -4, -12, -18, -17, 82, 34, 8, 30, 8, -5, -6, -3, 15, -35, -3, 8, 40, 17, 1, 19, 2, 7, -6, 18, 10, 15, -3, 14, 68, 12, 56, -15, -27, -16, -21, -8, 2, 46, -6, 9, 7, -10, -27, 59, 35, -8, 27, 1, 8, 11, 0, 1, 1, -15, -10, 16, 56, 4, -18, -60, -4, 21, 26, -12, -27, -1, -11, 8, 9, 15, -20, -12, 5, 1, -10, -30, -5, 18, -26, 11, -13, 9, 5, 23, -5, 1, -14, -3, 2, 15, -16, 7, 4, 0, 0, -14, 88, 39, -4, 52, 18, -7, 4, 12, 34, -38, -35, -4, 4, 42, 33, -33, 7, 22, 102, 40, -15, -4, -34, -33, -46, -51, -26, 20, -38, 4, -29, -18, -30, 4, -34, -34, -23, -43, -14, 7, 54, 23, 25, 20, -31, 13, 40, 82, 7, 39, 54, -40, 15, 39, -42, -25, -50, 16, 14, 51, -66, -30, -19, 51, 56, 21, -44, 107, -47, 30, 24, 17, -22, -44, -44, -37, 20, -17, 94, 18, 15, 60, -47, 37, 10, -26, -33, -84, -78, -66, 20, -40, 55, -28, 6, 48, -15, 47, 2, 71, 98, 1, -11, -3, 13, 14, -127, 39, -39, 102, 70, -25, -38, -24, 7, 90, 18, -79, -65, 50, -29, -23, -3, -40, -5, 19, -17, -7, 38, -61, 11, 63, -14, -17, 18, 38, 8, 5, 71, 48, -18, -33, -35, -59, 6, 33, -79, -61, 26, -36, 7, 55, 51, 17, 106, -53, 10, 64, -18, 59, -33, -21, 7, 70, -13, 8, -50, 30, -18, -4, -12, -20, 0, 26, -105, -4, -9, -2, 67, -22, -82, 75, 22, -46, -81, 0, -1, -18, 4, -8, 6, 13, -50, -50, -7, -48, 2, -60, -11, 26, -106, 82, -22, 65, 72, -25, -2, -19, -35, 12, -72, -53, 9, -2, -42, -17, -46, 17, 18, -17, 7, 4, -5, -30, -37, 22, -34, 23, 13, 57, -2, 16, 37, 119, 22, -55, -37, 18, -12, -3, 60, 29, 3, 40, -11, -95, 33, 83, -28, 14, -35, 17, -2, -51, 21, -86, 19, -77, 3, 10, 23, 7, 25, 31, 55, -18, -18, -4, -30, 0, -44, 83, -14, -19, -13, 6, 41, 1, 17, 37, 7, 38, 35, -40, -9, -12, -66, -29, 8, -57, 32, 45, -56, 28, -21, -12, -13, 5, 13, -7, -3, -4, 35, -21, 21, -24, -21, -39, 18, -18, 53, 33, 6, -13, 49, 1, -15, 24, -26, -30, 22, 15, -29, 31, 43, -42, -28, -8, -15, 31, -44, 31, 9, 32, 1, -38, 20, 27, -42, 15, 15, -10, 63, -15, -27, 22, 7, 14, 31, -5, -7, 8, 5, 2, 1, -15, -16, 49, -13, -18, -3, -32, -8, -30, 21, 9, 9, -6, -46, -33, 33, 18, -40, 8, 19, 20, 0, -7, 7, -52, -62, -62, 15, 46, -1, -36, -29, -15, 6, -19, 4, 7, -20, 57, -74, -127, 18, -42, 17, 61, 23, 52, -50, 11, -6, 12, 6, -17, 9, -22, 22, 17, 45, -35, -43, 9, -12, -37, -16, -11, 11, -1, 17, -11, -16, 15, 6, -38, 21, -24, 18, 42, -17, 18, -23, -43, 11, -29, -7, -19, -24, 58, -59, -51, -2, 3, 26, -17, 6, 59, -123, 29, 0, 27, -47, -3, 4, 0, -5, 6, 16, -4, 50, -15, -47, 121, -17, 6, -16, 15, 9, -56, 23, -39, -15, -29, 10, 6, -4, -7, 32, -66, 61, 15, 12, 69, -29, 18, 13, -1, 58, 22, -5, -10, -80, 3, -12, 0, 6, -35, 77, -17, -30, -70, 42, 7, -20, -36, 52, -61, 28, 4, -22, -64, 20, -19, 54, -17, -3, -32, 27, 16, -20, -93, -5, 50, 3, -29, -72, 60, -69, -71, -20, 43, 127, -15, 28, -42, -24, -49, 30, -7, 45, 27, -38, -4, -25, 0, -46, 26, 40, -19, 36, 50, -58, 4, -18, -1, 48, -107, 15, -13, 8, 20, 84, 9, -20, -21, 18, -69, -1, -23, 75, -53, -13, -63, 10, -68, 40, 44, 4, -18, 14, -42, 36, -8, 38, -21, -70, -57, 8, 13, 57, -25, -31, -49, -26, 3, 43, -14, 24, 32, 21, -14, -13, 48, -18, -26, -18, 5, -24, 28, 19, -26, 40, -17, 30, -16, 39, 57, -80, 0, 6, -20, -37, 18, 24, 21, -38, -3, -31, -39, -6, -33, 56, 28, -26, -26, 10, -39, -67, -46, -28, 7, -14, 10, -8, -30, 0, -43, -32, -112, 1, 22, 3, -23, -63, -36, 33, -18, 1, 43, -66, 7, -59, 1, -13, 16, 0, -15, 5, -25, 54, -11, 8, 28, -90, -15, -7, -83, -44, -50, -5, 42, 24, -62, 48, -27, 35, -5, -14, 23, -24, -4, -1, 64, 66, -42, -26, -1, 23, -1, -7, -35, -40, 0, -56, -28, 29, -2, -23, 46, -15, 9, 38, -35, 21, -19, -56, 18, -15, 34, -4, -17, 20, -79, 66, -41, 0, -12, -69, 3, 7, 45, -26, 28, -54, -66, 9, -42, -31, -24, -1, -21, 64, -44, 37, -12, 50, 27, 22, -9, 77, -53, 11, 6, -8, -20, -36, 25, -19, 21, 27, 50, 10, -4, 50, 18, 29, 23, 47, 9, 53, -21, 40, -44, 25, -23, -16, 37, 26, 37, -8, -52, -11, -92, 6, -59, -5, 11, 75, -35, 78, 47, 5, 7, 22, -20, -20, 25, 19, 1, -52, 45, 28, -51, -12, -7, 17, -14, 10, -6, 21, -24, -1, 16, 38, -8, 39, -38, 12, -19, 31, -23, 2, -34, -8, -60, 25, -48, 38, 37, -20, -23, -25, 32, -90, 0, 40, -24, 2, 75, 22, -47, -29, 2, -68, -30, 5, 24, 17, -25, -12, -11, 12, 23, 25, 5, 39, 0, -31, 38, -17, 13, 16, -12, -11, 1, 44, -5, 17, -54, -7, 29, 21, 66, 28, -127, -9, -40, 41, 36, 20, 1, 13, 7, 3, 6, 8, 54, 0, 95, 23, 45, -64, 31, -62, 7, -2, 37, 11, 19, 9, 8, -57, -31, -1, -23, -18, -4, 27, 34, 77, -20, 12, 63, -4, -17, 1, -48, -24, 13, 12, 22, 61, 42, 15, -11, 5, 7, 34, 10, -13, -41, 8, -27, -21, -59, -5, 32, 2, -3, 16, 34, 10, -24, -26, -21, 24, 37, -14, -63, 3, -11, -33, -22, 7, -6, -120, -44, -2, -53, -25, 26, -7, 36, 20, -44, 26, 25, -51, -28, 17, -35, -7, 14, -4, -35, 20, -15, 21, 4, 9, -7, -23, -15, -37, 15, 10, -12, -12, -40, -24, -43, -24, -14, -26, 34, -31, 25, 38, 62, 22, 35, -22, -14, 32, -68, 81, 35, -41, 38, -20, 15, -14, -37, -1, -6, -113, -6, 35, 76, -15, 49, 52, -45, 50, 2, 19, -21, -38, -30, -34, 8, -20, -15, -14, -6, -26, 0, 60, -28, 44, -1, -7, 2, -20, -47, -1, -30, -40, 18, -2, 5, 11, -127, -44, 72, -9, 10, 41, -26, -2, -78, -50, -21, -2, 18, 12, 8, 18, -25, 21, -8, 5, 81, -109, -14, -21, -14, -3, -1, -22, -17, -100, 54, -1, 19, 23, 17, 8, -5, -9, 10, -37, -12, -7, -27, 44, 68, -30, 53, 98, 80, 45, 80, -30, 34, 41, 9, -10, 88, 11, -7, 49, -2, 34, 13, -114, 20, -100, -60, 60, -8, -16, 4, 6, 16, 3, -50, -29, 48, 52, 3, -77, -5, -26, -27, -29, -9, 36, -43, 63, -14, -1, -7, 0, -15, -12, 6, -23, 15, 4, -18, -21, 14, 7, -17, -9, -99, 54, -25, 64, 11, -31, -19, 64, -19, 22, 1, 19, 25, 1, 20, -50, 52, -40, 0, 40, 43, 46, 9, -5, -19, -9, 19, 15, 13, -21, 34, -31, -30, 22, 58, 12, -18, 3, 0, -69, 7, -10, 32, 7, 69, 7, 29, -12, -41, 29, 49, 44, -15, -35, 7, 7, 26, -76, -45, -25, 1, -22, -28, -8, 36, 16, 33, 12, -20, 25, 24, 39, -29, 74, -14, -14, 8, -44, -32, 127, -18, -5, 14, -20, -37, -6, 22, 4, 37, 44, -41, 1, -18, -50, -75, -25, 2, 2, 9, 81, 15, -22, 0, -6, -45, -75, 40, -44, -77, 44, -83, 55, 9, 31, -2, 1, 93, 4, 73, -71, -4, -32, 26, -23, -8, -113, -13, -2, 41, -31, -24, -109, 45, -24, -100, 2, -75, 25, 22, 2, -26, 4, -3, -19, 56, 37, -29, 4, -26, 23, -6, -31, -31, 32, 25, 44, 35, -53, -31, 11, 7, -27, -59, 13, -21, 7, -47, 46, 30, 18, -23, -23, 32, -42, 3, 30, -48, -1, 13, -8, 30, 17, -52, -69, -24, 51, 3, 3, 10, 68, 15, -19, 97, -51, -4, -5, 60, 15, 46, -31, -11, 28, 19, -6, 79, -16, -39, 8, 2, -2, 38, 23, -22, 59, -26, -53, -9, 4, 10, 18, -27, 73, 9, -22, -52, -43, 9, -79, 5, 28, -22, -1, -38, -11, -14, 11, -21, 1, 6, -1, 15, 27, 30, -30, -65, 8, -18, -27, -1, 37, 4, 53, -43, -14, -6, -55, -16, 0, 9, 68, 10, 31, -2, 29, -42, -16, 16, -35, -29, -68, -78, 22, -18, -30, -53, -48, -7, -4, -14, -7, 17, -27, 90, -18, 6, -7, 40, -18, -5, 25, 39, -35, 9, 6, -6, 94, -19, -17, -13, -9, -67, 56, -8, 56, -25, 14, -40, 15, -31, -70, -27, -15, -20, 62, 100, -19, -60, 52, -68, 42, -6, 42, 75, -9, -119, -59, -62, 19, 9, -34, -9, -26, 8, 62, -4, -25, -19, -57, -62, -60, 4, 32, 127, -2, -44, 12, 55, 53, -2, 3, 69, -27, -26, -26, 19, -17, -28, -8, 4, -83, 0, -84, -82, -28, -49, 26, 25, -6, -41, 6, -52, -26, -38, 98, 14, -65, -12, -19, -24, 40, 69, 47, -39, -46, 22, 24, -15, -35, -46, -14, -95, 67, 14, 3, 23, 14, -35, 68, -41, 37, -21, -42, 58, 91, -23, 105, -58, -38, -6, 64, -26, -16, 38, -25, -11, -12, -30, -14, -57, 14, -28, 43, -62, 45, 35, -12, -14, -74, 12, 12, 0, -30, -49, 14, -21, -51, -13, -48, 28, -13, 28, 78, -19, -43, -4, -67, 6, -31, 22, -7, 7, 42, 31, -83, 54, -51, 76, 7, -1, -13, 25, -1, -15, 40, 42, 41, -57, -40, 6, -47, -27, -60, 3, -38, -38, 6, 79, -19, 5, -18, -24, 74, 13, 67, 14, -106, -46, -14, -47, 9, -26, -21, 45, -6, -125, -13, 9, -59, 55, -13, -27, -16, -15, 58, -7, 29, -57, -47, 23, 11, -94, 0, -24, 31, 50, -62, 1, -6, -19, 88, -12, -8, -13, -18, -68, -8, -89, -10, 9, 5, 21, 61, -17, -41, -46, 65, 5, -64, 39, -65, 25, 6, 6, -76, -36, 22, -26, 25, -15, 10, 0, -19, 16, 42, 17, -9, 0, 37, -12, -3, 15, -66, 48, 21, -5, -10, -6, 35, -55, 47, -5, 13, -10, -1, 20, 11, 9, -10, -7, 34, 54, -5, -8, 8, 38, 26, -42, -51, -29, 12, -7, 14, -32, -20, -24, -21, -26, 57, -22, -13, -38, -7, -32, 127, 16, 4, -30, -3, 7, -13, 40, -30, -8, 20, 24, -17, 49, -9, 1, -15, 18, -6, -5, -19, 64, 14, 48, -20, 20, -11, -2, 1, -72, -23, 4, -10, -47, -14, 6, 3, -26, -7, -28, 3, 24, 45, 6, 26, 32, 41, 43, 3, 48, -19, 55, -14, 18, -31, 24, -44, -23, -7, -39, -44, 38, -71, 21, -5, 29, -63, -7, 8, -40, 5, 5, -2, 10, 1, -52, 35, -7, 28, -30, -26, 0, -10, 34, 39, 50, 30, 4, 3, 22, -48, 10, -5, 4, -1, 31, -3, 31, 51, -13, 58, 10, -47, -1, -59, -1, -24, 11, 32, 21, -9, 13, 3, -29, -3, -14, -8, 0, 16, 8, 0, 15, -3, -15, 29, 1, 21, -16, 20, 30, 14, 53, -5, 15, -15, -30, -71, -31, 60, -20, 5, 29, 21, -47, 58, -23, -39, -63, -10, -23, 8, -49, -71, 16, 19, 0, 7, 4, -22, 58, 2, 17, -23, -15, 16, 6, -58, -40, 16, 17, 0, -18, -25, 1, -35, -2, -52, 21, 24, 3, 11, 65, -32, -11, -25, -20, 4, 19, 29, 46, 16, -47, 27, 15, -6, 18, -43, 4, -22, 30, 18, -30, 19, 1, -69, -33, -23, -12, -37, 6, 10, -42, 47, -26, -64, 24, 34, 2, -10, -60, -48, -37, -27, 33, 9, -32, 23, -74, 46, 28, -55, 11, -15, 23, 7, 19, -21, -7, 44, 30, -29, 33, -2, 12, 17, -53, 28, -34, 67, 32, 12, 23, 5, -85, 4, -52, 28, -28, 29, -2, -4, -18, 41, 42, 8, -44, 64, -42, 7, 39, 48, -4, -38, 85, 19, 9, -2, -11, 13, -8, 49, 7, -84, -35, 14, 22, 53, 4, 47, -19, 44, 37, 11, 8, 1, 1, 13, 14, 13, 49, -44, -39, -33, 16, -33, 19, -17, -3, -10, 47, 8, 29, 7, -59, 74, 10, 10, 35, 16, -14, 71, -72, 46, 41, -31, -30, -16, 52, 65, -46, 23, 24, 93, -35, -47, -23, 29, -44, 0, 30, 19, 30, 2, -13, 20, 15, 4, 31, 68, 32, -40, 4, -19, -28, 26, -59, 11, 7, 63, 41, 37, -44, -17, 14, 127, 1, 75, 40, -77, -86, -18, -5, -27, 45, 57, 53, -43, -59, -17, 28, -20, 2, 0, 17, -21, 39, -34, 55, 17, -57, 43, -28, 19, -15, 30, -8, 7, 6, -23, -4, 26, 65, -6, -55, 9, -43, 9, 18, 28, -16, 3, 9, 13, -6, 7, 8, 9, 4, 27, -11, 27, 47, 60, 10, 24, 7, 82, -1, 2, -4, 5, 28, -12, -15, 14, 71, 45, 1, -23, -6, 12, -6, -6, 3, -28, 8, 20, 61, -21, 46, 12, -52, -33, 5, 8, 10, -30, -40, 2, -84, 11, -70, 20, 48, -93, 20, 10, 8, -3, -35, 6, -18, -20, 25, 27, 39, -62, 44, 37, -36, 28, 35, 35, 29, 8, -64, 14, 49, -69, 0, -16, -9, -48, 30, -22, -43, 0, 44, 36, 0, 12, -9, -2, -59, -32, 55, -23, -31, -48, -41, -2, 10, 96, 31, 38, 16, -25, 2, 0, 18, 64, 17, -71, -21, 17, -20, -58, 40, 60, 3, 33, -3, 42, 15, 26, 5, -3, 56, 50, -14, -30, -13, -7, 4, 21, 29, 49, 62, -23, -35, 18, 27, 37, -40, -13, 36, -16, -23, -20, -30, 15, -22, -8, -17, 25, -66, 26, 12, -6, -13, -10, -25, 3, -4, -67, -3, 2, -3, 0, 3, -22, 43, 5, 33, 8, 50, -14, 6, -17, -6, 2, -16, 3, 51, -1, -1, -7, -17, 19, 7, 50, -19, -9, 3, -9, 1, -47, 29, -24, 2, 2, -64, 19, -5, 23, 20, -5, -48, -59, 7, -6, -39, 32, 16, -19, 26, 9, 23, -23, 46, 14, 72, -80, 15, -3, 10, 0, -11, -58, 48, 24, -77, -2, -6, 24, -42, -19, -63, 1, 34, -2, -20, -127, 13, 86, -46, -4, 79, 0, 24, -2, -53, -84, -15, 20, 6, -64, -71, -27, 67, 61, -109, -13, 2, 74, 50, -127, -2, -3, -3, 7, -29, -30, 20, 27, 20, 46, 58, 17, 42, 13, 30, 50, -37, 76, 31, 11, -18, 22, 39, -21, -100, -44, 54, 40, -20, -46, -85, -36, -1, 4, -22, -18, 37, -58, 72, 15, 49, -8, 32, 15, 9, -107, 94, 0, 46, -21, 1, -67, 27, 81, 36, 54, 105, 57, 3, 90, -18, 48, -4, 0, 24, 54, 45, -24, -55, -22, -1, -42, -17, -7, 10, 120, 37, 36, 37, -1, 35, -9, 5, 100, 0, 68, 65, 42, -24, 1, -28, 35, 53, -62, -22, -2, -31, -20, 6, 2, 57, 28, 52, -7, 6, 88, 48, -17, -1, -19, 9, -34, -33, -55, 15, 42, 12, 12, 79, 54, -7, 34, 12, -4, 26, -41, 54, -43, 15, -31, 60, -53, -25, 71, -47, 51, -26, -44, -28, 33, -5, 14, -87, -1, -10, 61, -9, -34, 109, -10, -4, 11, -7, -31, -30, -49, -41, 60, 9, 15, -11, 0, -24, 63, 1, 26, 8, 36, 4, 33, -5, 13, 50, -40, -5, -51, -14, -9, -59, 68, -78, 50, -32, -63, -24, 7, 17, 75, 6, -47, 0, 9, -1, -25, -45, 7, -14, -12, -19, 109, -23, 16, -67, -30, -72, 0, 19, -19, -61, 11, -20, 33, -43, 22, -60, 73, -6, -47, -49, -21, -9, 10, 19, 54, 17, 35, -16, 43, 40, -40, 67, 5, -37, -58, 15, 21, 47, 13, 3, 16, -15, 14, -4, -21, 0, 23, -17, -4, 7, -7, 14, 34, -17, 2, 16, -11, 5, 33, 4, 10, -26, 12, -61, 15, -51, -38, 5, -18, -26, -18, 7, 41, -27, 60, 23, -1, -38, 29, 28, -27, -28, 63, -8, 62, 5, -18, -11, -8, 27, -40, -40, -33, -30, -24, 5, 64, -30, -22, 60, -7, -64, -54, 9, 23, -12, 0, 26, 73, 2, -79, -7, -42, 4, -72, 28, 47, 22, 5, 4, 35, -24, -13, 4, -2, 5, 9, -9, -7, 19, 27, -28, 59, 56, -21, 38, 27, -53, 68, -15, -19, 9, -1, -43, -17, -88, -6, 31, 62, -15, 8, -9, 1, -3, -39, -26, 10, -11, -6, -33, 30, 30, -67, -28, 5, -127, 4, 35, -10, -2, 3, 49, 59, -19, -22, -9, 0, -27, 40, 26, -27, 5, 38, 24, 35, -10, 32, -4, -18, 39, 0, -6, -17, -17, 36, 41, 25, 31, 42, 14, 47, 15, -53, 38, 25, 7, 98, 8, -18, 38, 51, -53, -12, 8, 45, 97, -35, -15, 27, 71, -4, 13, 35, -25, -6, 26, 39, -15, -62, 45, 18, -13, 29, 80, -2, -25, -49, 33, 0, -7, 55, -57, 31, 60, -10, 24, 17, 16, 45, 84, -33, 11, -49, 14, -8, -41, 1, 8, 2, 46, 54, -22, -73, 28, 5, -53, 27, -31, -52, -17, 26, -12, 19, -3, 9, 79, 4, -2, -107, -42, -76, -12, -86, 7, 0, 7, -7, -14, 0, -36, -28, -14, 73, -19, 0, 1, 47, -25, 11, -32, 53, 1, -9, 26, 24, -34, 18, 25, -19, 11, 47, 33, 51, 17, -6, 43, 8, 5, -14, 9, -117, 48, -28, -2, -19, -22, 33, -52, -26, 4, -23, 19, -30, -17, 52, -26, -59, 31, 42, 0, 72, -11, -13, -7, 20, -19, 23, -24, -21, -10, -14, -27, 2, -4, 11, -17, 72, -3, 42, 28, 39, -5, -9, -23, 44, 28, -74, 47, 30, 34, 46, 37, -2, 25, 31, 45, -31, 9, -38, 2, -65, -54, -58, 62, -8, -59, 20, 87, -2, 13, 19, 1, -11, 8, 27, -19, -7, -57, 5, 10, -37, 123, -14, 15, 39, -42, -9, 11, -28, 101, 9, -84, -24, -19, 13, 31, 21, -60, 7, -10, 11, -30, -94, 28, 44, 31, -5, 8, 12, -36, -8, 3, 19, 1, -1, -43, -41, -40, -7, -28, -46, -100, -54, 50, -33, 1, -53, -20, -97, -3, -57, -12, -22, 8, -11, -71, 34, -53, 43, 4, 11, -46, -25, -28, -1, 70, -33, 21, 9, -24, 23, -25, 21, 70, 17, 11, 32, 6, -39, 1, -19, -9, -6, 127, 11, 18, 1, 18, -55, -1, -37, -93, -19, 6, 14, -69, -1, -15, -41, 20, -43, -1, 53, -10, 1, 34, 27, 31, 15, 1, 95, 21, 11, 94, 53, -19, 14, -4, 25, -13, 33, 37, 108, -1, 37, 26, 11, 11, -3, 2, -19, -53, -66, 58, 28, -15, 6, -7, -20, -15, -27, 14, 30, 24, -27, -63, 56, -41, 10, 22, -68, -8, -68, -30, 11, 4, -14, 69, 16, 62, 39, -5, -10, -37, -61, -69, -15, -42, 2, -53, -39, -8, -25, 17, 24, -42, -27, -31, 41, 22, -23, 11, 1, -7, 0, 57, -54, -1, 5, -13, -14, 17, 13, 6, 3, 24, -16, -1, 127, 65, 26, 1, 32, -83, 39, 33, 19, 18, 100, 32, 17, 53, -17, -17, -2, -13, -29, 27, 24, -16, 100, -95, 57, 55, -50, -16, -4, 28, 0, 44, -19, -57, -9, 35, -14, -18, -51, 33, 21, -91, 38, -21, -18, -13, 28, 1, 13, -34, -10, 19, -22, -40, -23, -2, -24, -4, -11, -25, 44, 12, 10, 1, -17, -14, 42, 38, 14, -6, 7, 79, -70, 8, 73, 22, -6, -17, -18, -67, -24, -1, -32, -18, 100, -41, 59, 103, 23, -21, 30, 4, 4, -9, -1, 50, -24, -54, -34, 17, 24, 21, 4, 7, 47, 25, 17, -42, 28, -9, -8, 30, -30, -33, 5, 1, -12, 14, 45, 35, -1, -13, -54, -29, 22, 53, -8, -10, 16, 36, -14, -3, -18, 2, -47, -10, -24, 24, -33, 46, 11, 5, -22, -50, 74, 10, 16, -6, 6, -28, 5, 35, -71, -81, 34, 67, -32, -20, -14, -10, 40, 70, -30, -1, -26, -47, -2, 14, -22, -26, 38, 7, -38, 18, -6, 19, 26, -61, -127, -19, -4, -10, -27, 20, -6, -11, -21, -36, 0, -68, 12, -18, -27, -33, 52, 21, -19, 29, 5, 28, 9, 11, -35, -30, 18, -26, 127, -25, 22, -27, -21, -13, -88, 8, -38, -34, -109, -80, -23, -4, -3, -26, -2, -33, -40, 8, -26, 30, -48, 21, 24, -57, -28, -41, -32, -11, 22, -19, -121, -38, -79, 8, 6, 19, -15, 13, 49, 29, -2, -39, -71, 4, -24, 6, 20, -14, 23, -3, -5, -18, 5, 9, -21, -12, -29, -39, -26, 5, 14, 0, 8, -24, 84, 23, 39, 18, 29, 5, 119, 11, -20, -6, 51, -8, -105, -46, -32, 19, -26, 20, 4, -40, 76, 4, 1, -15, -16, 9, 50, 12, 62, 8, 24, 7, 9, 65, 4, 25, -5, -2, 6, -25, 5, -3, -30, 1, -48, 10, -48, -22, 5, 8, -10, -89, -73, -15, 45, 4, -37, -7, 22, -37, 22, 0, 29, -65, 10, 38, -5, 51, 46, -12, 2, 30, -42, 36, -16, 35, 1, -12, 17, 12, -47, 5, -32, 13, -19, -17, -21, -16, -7, -27, -30, -35, 25, 22, -31, 79, -24, -60, -22, 2, -60, -40, -55, -10, -17, -32, 20, 5, 50, 43, -44, 6, 10, -21, -50, -4, 10, -18, 27, 12, -9, -22, 50, -38, 10, -58, -35, -17, -2, -20, 82, -7, -55, -5, 0, -2, 9, 59, -18, 33, -10, -63, 40, -61, -9, -9, -7, -10, 81, 25, 14, -27, 11, 55, 40, -17, 17, -9, 22, 47, 20, 32, 19, -32, 16, 41, -17, -42, -5, -18, 2, 54, 1, 17, -23, 10, 79, -42, -10, 0, -35, -32, 30, -17, -21, 19, -7, -29, -28, 15, 27, 25, 13, 13, 5, -30, -16, -9, 18, -127, -19, -74, -103, -42, -38, 44, -34, -33, 16, 15, -27, 2, 27, -31, 26, -57, -4, 0, 19, 23, 10, -43, 61, 18, -13, 0, 11, -28, 19, -3, -53, -52, -45, -39, -5, -19, -26, 6, 10, -42, -1, 55, -19, -19, 23, 31, -89, -2, -43, -40, -14, 35, -23, -12, -18, -45, 8, -24, -27, 18, -16, 29, -24, 14, -17, -20, 75, 38, 75, -35, 17, 9, 27, -58, 11, -9, -23, 37, 1, 27, -14, 20, -45, -11, -44, -5, 54, 29, 21, -31, 55, 84, 126, 69, 17, 27, 19, 116, -60, 23, 17, -11, 35, 113, -13, -34, -1, 48, 23, -13, -5, 59, 13, -10, 24, -44, 4, -4, 9, 35, 15, -42, -25, -5, -32, -20, -13, -22, 8, -5, 94, 28, -25, 29, 19, 32, 24, -32, 35, 20, -58, -36, -23, 16, 1, 18, 18, -12, 7, -49, 19, 23, 1, -34, 49, -1, 43, 3, 5, 10, -17, 17, -21, -16, -56, -46, -15, -7, -36, 36, 30, 1, -20, -6, -10, -6, 14, -35, -21, -24, 113, -6, 37, 2, -3, 21, 127, 20, 3, -5, -18, -23, -8, 5, -72, 12, 28, -20, -77, 27, -17, -9, -34, -34, 11, -25, -30, 34, -23, -15, 47, -16, -9, 20, 59, -6, 19, -4, -14, 2, 8, -15, 15, 32, -58, -58, -21, 66, 4, 18, 25, 3, -25, 23, 21, -13, -20, 41, 4, -71, 31, 20, -34, -17, -10, 2, -12, -13, 51, 5, -11, -16, -6, 19, -46, -9, -13, -54, -10, -19, 14, 2, -21, 34, 0, 1, -6, 9, -14, 26, 2, -21, -30, -90, -10, -13, -19, 18, 8, -8, 14, -39, 4, 14, 31, -15, 33, -57, 6, -42, -1, 18, 38, 19, -15, 36, 9, 54, -13, -41, -39, -32, -2, 8, -5, 16, -51, 6, 22, -44, 5, 13, 12, -39, 27, -23, 15, 9, -8, 24, -35, -12, 29, 4, 32, -19, -21, -5, 16, -18, -22, 22, 45, 23, 11, 33, -7, 4, -1, -1, 10, -8, -29, -48, -4, 20, -2, 33, -34, 39, -5, 26, 11, -15, 35, 31, -6, -2, -10, -7, -6, -1, -3, -40, 11, -36, -29, 28, -7, -11, 78, 4, -52, 17, 24, -44, 11, -2, 19, 0, -9, -4, -45, -9, 26, -7, -42, -3, -4, 17, 53, 26, 17, -11, 16, 24, -34, 7, 24, 4, 79, 1, -9, 3, -21, -22, 16, 7, -7, -42, -56, -2, -71, 36, 11, -86, 8, -6, 47, -44, -58, -38, -27, 100, 17, 70, -3, 31, 38, -12, 21, -40, 16, 50, -21, 25, 11, -10, 10, 41, 24, -86, 14, 5, 17, -1, 6, 2, 45, -25, 33, 17, 25, 12, -8, 77, -42, -5, 15, 12, -3, -8, 31, -62, -46, 30, 42, 8, -40, -89, 20, 24, 27, 20, -19, 38, 8, 0, -47, 19, -1, -2, 12, -91, -49, 6, -103, 2, 27, 12, 11, 13, 4, 35, -34, 16, 7, 34, -55, 89, 6, 4, 22, 39, -58, -11, 48, 22, 9, 18, -29, -50, 54, 119, -10, -6, 112, 45, 26, 55, -20, 13, 23, -2, -66, -33, -51, -25, 12, -15, -20, 63, -14, -15, -5, 6, 13, -26, 5, 22, 17, 26, -40, 7, -28, -57, 28, -7, -42, 12, -30, 42, -92, 25, 12, -6, -1, -17, -105, -12, 21, 5, -42, 6, 40, 17, 10, 59, 8, 27, -20, 19, -4, -31, 5, -20, 87, 32, 24, 15, -32, -46, -5, -46, -8, -35, -3, 33, -43, 13, 114, 14, 61, 21, -32, 5, 12, 18, -24, 9, -127, -67, 17, 41, -9, 25, -33, 4, 4, -13, 48, 24, -38, 53, -17, -92, -5, -6, 50, 20, -45, -101, -53, -29, 15, 44, 4, 22, -73, 71, 11, 6, 19, -14, -15, -1, -11, -27, 13, -6, 44, -13, -9, 38, 28, 111, -10, -12, 40, 22, 1, 8, -8, -27, -7, 94, -11, -2, -22, 59, 48, -21, -32, -9, -19, -9, -66, -17, 10, -9, 4, -42, 31, 15, -46, -25, -18, -4, -12, 64, 25, -15, 13, 4, 25, -10, 15, 18, 26, 31, -26, 67, -60, 15, -3, 88, -10, 0, 7, -13, 25, -66, -20, -7, 51, -11, -65, 31, -15, 39, -16, -20, 7, 1, 1, -11, -47, 30, 2, 20, -8, 32, 3, 33, -7, 31, 31, -12, 17, 15, -29, 20, 49, -10, -16, 11, 18, 31, 27, 17, 18, -19, 106, 4, -14, -32, -73, -35, 6, -17, 15, -21, -57, -5, 71, 9, -48, -1, 29, 2, 7, 58, -10, 78, -49, -29, 40, 6, 4, 43, 9, -57, 20, 20, -6, 36, -2, 37, -15, 16, -22, 49, -16, -14, 53, -65, 32, 21, 30, 21, 11, -7, 13, -40, 12, -26, -85, 2, 24, -19, -24, -39, -2, 9, 127, -20, -26, -36, 7, -12, 3, -15, 18, -43, -10, -49, -78, 79, 5, -3, -19, -8, -42, -59, 21, -23, -11, 43, 26, 35, -56, -13, 11, -6, 18, -25, -21, 3, -1, 40, 5, 18, -35, 34, -12, -2, -4, 14, -46, 10, -47, 16, 11, 3, -12, -50, 18, 18, -74, -9, -7, -19, 70, -21, -15, 61, 7, 36, 29, 7, 56, -33, 41, -29, 11, -56, 28, -16, 36, -2, 108, -19, 5, -62, 19, -8, -30, -6, 5, -44, 59, 17, 0, 4, 24, -49, -14, -31, -43, -4, 19, -7, -10, -40, -21, 13, 58, -5, 22, 2, -47, 25, 3, 50, -2, 92, -21, 54, -55, -21, 19, 28, 29, 14, -17, -32, -5, 26, -28, -27, 25, -27, -115, -15, -119, 31, -26, -3, -17, 19, -46, 8, -41, -16, -94, -43, 19, -49, 6, 47, -28, -39, -30, 14, -36, -72, 18, -4, 15, 37, 92, -19, 2, 22, -3, 62, -75, -36, -26, -63, 11, 4, -22, 13, -55, -30, 14, -14, -10, -34, 54, 14, 34, 20, -2, 16, 31, -5, 9, 9, -4, 42, 44, 42, 99, -29, -97, -4, 73, 18, 37, 64, -28, -51, -9, -25, -27, 22, 20, -29, 27, -14, -127, 7, -1, -53, -17, 55, 7, 61, -9, -32, -23, 4, -12, -26, -26, 16, 18, 12, 49, 11, 81, 123, -68, -21, 7, 15, 13, 49, -28, 9, 59, -1, -100, -11, 16, 22, 67, 11, -56, -27, 29, -32, 11, 100, -29, -40, -2, 1, 12, 0, -5, -107, -24, 12, 17, -88, -15, 1, -88, 76, 32, -16, -4, 15, 1, -54, 56, 74, 40, -6, 36, -33, -34, 14, -53, -27, 22, 14, 18, 59, 25, -33, -2, 6, 17, 120, 22, -31, 27, 49, -31, -15, 12, 72, -14, 21, -24, 32, 72, 70, -73, -17, 17, -65, 27, 18, 41, 9, -5, 69, 14, 39, -51, 11, 5, 20, -4, -11, 20, -22, 22, -11, 13, -73, 2, 0, 16, 4, -49, 36, 41, -18, 115, 72, 106, 66, -14, 21, 70, 53, -18, -66, -67, -121, 60, -67, -67, 43, -55, -23, 44, -69, 38, 8, 12, 62, -50, 102, 12, -20, -55, 52, -77, -1, 40, -50, 41, 13, 59, -51, -52, 16, -19, 14, -88, -31, 18, 21, -58, -15, -84, 7, -15, -23, -84, -30, -14, 59, -2, -64, -92, -16, 20, 9, 80, -52, -18, 53, 22, -4, -2, 31, 62, 21, 42, -42, -25, 64, 52, 19, 64, 28, -27, -47, 93, 29, -22, -12, 16, -24, 23, -60, 16, 66, -4, -37, -3, 12, -25, 12, 4, 71, 27, -54, 38, -14, 18, -82, 32, -23, -29, -64, 7, -12, 19, 12, 33, 99, 56, 38, -67, 27, 37, 52, 127, -2, 28, 42, -42, 72, -70, -25, 63, -35, 55, -16, -35, -30, -34, -8, 12, -19, -118, 21, -3, 8, -1, 25, -36, -13, 13, -38, 13, -59, -81, 26, 6, 5, 35, -72, -16, -9, 45, 9, 5, 30, -83, -93, -8, -42, 50, -27, 8, -12, 2, -70, 45, -46, -8, -10, 39, -15, -8, -41, -28, -62, -18, 1, -53, -34, -77, 50, -6, -7, -40, 75, 2, 16, 29, 47, 82, 19, -32, -39, 81, -7, -1, -59, -36, 19, -12, -5, 20, 14, -18, -3, 13, -2, 15, -54, 1, -7, -39, -118, -20, 12, -16, 0, 28, 6, 47, 32, 1, 7, -15, 37, 27, 13, 88, -2, 33, -3, 42, 67, -84, 23, -42, 69, 21, -4, -20, 32, 51, -13, -18, 27, -7, 34, -10, -48, 127, 61, 1, -28, -12, 0, 27, -6, 39, -8, -34, -5, -20, -26, -29, 12, 1, 2, 40, -66, -61, 6, 71, 4, -82, 36, 33, 0, -38, 25, -23, 14, 20, 9, -25, 32, 26, -30, 63, 61, -22, -32, 52, 4, -49, -9, -34, 11, -36, 15, 1, 91, -19, 19, -16, 20, -59, 14, 0, 11, -51, -44, 7, -38, 15, 33, 20, 0, 43, 17, -11, -1, 42, -20, -4, 2, 16, -14, -47, 5, 25, 53, 41, -8, 6, -58, -52, -36, -16, 11, 36, -47, 52, 29, 3, 18, -7, 5, 28, -71, -9, 40, -2, -23, -25, -49, -12, 16, -6, 22, -16, 100, 13, 24, -9, 16, 16, 7, 16, 30, 4, 4, -2, 21, 2, 74, 28, 12, 10, 20, 6, 13, -1, 9, -84, 32, -14, -6, 15, -13, -29, 100, 38, -18, 15, -52, -10, 30, -3, -30, -34, 8, 8, 23, 1, 9, -21, 46, -52, -11, -8, 37, 40, 39, 19, 33, -9, -4, 44, -21, 47, -29, 28, -13, 30, 28, -105, 20, -26, -33, 23, 29, -16, -16, -5, -11, -6, 27, 58, 32, -16, -4, 24, 19, -21, -3, 0, 34, -24, 43, 12, 54, 23, -36, -4, 0, -50, -48, 21, -12, -24, -30, -4, -88, -8, 12, 63, 7, 11, 113, -16, -71, 1, 3, -33, -10, 62, 10, -6, -58, -96, 53, 14, -25, -60, 15, 32, -27, -12, 18, -37, -79, 7, 8, 90, 60, -11, 27, -11, -17, -29, 21, 52, 28, -16, -18, -20, 40, 24, 33, -37, -2, 23, -44, 32, -62, 49, 2, -34, -44, -18, -14, -3, -21, 82, -14, 42, 42, 3, -18, -17, -8, -11, 1, 15, -99, -42, 63, -19, 37, -2, -21, 11, -2, 16, -10, 32, 127, 11, -28, 113, -54, -5, -2, -12, 36, 14, 13, -6, 30, -11, 0, 26, 34, -76, -56, -7, -51, 17, 19, 11, 36, -6, 22, 30, 56, -20, -12, 52, -18, 5, -54, 32, 23, 15, 27, -44, 28, -61, 112, 20, 48, -89, -3, -123, -25, -26, -2, -39, 0, -39, -37, -25, 12, 19, 24, 3, 17, -86, 19, 25, -4, 26, 3, -35, -16, 19, 28, 38, 36, -20, 21, 99, 114, 1, -24, -13, 1, -70, -6, 4, 18, -8, 52, 77, -23, -27, 25, -84, -102, -85, 22, 7, 95, 0, -35, -30, 26, -48, 14, 15, 4, -5, -21, -16, -11, 4, -3, -27, -37, -83, -26, -11, 98, 9, -61, 25, 27, 47, -11, 28, 2, 1, 59, 39, -69, 0, 81, -17, 27, -20, -34, -7, -48, 20, -30, -9, -37, 20, 14, -24, 5, 27, -88, -11, -3, 4, -4, 30, -13, 39, 24, -12, -107, -21, -23, 5, -9, -88, -11, 33, -5, 59, 9, 72, -24, -5, -20, 0, -25, 6, -51, 41, -20, -13, -32, -79, 30, -70, -18, -92, -5, -31, -41, 0, 2, -3, 24, 20, -8, 16, -17, -35, 9, 10, 10, 5, 3, -2, -20, 32, -15, 7, 52, -1, -127, -3, 14, 76, 12, 15, -21, -30, 38, 16, -22, -27, 48, -62, -38, -39, -70, -4, -15, 32, 1, -11, 8, 7, -21, -20, -25, 82, 5, 0, 16, -57, 26, 37, 6, -30, 83, -11, -37, -52, 3, -33, -30, 2, -39, -4, -45, -51, -28, -29, 26, 6, 3, 34, -22, -14, -22, 12, -32, 96, -10, -4, 72, -8, 14, 26, 75, 13, 8, -12, 70, 10, 51, 46, -78, -30, 13, -4, -27, 20, -33, -75, -27, -24, 8, -21, 114, 7, 32, -23, 11, -37, -58, -12, 31, -68, 3, 52, 23, -9, -38, -24, 7, -31, 105, -61, 16, 6, -44, 17, 9, 32, -14, -30, -22, 17, 63, -43, 15, 10, -20, 27, 59, -16, -68, -9, -3, 27, -3, -33, -7, -24, -23, -27, -9, -17, -19, -2, 20, -13, 4, 12, 27, 33, -75, -35, -37, -4, -15, -6, 40, -49, -10, 27, 32, -11, -5, -41, 16, -38, -1, -23, 1, 1, 36, -5, -5, -3, 60, -64, -4, -3, -8, -18, 43, -1, -69, 1, 18, -14, 26, -44, 26, 44, 15, -12, -15, 1, 103, 5, 20, 18, 14, -58, 37, -11, -20, 31, -7, 33, -15, -12, 32, -42, -3, -48, 36, 3, -27, -17, 41, 10, -35, -14, -22, 20, 32, -11, -13, -19, -22, -46, -17, 39, 69, 39, -10, 22, -37, 10, -25, 20, 25, 24, 0, -1, 35, 45, 24, 2, 49, 68, 21, 2, -13, 102, -3, -1, 3, 60, 2, -35, 45, 4, -5, 14, -66, -19, -15, -36, 11, 127, -107, -65, 1, -4, 41, 49, -15, 21, 7, -5, 20, -36, -71, 2, 24, -25, -22, -30, -82, 21, -2, 20, -109, -31, -29, -2, 36, -40, 26, -26, -67, 0, -69, -22, 45, -47, 33, -62, -11, -42, -93, -100, -12, -105, -29, -6, -75, 14, 2, 12, -63, 28, 57, 18, 12, -48, -76, 123, -15, -54, -43, 82, -16, 76, 38, -19, -68, 2, 3, -47, 4, -11, -4, 86, 5, 22, 32, 18, -10, 9, -25, 40, -51, -111, 45, 4, 56, -18, -3, 12, -6, 6, 43, -2, -65, -4, 78, 6, -3, -27, -23, -35, -25, 12, 14, 7, -25, -15, 24, 32, 5, -44, -35, -39, -16, -3, 5, -60, 1, 31, -5, -85, -22, -19, -21, -4, -27, 16, -8, -63, -61, 18, 22, -50, 8, -18, 15, -3, -59, -47, -9, -21, 44, -4, -45, -76, 44, 5, 29, 20, -5, -36, 37, 127, 62, -74, -3, -34, -8, -41, -49, 29, -25, 41, -7, -28, -56, 2, -24, -30, 7, -43, 21, -36, -3, -18, -16, -35, 2, 62, 36, -5, 40, -22, 29, -48, -57, 9, 69, 20, -21, -16, 12, 51, -8, -19, 90, -72, -11, 79, -6, 6, 13, 99, 60, 65, 60, 48, 45, -30, -23, -96, -39, 61, 45, 107, 2, -33, -9, 45, -13, 98, 9, 30, -12, -6, 74, -9, -16, 5, 101, 60, 31, -2, -12, -32, -7, -51, -28, 51, 58, 30, -60, 20, 40, 12, -21, -34, -75, 31, -22, 48, 1, 47, 64, -11, -51, 3, -61, -49, -5, 39, -9, 8, -3, -8, -49, 13, -27, -6, -34, -3, -79, -8, -13, 59, -28, -97, -42, -4, -36, -41, 11, 87, -56, -73, 25, 3, -11, -6, -43, 2, 14, -73, -11, 23, 13, -20, -35, -39, -13, -21, -34, -23, 22, 1, 3, -31, 32, 22, 50, -4, 4, 28, -53, -7, -5, -17, 70, 55, 23, -35, 58, 51, 37, 25, -1, -43, 70, 5, 10, -58, -9, 57, 42, -44, 10, -31, 12, 37, -26, 27, -85, 28, -20, -56, 32, 37, -34, 69, -26, 18, -41, 18, 13, -57, -44, 8, 23, 12, -5, -54, -44, 17, -32, -25, -25, -61, 67, -16, 17, 31, -40, 80, -40, -18, -19, -15, -35, 8, -49, -10, 53, 17, -9, -29, -13, 8, -5, -38, -34, -60, 3, 19, -64, -12, -43, 48, 30, 63, 13, -23, -31, -84, -54, -32, -3, -11, -13, -26, -16, 68, 13, 11, 35, -48, -6, -103, -11, -31, -48, -26, 16, 22, -57, -5, 9, 19, -41, 28, -6, 45, -19, -57, -70, -19, 44, 21, -37, -3, 15, -3, -2, -56, 18, -85, 9, -20, -54, -17, -47, 11, 61, -70, -13, -35, 24, -78, -2, -38, 3, 20, 26, 8, -16, -9, 26, 13, 29, 28, 13, 3, 41, -42, -39, 20, -63, -10, 21, 57, -62, 65, -8, -27, 17, -22, 13, -32, -31, -26, -12, -17, 11, -15, 18, -16, 14, -14, -44, -16, 10, -76, -37, 23, -18, -57, -72, 28, 75, 25, 15, 12, 56, -18, 55, 10, -16, 15, -11, 33, -93, 94, 51, -13, -35, 0, 42, 19, -13, -25, 23, -23, -70, -43, -35, 6, 33, -13, -59, -43, -35, -7, -23, -33, -46, 1, 43, -6, -31, -23, 19, -117, 21, 22, -20, 63, 28, -2, 56, -65, -21, -17, 46, 127, 37, 75, 118, -40, 5, 86, -8, 42, 2, 31, -30, -25, 37, 3, -62, -4, -63, 23, -40, -12, -25, -11, -32, -30, 9, 39, -34, -39, 29, 42, 49, -15, -40, -36, 3, 22, 1, -18, 68, -67, -85, -34, 72, -2, -1, 9, 25, -6, -26, 34, -29, 7, -89, 10, 35, 31, 35, -34, -27, 0, -5, 5, -48, 21, 12, 3, 35, 1, 13, -2, -26, -72, 30, 5, -12, -11, -44, 17, 11, 36, 105, 8, 40, -16, -20, 42, -85, 18, 68, -6, 40, 79, 38, -40, 32, 7, 16, -52, 23, -8, 20, 102, -4, -33, 85, 3, 9, 23, 1, -127, 16, 18, 7, 29, 0, -19, 28, -74, -98, 27, -47, -78, -4, 1, 74, -29, -3, -20, -56, -8, 7, 7, -23, 116, 84, -17, -6, -7, 14, 10, 7, 23, 46, -20, -69, -49, 70, -1, 11, 8, 39, 36, 19, 63, 9, 37, 25, -9, -26, 2, 5, 34, -16, 1, -24, 41, -27, 25, 34, -25, 2, -10, -16, 6, -9, 18, 10, 4, 83, -91, -25, -24, -41, 46, -41, 16, 46, -37, -2, 69, 1, 55, -4, 11, 2, 11, 55, 12, -13, -30, 25, 5, 56, -28, 47, -6, 16, -8, -43, -22, -38, -52, 44, -2, -50, -88, -5, 15, 42, -22, -102, -11, 11, -37, -8, 10, 101, -1, 8, 43, 8, 6, 30, 30, -13, 39, 1, -26, 4, -33, -28, 55, 31, 88, 11, 49, -13, 13, -26, -20, -44, 24, 46, -55, 21, -19, -33, 44, -78, 5, 51, 23, 68, -29, 1, 43, 30, 5, -20, -4, -85, -10, 9, 16, 20, 30, 3, -41, 14, 6, 0, 20, -31, 16, 2, -30, 39, 54, 69, 59, -9, 10, 19, 32, 15, -6, 26, -6, 25, 22, -25, 0, -10, -7, 7, 9, -4, -35, 15, -55, -31, -11, -64, 24, 27, 8, 116, 14, 0, -72, -39, 18, -45, 24, -13, -27, 61, -100, 77, -17, 31, -65, 38, -42, 17, -39, -21, -127, -8, 7, 83, 29, -35, -1, 37, 39, -50, 35, 96, -1, 15, -20, -46, 23, 32, 34, 27, -21, 44, -84, 37, 10, 48, -85, 0, 20, -22, 4, -48, -17, -11, -68, 8, 25, 65, 40, 12, -25, 36, 30, 8, -10, -70, -112, 1, 13, 17, 4, -29, -28, 119, -14, 11, -6, 62, -49, -12, -55, 32, 24, 11, -3, 13, 51, -28, -49, -32, 11, 42, -27, -75, 5, 29, 31, 6, 4, 13, -63, 4, -5, -44, 63, -7, 42, -67, 24, 23, 41, 53, -4, 24, -8, -36, 19, 11, 4, -51, -34, -61, 54, 13, 13, 7, 28, -11, 1, -42, -67, -11, -43, -4, -43, -13, 61, -16, 12, 5, -44, 10, -79, 95, 56, 1, 0, -29, 108, -44, -18, 15, 26, 7, -16, 21, -17, -22, -6, 0, -25, 49, -113, -8, 11, 59, -23, -3, 55, 29, 52, -4, 16, 38, 6, 0, -107, 39, -17, 19, 45, 27, 54, 9, 54, -52, 1, -17, -17, 18, 86, -38, 14, 37, -5, -26, -6, -3, 11, -5, 25, 19, -57, 109, 20, 36, 3, 109, 32, 19, -11, -64, -53, -36, 14, 9, 10, 18, -2, -40, -46, -10, 45, 28, 8, -32, -40, -39, -45, 6, -21, 18, -21, 5, 3, 80, -4, 38, -22, 3, -11, 2, 24, -3, 33, -3, 0, 22, 26, -45, 82, -30, -14, -37, -15, -52, -31, -47, -1, -8, 9, -5, -7, 4, -27, -31, 50, 25, 23, -23, -32, 55, -12, 8, -33, -41, -22, 3, -4, -4, -12, -43, -22, -38, 51, 67, -7, 20, -14, -43, 23, 18, -17, 12, 18, -46, -12, 68, -21, 1, -88, -10, -12, 14, -49, 1, -62, 6, 54, -7, -9, 1, -61, -14, -8, -28, -26, 10, 67, -7, 9, 5, -21, 12, 40, -2, -37, 75, 23, 35, -42, -39, -127, -51, 22, 4, 25, -4, 3, 3, 19, -35, 28, 29, -18, 57, -23, 7, -57, -25, 11, -14, -24, -61, 19, 40, -30, 18, -43, 54, -52, 38, 71, 31, 30, 32, 45, -14, 46, 46, 32, -88, -34, 55, -85, 29, -7, -43, -40, -10, 3, -28, -37, -50, 9, 15, 2, 18, 2, 3, 4, 9, -21, 15, -44, 33, 2, -19, -14, -23, -4, -3, 47, -17, 2, -16, -10, 24, -20, -21, -1, 21, 7, -68, -11, 97, -13, 32, 32, -31, -37, -42, 17, -8, -56, -45, -10, 14, -18, 2, 7, 2, -11, 45, 39, -29, 8, -113, -9, 0, 70, -56, 27, -12, -32, -6, -19, 27, -60, 55, 23, 33, 7, 30, -20, 15, 2, 25, -27, -18, 47, -12, 21, 2, 8, -4, -16, -4, -17, 4, 9, -29, -25, -11, -36, -26, -11, -9, 8, -9, 7, -3, 21, -1, -6, 118, -8, -30, 4, -17, -62, 28, 30, -11, -38, 48, -45, -16, -27, -36, -9, 5, -11, 20, 73, 7, 28, -2, -6, 19, -26, 14, -8, -15, -27, -38, -16, -1, -2, 39, 12, -49, -8, 20, 6, 20, 8, 8, -1, -11, 22, -16, 45, -18, -114, 19, 77, -12, -17, 37, 1, 26, 27, -2, -31, 38, 5, -28, -14, 14, -36, 10, 20, -29, -5, -13, 40, -8, 20, 8, -4, -2, -23, -24, 12, -10, 2, 8, 46, 6, 0, 5, -18, -9, 23, -6, -3, 2, -21, 24, -10, 32, 17, 11, 7, 32, -9, -45, -12, -6, 26, 24, -3, -13, 19, 1, 35, 2, 6, 48, -15, -3, -33, -95, -8, 2, -6, -30, 12, 6, -5, -32, 4, 13, 17, 4, 23, 3, 15, -8, -19, -13, 0, -90, -9, -6, -9, 6, -11, 35, 11, 26, -24, 1, -17, 2, -9, 16, 4, -29, -16, 71, -12, 29, -10, 23, 7, -21, -16, 1, -72, 20, -18, 62, 10, -30, 3, -10, -23, 11, -14, 4, -2, -12, -3, -7, 0, 1, 6, -10, 0, -12, -3, -25, 13, -40, -15, -22, 6, -26, -127, 14, 53, -17, -24, 8, -4, -15, -96, -1, 54, 28, -41, 5, 19, 3, -2, -11, -20, -18, -24, -21, -1, -40, 4, -31, -11, 30, 34, 8, 3, -2, -37, 58, -9, 33, 1, 12, -6, -15, 65, -15, -16, 40, -14, -15, 17, 4, 44, -77, -21, 19, -12, -32, 76, 20, -32, 26, 5, -32, -32, -1, -7, -18, -28, -15, 57, -2, 28, -38, 4, -1, 24, 23, 7, -24, -69, 3, -5, 26, -49, -36, -72, -11, 29, -21, -41, 4, -11, -27, -21, -38, -4, -26, 16, 21, 23, 47, 3, 77, -1, 15, 40, 6, -3, 13, 76, 20, -81, -9, -35, -56, 55, -16, 31, 2, -42, 18, -14, 54, -30, -19, 0, 8, 43, 6, 28, -29, -24, -85, 11, 4, 25, 52, -3, -8, 26, 63, 30, 11, -23, -11, 47, -18, -2, -50, -37, 44, 13, 7, -30, 5, -2, -11, -20, -43, 15, 69, -9, 14, -67, -43, 53, -52, -7, -37, 6, -12, -5, 2, -29, -32, -13, -30, 12, -49, 6, -21, 23, 3, -97, 27, -13, -50, -53, -8, -18, -25, 0, 1, 37, 21, -49, -24, 4, 9, -9, -11, -42, 6, -24, -84, -37, 3, 26, -41, -35, -20, -25, 12, -41, -15, 34, 3, -5, 38, 22, -55, -54, 17, -15, 14, 41, 19, 75, -23, 10, -35, -33, 12, 25, 47, 5, 26, -29, -20, -26, 17, 27, -19, 51, -5, 1, -127, -1, 29, 13, -15, -8, 2, -17, -21, -14, -23, 0, 13, -21, -26, 15, 4, -26, 19, 23, 3, 44, 82, 5, 22, -44, 23, 82, 80, -82, 17, -18, -52, 28, -10, 46, -43, 80, -8, -3, 38, 4, -49, -1, -67, -3, 38, -45, -17, -12, 36, -35, -17, 20, -14, 70, -34, -3, -20, -44, 1, 31, -15, -48, -39, 20, 67, -5, 45, 88, 24, 31, -53, -13, 0, -46, 40, -7, 4, -13, -51, 48, 36, 51, -19, 0, -1, 59, 3, -32, -17, 13, -53, -28, -11, -14, 55, -12, 20, 71, 15, 49, 27, 40, 26, -87, -35, 20, 18, -3, 66, -30, 3, -1, 48, -105, -27, -48, 22, -2, 2, -3, 30, 77, 30, -21, -28, 75, -13, -54, 27, 1, 62, -6, -36, -5, -22, -6, 43, 38, 6, -37, -55, -21, 16, -7, -2, 39, -12, 22, 68, -43, 41, 3, 42, -3, 15, -1, 22, -45, -14, -17, -10, -17, -10, -16, 81, 17, -44, -48, 0, -35, -37, -6, -27, 17, -16, 90, -49, -47, 55, 51, 9, 98, 40, 44, -54, 59, -32, -2, -7, 1, -26, 8, 23, 10, -2, 9, 46, -27, 46, -57, -39, 4, 51, 60, 15, -72, -10, 17, 7, -31, -31, -24, 62, 34, -100, 9, -52, -3, 32, 127, 11, 53, -46, 109, 5, 36, 28, -14, 49, 5, -1, -55, -34, -26, -5, -41, 10, 42, -76, 12, -51, -13, -68, -14, 61, -90, -55, -25, 17, -7, 59, -11, -42, 45, 55, 48, 16, -45, -18, 9, 10, 88, 1, 5, -6, 21, 19, -11, 3, -30, 5, -29, -35, -7, -126, 30, -18, 6, 8, -33, 6, 5, -9, -12, -90, -68, -53, -15, 49, -100, 13, 0, 8, -1, -13, 4, 9, 18, -68, -39, -20, -56, -20, -17, -29, -23, 51, -39, 12, 24, 14, 47, 34, 11, 31, 38, 54, 78, 31, 7, 12, -16, -19, 10, 28, 11, -81, -64, 49, 11, -15, -37, 127, -9, 28, 50, 47, -3, 2, 30, -20, -43, -25, 54, -8, -30, -24, -29, -5, 14, 15, -63, 52, 33, -22, 15, 22, -18, -39, 5, -41, -40, 7, 20, 125, 51, 69, -12, -39, 24, 4, 6, -61, 17, 8, 4, -41, -6, -11, 32, -8, 84, 43, 55, 37, -33, 16, -5, 16, 71, -6, -28, 74, -23, 44, -17, -56, -3, -23, -13, 17, -17, -62, -2, 9, 0, -126, 38, -9, -66, 56, 71, 42, -30, 12, 33, 7, 84, -5, 11, 60, -5, 22, 14, -21, 0, -35, -23, -31, 1, -11, -4, -54, -14, 14, 32, -16, -3, 18, 26, 43, -53, 9, 40, -21, -36, -20, 50, 15, 28, 30, 40, -22, 24, 8, 92, 9, -62, 14, 16, 41, 12, -21, -28, 7, -21, 53, -5, -101, -2, 8, -35, -22, 0, 4, 1, 15, 62, -78, 34, 6, 12, 17, 9, -9, 50, 2, -1, 23, 20, 2, -21, 23, 38, 2, 11, -8, -9, -11, 35, -2, -43, 20, 21, -6, 17, -24, -68, -12, 87, -57, -5, -9, -53, 36, -33, -3, -12, -17, -1, -3, 0, -18, 7, -19, -18, -33, -39, -8, -26, -62, -53, 30, 28, 13, 64, 24, -11, -77, -22, -72, 3, -66, -17, -29, 67, -2, -61, -24, 3, -64, 12, 7, -51, 7, -71, 74, 35, 126, -8, -52, 18, -75, 11, -45, -24, -24, 32, 16, 110, 0, -42, 10, -51, -25, -12, -27, -21, -22, -7, -47, -35, -39, 38, 32, -35, 22, 97, 32, -34, 49, 11, -44, 13, 58, -23, 10, 17, -40, -55, -2, -124, 29, -6, -3, -4, 15, 0, -22, -26, -8, 15, -37, -22, 3, 3, -37, -65, 12, 4, -49, 7, 32, 97, 11, -47, 33, -10, -15, -56, -12, 13, 17, 94, -41, -33, -63, 45, 56, -6, 32, -21, -2, 33, -36, -49, 38, 17, 3, -9, -80, -13, 10, -127, 11, -57, 36, 43, 7, -18, -13, 64, -1, 26, -82, -47, 1, 2, -4, -86, 19, 15, -10, 58, -3, 37, -3, -13, -39, 19, 10, 65, 9, -34, 22, -38, -19, 25, -27, 0, -85, -26, 22, 68, -14, -1, 8, 12, -25, -29, 15, 10, 1, 66, -8, 29, 89, 22, 62, -19, -6, -48, 17, 5, 39, -32, 6, -22, -11, 5, 4, -28, -24, 37, 59, 30, -19, -12, -26, 4, 19, -48, 27, 25, 56, 30, 2, -9, -25, 31, -63, -25, 1, 32, 11, 11, 51, -24, 19, -62, 54, -12, -5, 66, -1, -51, -64, -16, 37, -67, 23, 41, 33, -4, 34, 22, -43, 7, 22, 13, 29, -93, 35, 3, 46, -42, -23, 82, 6, 4, 12, 15, 41, -9, 36, 27, -15, 87, -16, 24, -32, 0, -20, -79, -18, -7, 1, -38, -18, 127, -58, -17, -80, 10, -12, 17, -33, -19, -43, 25, 11, -6, -47, 2, -13, 76, 47, 39, 1, 4, 20, -16, -36, -32, -6, 54, 9, 14, -39, 15, -3, -13, -17, 5, -7, 18, 12, -6, -74, -42, -9, 54, 0, 64, -29, -3, 31, -9, 64, -7, 25, 16, 19, 51, -13, -38, -23, 50, 30, 52, -68, 43, -35, 48, 57, 11, 35, -7, -22, 12, -68, -52, 24, 17, 11, 64, -46, -4, -24, -80, 49, 7, -60, -1, 17, 12, 7, -27, -25, 59, 23, -53, -27, -39, -27, 14, 37, 23, 43, -28, 63, 39, -26, -38, -17, -34, 3, 50, -45, -10, -10, -7, -29, 1, -8, -33, 44, 41, -6, -37, 88, -11, 15, 55, 23, -3, -62, -16, -20, -12, 29, -18, 31, -21, 17, 26, -35, 54, 13, -78, 11, -1, -22, -44, 8, 3, 3, -29, -63, -17, 32, 32, 12, 7, 2, -37, -4, 5, 11, -5, 20, -17, -3, -47, -12, 44, 9, -23, -37, -8, -21, 36, 44, 68, -3, 10, -1, -32, 21, -64, 35, -127, 15, -2, 5, -65, -15, -73, 3, -61, 19, -26, -7, 8, 36, -83, -9, 65, 20, -17, -59, 8, 31, -63, 8, 43, -4, 9, 13, -42, 65, 35, 19, 20, 32, -71, -2, 45, -60, -10, -55, -72, -22, 111, 12, -24, 65, 5, -13, -25, -69, 36, 71, 24, 49, 6, -54, -35, 7, -43, 2, 24, 6, -11, 56, -31, -81, -9, -17, -35, -30, -23, -12, 4, 66, 60, 40, 119, 3, 8, -27, -9, -57, -9, 14, -3, -37, 2, 45, 5, -12, 39, 49, 28, 25, -38, -49, -10, 60, 33, -15, 54, 4, -30, 27, 37, 34, -63, -52, 45, 11, -6, 61, 27, 3, 58, -30, 37, 65, 18, -55, 3, -1, 42, 44, -6, 8, 6, 14, -4, 35, -38, -67, -18, -48, 17, 8, 40, 38, -12, -2, -17, -28, 36, -6, 45, -38, 5, -77, -69, -6, -47, 27, 17, -14, -57, 6, 49, -13, -1, -14, -15, -37, 57, -4, 8, -4, -23, 2, 14, -2, 12, 42, 20, 8, 1, -11, -6, 71, -44, -89, 4, -26, 8, -2, 28, 7, -49, -20, 0, 19, 57, -59, 50, -6, -16, 74, -46, 26, -54, -26, 11, -17, 15, 21, 23, 4, -12, -35, 5, 9, -38, 28, 18, 17, 32, -38, -30, 26, 8, -7, 4, -60, -11, -101, 39, -13, -9, -46, 6, 0, 62, 40, 25, 10, 8, -17, 8, -14, 0, -63, 22, 39, -2, 11, 46, -18, -33, -75, -32, 2, -10, -1, -22, 8, 39, 4, -37, -11, 3, -24, 4, -12, -24, -21, -29, 5, -11, -25, 21, 44, -6, 21, 11, 17, 9, -17, -9, 39, 2, -8, 5, 0, -33, 12, -24, 9, 14, -18, 0, -48, -25, -1, 13, -11, -29, 63, 73, -16, -35, 10, 18, 9, 38, -16, -32, -14, 16, -19, 15, -39, 0, 32, 28, 6, -22, -36, -2, -6, 17, 24, -30, 7, 24, 14, 4, -17, -11, 47, -36, -10, 2, -16, -4, -69, -5, 12, 7, 6, 40, -7, 13, 15, -10, -7, 6, 3, -14, -8, 7, 18, 10, -74, 2, -2, -30, -14, 25, -17, 20, 21, -51, 38, 7, 0, -38, -50, -4, 18, -3, -23, 22, -22, 13, -3, 86, 13, 46, -24, -23, 70, -8, -31, 12, 38, -19, 63, -23, -5, -33, 16, 23, -19, -15, -19, 20, 17, 16, 17, -15, -36, -12, 22, 22, 21, 7, 5, -12, 2, 1, -10, -12, 22, 6, -18, -19, 20, 9, 6, 3, -36, -5, -11, -14, 2, -12, 4, -39, 10, 7, -21, -30, -40, -36, 6, -43, -11, 49, 10, 2, -6, 10, -23, -5, 5, 9, -26, -26, 7, 7, -18, 3, 6, 6, -12, 24, -22, 20, -53, 3, 127, 8, -14, -16, -25, -24, 3, -40, 6, -2, -15, -17, 1, -71, 37, 20, 17, 20, 24, -23, 46, -59, -3, -6, 9, -17, 22, 8, 30, -15, 12, -25, 6, -9, -76, -15, -12, -37, -21, -32, -36, -11, 19, -45, -10, 9, 67, 14, 41, -46, 30, 70, -4, 18, 28, -15, 6, -1, -42, -8, -20, 19, 8, 41, 3, -15, 16, -10, -53, -18, -28, -47, -3, 20, 19, 11, 21, -29, 17, -21, -29, 64, -31, 9, -28, -11, 13, -25, 30, 1, 18, -41, -5, 4, 31, -44, 38, -43, 4, -12, 7, -32, 24, -24, -13, -20, -25, -22, 73, -24, -4, -5, -3, -31, 12, 29, -3, -20, -6, 1, -23, 13, -4, -11, -110, -15, 25, -84, 18, 5, 8, -35, -10, -12, 0, -21, 16, -22, 52, -5, 100, 70, 25, 39, 17, -51, -26, 20, -5, -10, 19, 46, 12, -1, 43, -6, 10, 14, -35, 42, -10, 0, -31, 17, 1, 29, -5, 39, 56, 44, 20, 2, 12, -6, 8, 28, -13, -7, 20, -41, 33, -11, 13, 59, -57, -57, -22, -48, -11, 17, 7, 13, 7, 21, 10, 7, -4, -29, -3, 55, 60, -11, 18, 41, -30, -10, 11, -31, 18, 19, -3, -39, 49, -28, 24, 60, -5, -13, -9, -93, -22, 7, 45, -9, -5, 67, 4, 9, -26, 5, -21, -54, 24, -13, 7, 0, 9, -127, -13, -20, -2, 9, 30, 8, 43, -15, -31, -37, 6, 1, 51, 5, 13, 13, 44, 64, -34, -5, 9, 2, -63, 33, 18, -17, 40, -1, 0, 36, 47, 20, 64, 14, 2, -8, -18, -21, -23, 7, -83, -43, -17, 14, -35, 0, -16, -35, -21, 26, -38, -23, -55, -23, 19, 59, -16, 64, -26, -48, 28, 79, -87, -7, 12, -43, -1, -88, 21, 5, -39, 0, 29, -47, 1, -12, 51, 28, -46, -32, -12, 4, 4, -64, -34, 31, -4, -21, -29, 38, -2, -111, 74, 31, -3, -19, 14, -48, 23, -21, 20, 3, -67, -48, 50, 16, 59, 53, -110, -21, -17, -29, -33, 7, 80, 4, 13, -12, 31, 8, -29, 60, 25, -44, -49, 38, 29, 4, -35, 24, -12, -17, 29, -66, -2, -26, -8, 23, 25, 23, -25, 17, 55, 15, -36, 20, -61, 46, 53, 17, -18, 46, -12, -72, -16, -43, -44, -103, 18, 57, 64, -8, -19, -5, -26, 9, 30, 23, 16, -32, -3, 12, 36, 6, 75, 32, -15, 30, 8, -33, 47, 16, -85, 19, 26, 5, 39, -69, 11, 15, -16, 60, 12, -5, 13, -2, 16, -9, -80, 102, 8, -16, 13, 28, -9, 25, 32, 1, 1, -8, -76, 14, 2, 7, 23, -89, -1, 13, 58, -34, -1, -3, -10, 13, -35, 31, -44, -4, 65, -9, 18, -9, 68, 41, 25, -27, -12, 43, -35, -47, -37, -22, 38, -36, -16, 14, 28, -64, 21, 19, 5, -14, -13, -2, -31, 20, -127, -22, -10, -18, -6, 7, 32, -4, -36, -12, 21, 63, -89, 42, 64, -49, 59, -38, 9, -58, -2, 2, 31, -29, -48, 24, -32, -3, -25, -14, -20, -12, -47, 48, -14, -75, 77, 6, -31, 28, -21, -15, -29, -28, -41, -5, 58, -31, -16, -39, -40, 5, 29, 47, 10, -8, 16, -54, -72, -9, -53, -23, -39, 68, -2, -64, 0, -55, -32, -23, 40, 10, -36, 26, -18, 21, 68, -34, -31, -8, -15, 6, 8, -34, 20, -29, -14, 34, -7, -40, -44, -10, 43, -3, 61, -46, -1, -27, 96, 70, 50, -33, -12, 3, -3, -4, -22, 9, 45, -5, -93, 15, -3, -10, -13, 83, -37, -17, 106, 5, 8, -31, -56, -65, 23, -23, -18, -6, -20, 11, -31, 49, 3, 44, 48, 37, 20, -10, 40, -3, -25, 31, -16, -42, -35, 20, 3, 3, 31, 15, -18, 22, 22, 5, -16, -15, 26, -34, 64, -13, -18, -127, -4, -7, -49, 26, 6, 20, 8, 53, 7, -23, -27, 17, 30, 7, 20, -21, -8, 1, 8, 5, 53, 22, -43, -39, 12, -63, -6, -24, -37, -59, 17, 56, -5, 11, 67, -32, 45, 24, 5, 2, -15, 24, 30, 50, -3, -3, -25, 3, 21, 77, -40, -29, -53, 4, -33, -63, -16, 9, 6, -38, 4, 1, 2, -27, 41, 30, -15, -16, 22, -1, -8, 54, 12, -13, -18, 45, -54, -5, -62, -77, -26, 9, 5, -12, -43, -60, 32, 31, -25, -27, -35, 5, -85, 21, -23, -1, -34, -83, 50, -127, -14, 42, 19, -18, -63, -14, 34, 36, -111, -7, -39, 43, 5, 4, -10, 46, 5, 13, 19, -23, 1, -84, 3, 7, 19, 78, 59, 51, -14, -42, 29, 57, 41, 19, 28, -21, 29, 6, 3, 77, -26, 5, -20, 28, 16, -18, -22, 28, -64, 25, 29, 6, 33, -68, -22, 44, -22, 20, 51, -8, 6, -57, 3, 21, -4, -32, 62, -53, -53, 21, -17, -60, 4, -13, 23, 12, -82, 12, -121, -31, 6, 5, -64, -5, 14, 2, -3, 47, -9, -56, 56, 102, 24, 57, -25, 2, -88, 12, 18, 30, -17, 42, 54, -59, -31, 13, -66, -2, 2, 114, 50, -5, 93, 57, -45, -67, 11, 34, 34, 6, -65, 51, 37, 72, -12, -10, -25, 1, 12, -16, 44, -33, -3, -31, 22, -32, 104, 45, 17, 49, 33, 83, -9, -47, 20, 0, -43, 42, -8, 60, 6, 0, -20, 46, -6, 30, -49, 16, 37, -5, -3, -37, 22, -62, 27, -31, 30, 32, -23, -127, -45, -28, 26, -21, 74, 5, 42, 23, -35, -14, -43, 64, -5, -28, -57, -7, -30, 26, 9, 36, 7, 8, -9, -17, -15, -103, 43, 25, -29, 44, -23, 37, 23, 20, 86, 1, 9, 34, 7, -91, 61, -27, -35, 11, -36, -12, -91, 1, 57, 50, -1, -2, 20, -16, -14, 63, 23, 22, 4, 60, 42, 59, 15, -30, -84, 31, 7, 12, -67, -50, -9, -67, -17, -6, 3, 27, -11, -21, 45, 32, -30, -8, -23, 20, -51, 73, 10, -50, 9, -35, 7, 4, 16, -38, 25, -11, -60, 66, 21, -17, -28, 59, -11, -24, 100, -38, -43, -36, -1, -22, -6, 18, -26, 91, 19, -19, -38, 2, -15, 59, -17, -55, -5, -76, 15, -34, 26, -8, 44, 31, 4, -16, 7, 27, 41, 9, 14, 12, 12, -48, 9, -80, -4, -41, 71, -14, 22, 3, 34, 33, -33, 38, -53, -3, -23, -13, 24, -60, 21, -34, -25, -7, 3, 11, 33, -7, -6, 37, -41, -28, -40, 13, 81, -34, -82, -12, -43, 18, -43, 29, -57, -25, -33, 26, -33, 103, 17, -54, 32, 7, -16, 66, -15, -49, 17, 24, -45, -40, -10, -50, 63, -21, -44, 5, 106, -75, -26, 20, 4, -7, 24, 14, -32, -18, 2, -15, -37, -111, -43, 20, -35, -69, -5, -41, -2, 11, -36, 49, -25, 3, -4, -62, -22, -59, -71, -49, -28, -26, 31, 7, -9, -5, 7, 11, 35, 17, -53, 9, -7, -30, 29, -50, 9, 3, -35, 7, -6, 70, 118, -28, 13, 37, -14, -6, 30, 0, 24, -2, -30, 63, -17, -60, -47, 77, -36, 18, 20, -51, -31, -17, -25, -54, -41, 2, 64, 48, -17, 41, -127, -6, 27, -24, -3, 4, -28, 15, 9, -2, 8, -13, -24, -7, -9, -3, 68, -2, -12, -2, -8, -1, 5, -17, -9, 4, 11, -9, -2, -24, -32, -18, -1, 10, -1, -21, 6, 0, 14, -4, -8, 2, -6, -11, 7, 12, 23, -2, 40, 8, 8, -19, 5, 3, -11, 1, -2, 3, -46, -1, -8, 39, 34, -2, 9, 6, 4, -19, 32, -3, 2, 1, 17, 54, -8, 23, 59, 18, -72, 55, -31, -16, -11, -39, -7, 3, -17, -58, -24, 22, -12, 30, 7, -38, 27, -16, 7, 21, -39, -42, 6, -1, 0, 11, -5, -17, 0, -27, -20, 1, -5, -1, 22, 17, -39, 25, -1, 10, -7, -1, 2, -18, 34, 3, -16, -15, 7, -18, -20, -27, -2, 1, 35, -7, 9, 27, 0, 24, -15, 17, -15, -22, -17, -4, -28, 18, 31, -5, -25, -2, 36, -17, -4, -11, -13, 10, 26, -31, 44, 3, -3, -31, 8, -17, 74, -6, -10, 3, -4, 31, 65, 17, -4, 26, 2, 2, -2, -2, -6, -5, -16, -8, 18, 28, 16, 0, -36, 27, -5, 1, -13, 11, 2, -20, -8, 7, 68, 1, -30, 9, -8, -19, -27, -3, -2, -2, -3, 2, -22, 4, 5, 13, 11, 16, -14, 7, -12, -9, 3, -3, -14, 2, 15, -4, -12, 0, -84, 9, -19, -25, -5, 24, -13, -127, -1, 1, 14, -19, 8, -6, 22, 59, -25, 8, 48, -13, -18, 6, -6, -28, 4, 2, 46, -28, -7, -43, -43, -4, -5, 9, 11, -30, -18, -48, 30, -21, -127, -1, 0, 38, 55, -71, 10, 26, -33, -5, 21, 62, -28, 2, 1, -7, 0, -9, -30, -17, 44, -1, -29, -4, 54, 42, 16, 26, 3, -10, -25, -8, 15, -6, -13, -12, 26, 17, 27, -6, 2, -17, 12, -7, -4, -25, 3, -15, 1, -22, 0, 77, -61, 56, 21, -9, -5, 45, 16, 20, 40, -14, -87, -32, -25, 63, -23, 22, -10, 6, 2, -92, 14, -71, -4, -21, 11, -5, -15, -3, 9, -16, 52, 26, -7, -40, -13, -58, -10, 22, -3, -34, 11, 3, -21, -38, 18, -11, -3, -26, 21, -10, -40, -5, -14, -2, -20, 1, 23, 26, 0, -11, 32, -2, -108, -30, -23, 18, -15, 2, -33, -17, -23, 12, -5, 25, -29, 2, -10, 11, 29, -10, -34, -27, 27, 21, -26, -19, 12, 1, 19, -33, 25, -19, 36, -14, -20, 102, -14, 7, -5, -7, -46, 3, 13, 9, 32, 53, 18, 41, 35, 6, 3, -30, -1, -48, 7, -35, -44, 7, -48, 13, -5, 17, 20, -19, 0, -27, -4, 4, 1, -18, 17, -13, 9, 19, 37, -7, 63, 20, 14, -52, -15, -25, -33, 57, 26, -19, -26, 25, -36, 7, -31, -3, -28, 31, 0, 44, 34, -20, -32, -12, -14, 40, -25, -26, 6, 16, 38, 31, -13, -12, -32, -21, 29, -30, -42, 0, 4, 3, -127, -6, -14, 21, -11, -76, 18, -23, 11, -5, 80, -21, -31, -8, -35, -1, 15, 16, -22, 23, -18, -81, 0, -1, -30, -7, -7, -2, 32, -56, 9, 6, 60, -36, 7, 15, -1, 69, 22, 20, -2, -56, 26, -57, -8, 2, 21, 70, -3, -14, -73, -52, -22, 10, 3, -14, 44, -10, 5, -43, -6, -54, -2, 33, -3, -14, 43, 0, -33, -17, -5, 14, 0, -15, 121, 24, -16, -54, -74, 0, -32, -35, 38, -37, -36, 17, 24, -14, -7, 94, -11, 34, 19, -27, 18, -23, 9, -41, 35, 9, 43, -47, -3, 24, 15, 63, -13, -23, 1, -11, 36, -14, -22, 4, 5, -28, 9, -57, 2, 56, 5, -66, -23, -64, -35, -12, 41, 37, -18, 10, -33, -27, 38, -6, -17, -37, -61, -2, -20, -52, 68, -61, 9, 0, 10, -20, -4, -29, 49, 6, -7, 40, -9, -58, -19, 73, 13, -17, 20, 11, 11, -43, 48, 14, -7, -39, 74, -59, -4, 31, -43, 7, 78, -60, -3, 14, 22, -34, 65, 20, -9, 34, -22, -1, -7, -15, -15, -18, -80, -74, -18, -22, 17, 22, -4, -23, -76, -24, -55, 5, -33, -33, 2, 8, -28, -91, 25, -22, -20, -56, -75, 39, -19, 90, -3, -3, -7, -55, 8, 8, 35, 17, -4, -25, -41, -11, 26, -38, -4, -3, -17, -9, 22, 17, -7, -40, 20, -23, -3, 1, -40, -1, -4, -5, -31, 46, -17, 3, 47, -15, 0, 3, 7, -5, -51, 0, 3, -9, -7, -27, -4, 17, 9, -10, 0, 20, -6, -16, 32, -22, -11, -59, 4, 29, 9, 17, -34, 37, 7, -7, -27, -2, 19, -3, 0, -20, 11, 81, -16, 0, 2, -47, -58, 1, 14, -1, -1, -5, -26, 5, -18, -16, 25, -25, 33, 2, -39, -26, -3, -10, 5, -117, -42, 29, -9, -13, 16, -54, -1, -24, 1, -18, -34, 42, 32, -24, -9, 3, 0, 11, 9, -55, -49, 12, -49, -2, -8, -7, -44, -22, 2, 9, 34, 10, -9, 14, -30, 24, -53, 1, 0, -2, -15, -26, -1, 35, 22, -9, -23, 6, -12, -30, 9, 7, 21, -36, -30, -12, 13, 33, 2, -11, 2, 7, 66, 8, 19, -28, -14, 82, 6, 19, 15, 63, 4, -30, 11, -9, -43, -14, -63, -2, -15, -15, -57, -35, -12, -8, -12, 2, -4, -5, 1, -8, -18, -36, 63, -38, 32, -10, 76, -11, 16, -3, 1, 34, -9, -9, -23, 2, 23, 0, -92, 42, -1, -2, 65, 13, 9, -9, -8, 11, -36, 4, 9, -3, 34, -18, 49, 6, 12, -16, 12, -8, 10, -22, 4, -33, -15, -5, -3, 5, -84, 20, -17, 26, -49, 86, -2, -4, 127, 9, -9, -42, 32, 4, 16, 33, -20, -5, 11, 9, -28, -2, -8, 4, -123, 19, -73, -69, -18, -18, 10, -18, 28, 12, -51, -15, -101, -50, -17, -17, -18, -7, 80, 24, -19, 4, 52, 12, 68, 94, -8, -68, -30, -69, 36, -2, -13, 10, -15, -17, -45, -23, -4, -29, 15, 12, -23, -32, 30, -38, 54, 41, 7, 16, 71, -38, 25, 36, -2, -36, -22, 22, -23, 30, -17, 31, -72, 60, -71, 19, -55, 4, 7, 49, 27, 50, -3, -14, 33, 7, 27, -56, 31, 35, 14, 52, 31, -28, -29, 23, 59, 16, -46, -6, -1, -2, -49, 70, -39, 16, -11, 73, -18, 13, 9, 72, 11, -27, 27, 29, 1, 4, -8, 37, -6, -5, 9, -32, 69, 83, 77, 65, 7, -52, -33, 7, 0, -27, -40, -27, -3, -37, -64, -14, 19, 5, 43, 92, 61, -8, 32, -21, -19, -24, 23, 10, 44, -17, -49, -48, -32, -13, 16, -53, -3, -16, -50, -15, 26, 18, -28, 35, 38, -17, 4, 97, -29, -47, 27, 6, -21, -28, 4, -10, 13, 28, 2, -22, -8, 45, 35, -12, 5, -52, -5, 32, -55, 37, 52, -10, 22, -25, -14, 105, 4, -34, -6, 21, -26, 10, -9, 15, 17, 12, 32, -22, 46, -1, -24, 48, -25, -20, 3, -56, -60, 1, -34, 18, -22, 20, -16, 24, 19, -42, -127, 19, 0, -6, 42, -50, -11, 13, 77, 2, -11, -30, 65, -12, -6, 4, -29, -30, -26, 77, -32, -10, -70, -9, 25, 1, 16, 49, 13, 44, 68, 7, 96, 53, -52, 12, 17, 28, -65, -21, 60, 24, 3, -85, 21, 3, -30, 5, 22, 51, 12, 37, 36, -39, -4, -43, -4, -19, -100, 31, -38, 9, -31, -9, -36, 63, 53, 4, 11, 12, -68, -5, 68, -31, -21, 15, -15, -5, 70, 33, -22, -28, -50, 18, -65, -29, -32, 34, -39, -3, 11, 20, 19, 20, -71, 15, 7, 13, -8, -59, -43, -15, -13, 46, 39, 1, 0, 24, -44, -50, 18, 29, -19, 6, 9, 43, 97, 23, -8, -4, -127, -3, 18, -26, -18, -70, 11, -11, 13, 16, -16, 31, 32, 103, -60, 39, -104, -116, -23, -4, 98, 5, -98, -9, 71, 99, -44, 51, -11, 40, 84, 57, -77, -87, 6, 26, -74, -1, 33, 20, -61, 32, -21, 71, -73, -17, -7, 28, 26, 2, -39, 2, 24, 46, -14, 18, 65, 33, 4, -4, 24, 14, 43, 18, 19, 30, -50, 29, 8, 61, 1, -21, -13, 50, 48, -10, 29, -57, 25, 15, 19, -5, -67, 1, 74, -37, 20, 19, 5, 58, -10, 69, 31, 43, -19, -9, 40, 49, 16, -1, 3, -16, 15, 58, -97, 83, 24, 16, -11, -26, -12, -81, -86, 29, -34, 18, -23, 53, 4, 59, 22, -85, 91, 6, -49, 6, -38, -28, -49, 20, 17, -19, 6, 13, -4, 27, -11, 4, -21, 9, 22, 21, 12, 0, 116, 28, 2, 1, -2, -34, -4, -13, -15, 5, 10, 1, -19, -22, -27, -22, 9, 4, -23, -16, 18, -65, 12, -16, 28, -8, -6, -3, -12, 66, 28, -8, -2, -21, -27, -29, -13, -46, 3, -22, 29, -23, 61, -1, 22, 59, -4, 11, -18, -43, -12, 3, 29, -11, -11, 2, 15, -46, 4, 31, -77, 42, -89, -21, -22, 30, 25, -8, -23, 22, -5, 22, 12, 6, -51, -54, 22, 6, -26, -15, -9, -127, 25, 8, -27, 17, 2, 11, 36, -23, 27, -8, 5, -3, -19, 7, 18, -7, 27, -2, 3, 14, -3, -25, 12, 24, 1, -23, 0, 32, 2, -42, 47, 18, -3, -42, 18, 3, -23, 7, -14, -1, -47, -16, 70, -42, 28, 21, 29, -8, 39, 0, 33, 15, -12, 8, 23, -7, -2, 3, 32, -12, -18, -20, -12, 19, 9, 6, -26, 40, -1, 23, 6, -50, 0, 14, 9, -61, 0, -24, 21, -10, 55, 18, -6, 5, 49, -38, 4, -12, 4, 39, 50, 55, 12, -1, 1, -46, 3, -12, 5, 11, -23, -1, 15, -51, -26, -23, -10, 3, 6, 9, -5, 7, -16, 10, -35, 26, 62, 6, -21, -45, 12, 19, -2, -60, 12, -5, 24, 33, -43, -7, 14, 27, -32, -16, -67, -24, -1, 9, -4, -2, -10, -12, -58, -18, -1, -46, 11, 22, -14, 4, 6, -5, 5, 41, 22, 24, -20, 1, 11, -3, 4, 40, -8, -4, -7, 6, 9, 13, -13, 23, -29, 62, 25, 24, -22, -30, 49, -6, -12, 29, 8, 3, -3, -85, 23, 5, 44, -28, -11, -59, -20, -22, 31, -20, -14, 17, -8, -23, 9, 21, -15, -44, 19, 6, -20, -14, -71, 4, -16, 3, 1, -33, 53, -43, 19, 14, -34, 15, 37, -40, 7, 0, 9, -11, 14, 4, 14, -25, -25, 1, 48, 8, -28, -6, -19, 12, 4, -9, -36, 31, 26, 23, -31, -4, -19, 35, -11, 32, 27, 41, 57, 3, -7, -62, 21, -8, -18, -21, -16, 17, -14, -16, 0, 14, -33, -127, 44, 46, -33, 42, -3, -3, -1, 88, 39, 32, 9, -24, -15, 8, 48, -22, -8, 15, -48, 46, -9, 5, -20, 28, -2, 3, -49, 1, -27, 28, -24, 12, -45, -19, 1, 9, 10, 16, 18, 0, -26, -24, -44, -46, 22, 25, 1, 0, 16, -4, 7, -44, -16, -28, -9, 4, 5, 13, 19, 32, -45, -14, -38, -39, 30, 82, 19, -23, 3, 21, 3, -11, -6, 19, 1, -31, 13, 38, -53, -45, 10, -38, 35, -31, 40, -9, -19, 3, 1, 11, 39, -17, 15, -9, -21, 3, -5, 7, 4, -3, -7, -28, -23, -13, -31, -14, 5, -35, -33, 18, 13, -13, 53, -27, 5, -25, 30, -5, 1, -12, 2, 0, -30, 57, -21, 34, -14, 19, -22, -21, 58, 32, -76, 19, 6, -5, 11, -44, -13, 24, 3, -16, -19, 9, 5, 10, 5, 33, 21, -19, 3, -15, -64, -29, -24, 22, -38, 25, -24, -10, 14, 0, 68, -24, 52, -20, 17, -49, -31, -15, 11, -9, 12, 5, -8, 13, 8, -9, -27, 7, 10, -24, -59, 5, 53, -3, -6, 27, 19, -5, -30, -127, -47, 32, -6, 40, 8, -15, -101, -21, -15, 2, 2, 29, 0, 13, 106, 64, 50, -51, 23, -45, 0, -27, 9, -40, 25, 32, -3, 2, 25, -13, 26, -5, 9, -6, 17, 3, 3, 43, 23, 6, 32, -8, -15, -25, -31, 15, 28, 16, 1, 24, -30, 31, 76, 38, 27, 47, 33, -4, -21, -36, 1, -5, 24, 53, -34, -27, 10, 2, 14, -17, 19, 7, -27, 0, -17, 28, 73, 67, -10, 18, 22, 15, -20, 42, -16, 58, 1, 18, 31, 24, 2, 22, -8, -14, 18, -31, -3, 38, -1, -16, -20, 18, 10, -71, -36, 67, -24, -74, 2, -23, -5, -41, 85, 24, 10, -23, -5, 39, -51, 17, 3, -29, 30, 2, 32, 21, -8, 52, -13, -5, 32, 44, -6, -47, -46, -19, -1, 13, 44, -11, 6, -31, 18, -10, -1, 22, -11, 5, -14, 8, -39, -43, -79, -80, 66, -32, -20, 12, -13, -9, -74, -17, 6, -14, -2, -13, 42, -57, -3, 8, 2, -8, -19, 4, -9, 2, 20, 21, -37, 58, 5, 18, 34, -61, -19, 4, -5, -7, -16, -2, -42, 22, 0, -28, -54, 25, -63, 52, 6, -2, -18, -53, 7, 7, 5, -49, -30, 6, 16, -19, -11, -3, -127, -27, -34, -8, 48, 1, -7, 0, 0, 3, 17, -24, -2, 27, 59, -4, 18, 17, -4, -48, -13, 5, 12, 9, -28, -22, 23, 14, -12, -19, 12, -81, 9, 32, 35, 19, 9, -6, 26, -14, 14, -27, 27, -23, 18, -23, 65, -43, -18, -73, -24, -42, -55, 3, 32, -6, -16, 31, 14, 26, -6, -27, -88, -11, 24, -30, 11, 23, -13, -6, -43, 3, 16, 14, -12, -35, -18, -13, -36, 18, -44, 40, 14, -102, -26, 6, -78, -24, 9, 17, -27, -19, -37, -27, 3, -18, -53, 34, -42, 46, -15, 7, -1, 32, 8, -8, -9, -32, -23, -16, -28, 6, -28, 59, -1, -26, -55, -1, 6, 12, 3, 61, -20, 18, 9, -13, 9, 30, 28, -3, -11, 5, -19, -9, -49, 68, 47, -3, -57, -36, -31, -23, 0, 22, 3, -46, 8, -3, 48, 12, -14, 27, 3, 14, 20, -10, 13, -5, -10, -17, 11, -2, -3, 57, -36, 16, 22, 6, -2, 19, 19, 25, 18, 3, -14, 77, 62, -14, 5, 6, 8, 14, -2, 11, -21, 60, 5, -3, -8, -25, -5, -24, 0, 10, 34, -54, 30, -52, -4, 39, 21, 4, 3, 46, 13, -49, 18, -33, -16, 4, -15, -7, 23, 33, -63, 27, 29, -60, 20, -28, -25, -43, -34, -3, -20, -11, -31, 6, 44, -45, -7, 41, -46, -27, 15, 89, 114, -5, -81, -35, -12, -77, 51, -38, -42, 38, 27, -9, -53, 34, -25, 51, 18, 49, -1, -49, 39, -11, 20, 19, -45, 25, 97, -21, 57, -13, -14, -8, 67, 18, 55, -23, -23, 100, -28, -63, -22, 22, -31, 47, 50, 58, 24, 14, -19, -47, 14, 3, 7, -11, -52, 8, 44, 12, 2, -17, -1, -21, -45, 5, 13, -14, -43, -41, 3, -21, 127, 40, 34, -1, -24, -19, -9, 25, -17, 116, -32, 36, -9, -50, 18, -31, 44, -23, 3, 7, 39, 6, 24, -28, 17, 34, -24, -16, -44, 2, 32, 125, 14, -1, -46, -46, -38, -85, -16, 29, 0, 104, 20, 1, 28, -69, 49, -17, -34, -66, -14, -45, 81, 10, 64, -9, -26, 36, 18, 3, -49, -28, 1, 15, -18, -27, 69, 38, -34, -42, -68, -14, -20, 40, 11, 63, -5, 77, -11, 14, 3, 16, -10, 46, 43, -53, -16, -24, -46, -46, -29, 58, 25, 42, -46, 21, 14, 24, -6, 29, -31, -27, 11, 22, 6, -6, 0, -14, -55, 51, 34, 22, 26, -5, -100, -5, -33, 23, -4, 34, 59, -8, -47, -39, 41, -50, 15, -7, 2, -8, -2, -1, -41, -42, 10, 52, -13, -106, 74, -92, 58, 2, 60, -7, 11, -53, -47, -85, -45, -5, 11, -61, -3, 26, -4, -41, 68, 23, -7, 8, -43, 28, 4, -6, 24, 27, -34, 89, 14, -44, 55, -6, 23, -63, -7, 73, -38, 12, -16, -24, -24, 15, -33, -30, -23, -11, -25, -34, 2, 92, -7, -67, -7, 48, 14, 42, 44, 19, -61, 2, 30, 30, -8, -27, -47, -32, 12, 25, -54, -11, 21, 57, 35, -36, 40, 5, -18, 7, 21, -8, -82, -10, -4, -1, -25, -10, 56, -87, -9, -11, -23, -16, -17, 43, -43, 20, 24, 6, -63, 22, 56, 6, 15, 2, 27, 10, 0, -11, 46, 56, -2, -8, -23, -21, 16, 7, -58, 46, -86, 10, -14, 50, -15, 75, 47, -67, 28, -23, -74, -32, -16, -12, 14, 7, -83, -7, -33, -6, -127, 27, 5, -44, -38, -44, -29, 28, 25, 0, 35, 44, 38, -83, -53, 14, -26, -6, 13, -23, 1, 20, -41, 3, 21, 17, 10, -13, 39, 14, 58, 66, -14, 29, -8, 6, 22, -13, 15, 42, -8, 17, -42, 3, -19, -38, 24, -37, -18, -15, 5, 16, -10, 42, -41, 38, -43, 7, -9, -22, -69, -36, -12, -28, 4, -28, 10, 14, 7, -29, -6, -32, 70, 40, 49, -6, 31, 14, 38, -27, -48, -5, -20, 3, -22, -97, -49, -55, 14, 13, -2, -7, 4, 6, -12, -3, 8, -2, -2, 1, 16, -6, 10, 9, -34, -5, -11, -8, -14, -3, -12, -14, 10, -13, -11, -2, 7, 8, 0, 9, -23, -1, -9, -10, -18, -51, 1, 13, 5, 27, 1, -6, -1, 8, 9, 2, 4, 9, -2, 17, 16, -18, 23, 10, -3, -8, 5, 0, 3, -6, 15, -11, 12, 21, 8, -13, 48, 3, -5, 2, 11, -7, 8, -18, 0, 11, -47, 13, 3, -1, -1, -5, -1, 6, -19, 2, 8, -7, 4, -40, 2, -5, 14, -9, 10, -13, -9, 127, 2, -1, 3, -9, 14, -12, -3, 2, -1, -20, -8, 5, 0, 8, 8, -5, 16, 6, -11, -6, 2, -9, -64, 13, 12, 14, 10, 12, -6, 10, 4, 2, 16, -6, 8, 1, -5, 41, 20, 3, -44, -27, -4, -14, 13, -4, 0, -11, -9, -1, 20, 5, 9, 6, 10, 4, 5, -31, 12, -2, 13, 13, -4, -9, 0, -15, -6, 0, 5, 12, 76, 22, -7, 14, -15, -4, -13, 1, -7, 6, 9, -2, -4, 5, 7, -19, -56, -11, 9, 0, -3, -7, -10, 3, 11, -2, -25, -1, -6, -3, -2, 10, 4, 8, -7, -11, 1, 5, -6, -8, -9, 19, 12, -4, -3, 0, 0, -24, -6, -13, -6, -11, 18, 2, -10, 3, 10, -5, 3, 12, 5, -17, 14, -23, 4, 4, 16, 1, -6, 0, 3, 46, 8, 33, -16, 14, -22, 58, -7, -30, -2, 105, -24, 43, 21, 16, 16, 14, 5, -5, -8, 55, -6, -21, -59, -13, -59, -6, -26, -5, 43, 48, 71, -42, 32, 18, -17, -43, -31, -13, -20, -58, 11, -6, -38, -1, 31, -3, 19, -69, 28, -3, 18, 100, 25, -5, 21, -16, -38, -23, 0, 22, -28, -12, 14, -2, -55, 5, 15, 1, -58, 12, -36, 60, 17, -13, 71, 82, 18, 17, -22, -25, -6, 28, 10, -5, -12, -63, -42, 3, -21, -23, 7, -3, 5, 4, -16, -15, -16, -11, 22, 10, -29, -17, 13, 27, 10, -16, 26, -8, 51, 15, 1, -8, -4, 17, -15, -44, 21, 13, 28, -6, 11, 7, 14, 18, 55, 11, 32, 14, -10, 30, 6, 11, 18, -11, -23, 11, 17, -11, -2, 5, 13, 127, -53, 35, -21, -8, -33, 16, 49, 81, -72, -26, 0, -40, -25, -41, 8, -15, -27, -9, -36, 42, -68, -6, -21, 20, 22, 25, 3, 54, 8, -11, -4, 8, 26, -5, -20, -31, -21, -43, 49, -105, -5, -44, -63, 28, 10, 48, -23, 67, 0, 3, -18, 4, 14, -8, 10, 43, -28, 43, 27, -33, 45, 14, -46, -6, -25, 39, 0, -8, -9, -21, -16, 0, 55, -9, -16, 16, -16, 93, 31, -47, 95, -8, 97, 35, -8, 29, -68, 64, 12, 32, 32, 2, 6, -69, 18, -15, 50, 0, -40, 42, -3, -28, 44, -9, -1, -18, -28, -25, 71, 60, 25, -13, -72, -35, -47, 11, -61, 19, 11, -83, 55, 10, -4, 19, 70, -93, 14, -8, -5, -6, -11, -127, 12, 2, -15, -38, 68, -30, 66, 23, 73, -17, -1, -80, 24, -14, 30, 33, 12, 32, -4, 27, 6, 35, -39, 19, 9, -77, 72, -78, -30, -46, -77, -62, 11, 4, -8, 18, -42, 34, -17, -81, 11, 68, 45, -13, 24, 47, 10, 25, -3, 24, 48, -14, 97, 17, 44, 33, -12, 47, -18, -97, -40, 40, 71, 25, 60, -2, -3, 31, 2, -56, -5, 40, -14, -40, 41, 30, 38, 45, -42, 15, -72, -10, -70, 13, 22, -83, 15, 5, -23, -19, 5, -72, -26, -7, 58, -8, -30, -2, -1, 89, 27, 18, -83, 36, 44, 2, 8, -55, -4, 38, 56, -10, 9, -15, -28, -38, 4, 17, 21, 9, 30, -25, 20, 10, 25, -32, -20, 2, -12, 52, 55, 43, 26, 56, 21, 7, -29, -18, 2, -8, -16, -25, 11, 14, -2, 35, -101, -55, 50, -11, -50, 74, -4, 43, -11, -43, 16, -105, -83, -18, -38, -23, 52, 10, -62, -96, 39, -8, 21, -36, 23, 22, 35, 9, -29, 12, 32, -5, 55, 31, -4, 4, 3, -12, 10, 28, 1, -18, -37, -12, -33, 4, 46, 87, 43, -121, -44, -57, 45, 13, 5, -27, 41, -20, -56, -14, 34, -86, -1, -20, -18, -34, -6, -8, 5, 127, 32, -31, -19, -15, -37, -6, 29, 16, 21, -52, 17, -24, -55, 8, 59, -2, -14, -20, 27, 74, 20, 16, 31, 22, -22, -37, 14, -22, -49, 24, 20, -25, -40, 21, 6, -21, 50, -28, -74, 58, -54, -27, -7, -13, -18, -24, -16, 45, -30, -41, 15, 34, 16, 30, -55, 18, 33, 47, 34, -55, -64, -52, 65, -24, -1, 24, 42, -13, 11, -3, 17, 12, -4, 4, 5, -57, 38, -21, 15, 36, 39, -29, 37, 33, -13, 46, -36, 10, -39, -34, 19, 9, 36, -33, -15, 10, 51, -11, -7, 86, -36, -7, 15, 10, -8, -16, -47, -7, 12, -45, 12, 15, 90, -42, 33, -15, -41, 10, -62, 56, -18, -4, 14, -43, -26, -62, -30, -7, 8, -11, -9, -48, 32, 15, 35, 25, 27, 17, 95, 11, -9, -10, 13, 5, -20, -11, -17, -22, 33, 27, 13, 2, -4, 68, -60, -1, -3, -7, 20, 3, -2, -19, 15, -27, -2, 28, 41, -68, 32, -20, -27, 54, -51, 15, 7, 35, 1, 19, -4, -32, -22, -28, 31, -5, 73, 36, 8, 34, 43, 19, -13, -83, -29, 81, -34, 19, -100, -60, -33, -6, 52, 32, 0, 11, -44, -29, 1, -13, -5, -16, 22, -20, -81, 15, 16, 7, -36, 23, -61, 7, 25, -15, -12, 19, 21, 20, -53, 34, -28, 13, 36, -13, 28, -33, 34, 45, 20, -13, 3, -6, -20, 29, 15, -50, 6, 7, -30, 1, 34, -43, -11, 110, 19, -9, -22, -6, 58, 27, 86, 17, -3, 10, 3, 23, 65, 3, 3, 49, -3, -32, -30, -3, 33, 55, -51, 37, -53, -21, -38, 87, 11, 36, 40, 42, 41, 22, 28, 24, -18, -103, -21, -32, -7, -92, 24, -69, -75, 10, -11, -29, 24, 6, 57, 127, 2, -76, 0, -82, 74, 44, -22, -10, -60, 44, 14, -30, 57, -1, -30, 16, 65, 121, 9, -42, 13, -33, -25, 64, 2, 18, -4, -18, -25, 1, -6, -34, -18, -32, -14, 25, 12, -8, -37, 5, 33, 6, -42, 8, -54, 30, -70, -20, -13, 66, 69, 20, -26, -22, -8, 20, -51, 21, -41, 24, 12, 59, 33, 34, 46, 26, -37, 59, -121, 37, 26, -41, 32, -19, -2, 22, 45, -39, -6, 31, 46, -6, -43, 35, 36, 25, 0, 16, 87, 15, -1, -20, 11, -3, -21, -11, -21, -10, -22, 49, -7, -41, 8, -21, 25, -37, -12, -34, -14, 32, 5, -12, 40, 47, -2, 28, -3, -5, -54, -4, -43, 2, 42, 15, -26, 13, -29, -11, -30, 19, 16, 15, -42, -25, -6, 15, 55, -13, -68, 9, -65, 19, -8, -7, 16, 70, 11, 31, -3, 21, -82, 47, 20, 48, -42, -15, -19, 20, 25, -34, 0, -32, -21, -17, 16, 36, -4, 22, 1, 17, 66, -28, -22, 25, 47, -61, 24, 18, -87, -25, 3, 2, 34, 36, -67, -35, 28, 1, 15, 38, -97, 3, -16, -21, -34, -1, -25, -16, -21, -4, 15, -10, 5, 24, -27, -17, -30, 48, -42, 0, -21, 4, 41, 74, 30, -34, -91, 38, -4, -20, 44, -3, 14, -3, 31, 30, 59, 31, -8, -17, 22, 3, 8, 20, 19, -3, -1, 38, -31, -20, -38, 5, 10, 45, 2, -46, -29, -70, 20, -19, 60, 46, -107, -9, 11, -10, -11, 15, -60, 8, -5, 42, 0, 60, -84, -7, 38, 13, 4, 18, 16, -85, -44, -5, -83, 57, 13, 16, 0, 23, -38, 0, -65, 11, 8, -14, -58, -65, 19, -22, 53, -63, -4, 5, 44, -17, 14, 41, -40, 42, 14, -58, 63, 32, -36, -3, -51, 77, 12, -10, 47, 31, 37, 9, -44, 15, -94, 10, 20, 127, 4, -59, -22, 26, -15, 68, 45, -1, 24, 34, 56, 22, -3, -29, 26, 0, 19, -1, 30, 1, 29, 18, 10, -13, 4, 54, -30, -46, -106, -62, -57, 10, -34, -27, 5, 36, 3, 72, 12, 40, -25, -28, 65, -45, 13, -72, 53, -43, 26, 51, 64, 27, -39, -11, 15, -11, 27, 3, 4, -14, 40, 26, -15, 4, 3, 72, -1, -15, 38, -14, 83, -6, -2, -14, 25, 78, -46, -37, -24, 7, 24, -1, 39, 0, -21, 33, 13, 49, 16, -19, -22, 46, -1, -17, 22, -19, 34, 7, 33, 46, 38, 11, 35, -23, 25, -3, -43, -5, -40, -46, -5, 28, 2, 76, 18, 14, -6, -1, 20, -47, 29, 57, -37, 18, -10, -39, 24, 5, 0, -3, 4, 52, 55, -39, -43, 1, 18, 88, 29, -33, 11, 27, 45, 0, 23, 12, 7, 13, -45, -73, -23, -26, 54, -9, -61, -10, 4, -34, 44, 42, -20, -15, -26, 18, 28, -23, 5, 41, 32, 10, 8, -45, -43, 4, -2, 6, -12, 41, -16, -25, 14, -27, -9, 28, 34, 16, 10, -12, 113, 6, 16, -5, 17, -63, 12, -50, 1, 30, -36, -20, 38, -66, 31, -16, 12, 15, 2, 4, -47, -24, 10, 70, -29, 54, 3, -55, 2, -5, 1, 16, -32, -63, 77, 20, 45, 49, -57, -87, -5, 20, 5, 41, -12, -29, -18, -44, 12, -15, 24, -32, 15, 27, -33, 1, 12, 7, -31, 47, -27, 23, 51, -19, 22, -5, 1, 60, 22, -79, -21, -10, 23, -20, 110, -11, 14, -8, -44, -19, 4, -68, -127, -60, -40, 27, 29, -24, -18, 35, 16, -5, 14, -50, 48, -15, 23, 70, -8, 4, -9, 1, 1, -19, 23, 27, 28, -28, 8, 7, -5, -13, -2, 42, 0, 8, -5, -33, 4, -23, -45, 41, 7, 8, 15, 8, 66, 54, 29, -51, 22, 9, -8, -35, 13, 22, 47, -29, -8, -46, -30, 35, -11, -11, -3, 16, -18, -24, 26, 37, 32, 22, -111, -8, -3, 32, 2, -8, -17, -9, -46, -19, 43, 22, 30, -33, 15, -79, 40, 41, 47, 15, -5, -12, 29, 23, 18, 12, -104, -3, 91, -54, -57, -6, 12, -3, 31, 6, 36, 63, 51, -26, 22, 30, -22, 18, 26, 43, -12, -30, 9, -94, 33, 17, 21, 27, 32, 10, 57, -33, -5, 42, 66, 31, 32, -3, 30, -11, 31, -17, 1, -47, 29, -13, 23, -17, 33, -1, 4, 39, -18, -13, 28, 41, -33, 19, 17, 36, -32, -10, -75, 23, 3, 18, -44, 65, -10, 83, 47, -53, -4, -12, 36, 28, 7, -30, -9, 22, 90, -13, 9, -34, 70, -2, -14, 33, 8, 33, 16, -48, 106, 48, -61, 89, -39, 23, 63, 51, 36, -25, 10, -20, 5, -13, 21, 7, 34, 7, -67, -4, 67, -21, 41, -10, 16, -17, -37, 3, 30, 53, -78, 84, 13, 22, -12, 6, 40, 76, -5, 48, -2, 7, 22, 27, -54, 6, 4, -34, 0, -32, 23, -22, -60, 9, -13, 52, 40, 12, -18, 15, -127, -5, 26, 11, -36, -68, 113, -10, -43, 12, -89, -30, 15, 3, -46, -81, -15, 17, 21, -8, 13, -16, 14, 4, 25, -4, -25, -122, 19, 59, -74, 11, 27, -26, 38, -7, -40, -50, -29, -17, -16, -25, -35, 21, -11, 9, 34, 70, 74, 67, 0, -10, 68, -8, 9, 21, -21, -25, 10, -11, 4, -20, 10, 5, -9, -22, 70, 11, 32, -57, -14, 36, -67, 4, 75, 21, -69, 11, -24, -23, -39, 20, 34, 45, -9, -52, 8, 53, 55, -68, 31, 21, -51, 23, -78, 70, 87, -32, 26, 62, -34, 28, 8, -2, -23, 20, 9, 24, 5, 20, 27, 19, -101, -14, -65, -7, 7, -32, 20, 7, -18, 43, 22, -25, 32, -44, -48, -3, -29, -19, 7, 21, -88, 40, -29, 26, -12, -16, 5, -126, -15, -69, 38, 6, 22, -57, -20, -39, -13, -4, -10, -2, 45, -51, -19, -18, 27, -4, -37, 77, -49, -16, 38, -15, -26, 39, 15, -36, 19, 43, 11, 78, -12, -11, -33, -15, -41, -15, 47, 1, 21, 3, -17, 24, 20, 41, -4, -11, 25, 20, 6, -82, 51, -31, 64, -12, -64, 13, 29, 50, -13, -33, -29, -18, 22, -60, 31, 81, 2, -127, -40, 3, -11, -13, -41, 31, -44, 4, 6, -10, 11, -31, -18, 12, 3, -30, -14, -9, 76, 20, 13, -48, -48, 22, 77, 0, 14, -31, 2, 61, -12, 12, 15, -40, -21, -3, 24, -11, -99, 102, 26, 24, 29, 10, 7, -32, 16, 11, 16, -12, 4, -30, -43, -67, 30, -24, 2, -4, 12, -3, -24, 21, -53, 19, 80, -17, -36, 24, 0, 93, -16, 29, 77, -4, -59, -57, -16, 34, 35, 15, 31, 93, -69, -19, 20, -29, -29, 33, -65, 22, 39, 35, -74, 6, -13, -18, -9, -4, 9, -27, 11, 16, -33, 4, -12, 36, 5, -2, -9, -10, -38, 18, -16, 61, 40, 8, 55, 22, -45, -34, -19, -21, -35, -18, 6, -34, 7, -17, -14, -21, -32, 32, -34, -16, 20, 12, 33, -9, -24, 4, 9, 70, 52, -15, 9, -35, 4, 24, 14, 10, -40, 29, -20, -16, -13, 44, -10, 55, -35, -19, 33, -4, -29, 16, 9, -20, -3, 73, 1, 25, -27, -20, -4, 44, 21, -28, 18, -10, -23, 2, 39, 41, 44, 4, 16, -18, -12, 21, 3, -56, 19, 28, 3, 50, 6, -25, 19, -21, -21, 48, 53, 30, -51, -15, -30, -25, -29, -24, 37, 46, -54, 30, -27, -127, 10, 42, -47, 63, -6, -86, 41, -32, 67, -23, -41, -13, 6, 22, -43, 30, -54, 46, 1, 34, -26, 9, 21, -4, -30, 35, -85, -15, -13, 10, 32, 35, -44, -32, 28, 35, -19, 42, 24, -9, 9, 61, -7, 4, -53, -29, -11, -40, -51, -21, 13, 29, 7, -1, -23, -6, 33, -6, 25, -38, 1, 28, 50, -29, -50, 13, 8, -74, -1, 32, -31, -65, -29, -3, 57, -39, -11, 41, -67, -19, 6, 24, 15, -19, -28, -15, 1, -32, 109, -12, 21, 1, -1, 25, -36, 56, 43, -5, 25, -19, -54, -16, -49, 29, 14, 32, -4, -5, -56, 18, 5, -3, 17, 9, 13, -15, -60, -53, -68, -20, -11, 40, 18, -28, -12, 14, 0, -17, 0, -5, 4, -43, 80, -16, 0, 31, -41, -21, 22, 60, 11, 11, 7, 67, -6, -1, -15, -28, -7, -7, -33, -20, -49, -13, -26, -5, -21, 7, -30, 22, 23, -2, 7, 24, 46, -41, 25, 41, -22, -73, -13, 8, -28, -27, 13, 39, -59, 36, -3, 5, 37, -47, -22, -38, -61, -40, 5, 23, 2, 6, -19, 25, 8, -25, 0, 41, 21, 10, 10, -6, -95, 12, -51, 7, 8, -3, 38, 16, -10, 5, -2, -17, 37, 19, -21, 13, 0, 53, 22, -22, 2, 9, 20, 34, -21, 31, 12, -42, -24, -85, -40, -41, -1, 109, 23, 5, -1, 34, -17, -14, -6, 47, 16, -126, -17, -18, -33, -75, -19, -11, -97, -8, 26, -22, 4, -4, -7, 5, 41, 3, -7, -37, -10, -20, 4, -26, -104, -29, 25, -9, 10, 22, -27, -26, -19, -30, -6, -2, -24, 0, 18, 4, 11, -9, 0, -19, -13, 11, 15, -5, -11, 4, -17, 43, -12, -40, -1, -1, 31, -88, 75, -38, 12, -84, 26, -7, 23, -8, -21, -3, -33, -9, 14, -5, 9, 21, -5, 9, -11, -5, 8, -15, 11, -33, -22, 24, -4, 13, -127, 10, 80, 23, 32, 2, -51, 110, -2, 11, -97, -8, 2, 7, 80, 33, -33, -18, -24, -17, -38, -22, 28, -4, -30, -23, 38, 46, 41, -30, -47, -48, -15, 28, -31, 11, -28, 19, 12, -11, 32, -47, 1, 16, 2, 30, -28, 5, -18, 49, -48, -63, -2, -58, 18, 28, -45, 4, -37, 50, -32, -6, -75, -35, -30, 6, 1, 31, -29, -124, -13, 4, -74, -10, -19, -76, -19, 14, -58, -9, 63, -43, -96, 25, 3, 111, -17, -33, 31, 10, -35, 12, 39, -19, -46, 18, -58, -24, -44, -127, -22, 29, 3, 10, -76, -6, 6, 10, -30, 7, 43, 12, -10, -48, -22, -13, 4, 1, 30, -40, 22, 55, 5, -46, -11, -6, -8, 30, -1, 4, -51, -19, -53, -45, 33, -34, -28, -7, 10, -10, -37, 25, 69, -20, 16, 27, -84, 8, -7, 71, -23, -94, 83, -5, -110, 6, -8, -26, -33, -37, -2, -35, 0, -8, 13, 32, -58, 17, -45, -4, -28, -88, -29, 13, -28, 42, 5, -49, -9, -78, -6, -36, 11, 56, -6, -10, 6, 6, -14, -20, -12, 15, 55, 0, -20, -19, -82, -11, -12, 5, 69, 109, 7, -4, -14, 26, 3, 21, 19, 39, -41, 36, -45, 37, 106, 16, 53, 39, 29, -99, 12, -35, 15, 30, 30, 18, 4, -59, -30, 2, -7, -21, 23, -9, 16, -40, 26, 36, -22, -14, -7, -70, -37, 124, 30, -40, 43, -75, -8, -11, -50, -17, 24, -39, 1};

float bias_raw[80]={-4.36494255065918, -6.076842784881592, 0.11005175858736038, 0.9736478924751282, -19.370351791381836, 8.843170166015625, -12.026487350463867, 4.469766139984131, 7.378990650177002, 4.444785118103027, -22.038246154785156, 4.234964370727539, -21.602102279663086, -9.25059986114502, 9.315537452697754, 8.281020164489746, 2.4638125896453857, 3.2741928100585938, 6.750655651092529, 7.61708927154541, -8.223211288452148, -8.122808456420898, 7.785109996795654, -9.578559875488281, -22.074182510375977, -1.5499649047851562, -6.72923469543457, 20.098081588745117, -2.7734673023223877, -0.23419620096683502, -2.4730050563812256, -20.984344482421875, 0.9359406232833862, 0.9721376895904541, 1.8615342378616333, 2.2060463428497314, -3.802058458328247, 10.502532958984375, -8.722298622131348, 11.982738494873047, -6.188943386077881, -2.009554147720337, 5.5651092529296875, 3.1219122409820557, 23.087499618530273, -9.556681632995605, -1.2507953643798828, 22.004587173461914, -0.5845964550971985, 5.899069309234619, -4.7720112800598145, 5.534934043884277, 3.4016971588134766, 5.263054847717285, 2.695319414138794, 17.39065933227539, 16.26097869873047, 15.241707801818848, 21.207780838012695, 3.942270040512085, 2.3270151615142822, 4.6324238777160645, 10.77005386352539, 14.258971214294434, 1.3830540180206299, 22.282047271728516, -2.7834486961364746, -4.5646233558654785, 5.560689926147461, -13.361109733581543, 13.143871307373047, 15.925175666809082, -11.437155723571777, -0.9256466031074524, -12.15784740447998, -0.2722598612308502, 6.709197044372559, 1.1748762130737305, 12.495205879211426, 8.633337020874023};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActNone;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={80,1,1,240};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={80};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteInt8;
const TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;
// const float scales_filter=;
// const int32_t zero_points_filter=;
// const float scales_bias=;
// const int32_t zero_points_bias=;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void ExtractConvParams(TfLitePadding padding, int stride_width, int stride_height, 
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteConvParams* data_params) {
  // TfLiteConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params,
                       char* tensor_data, TfLiteAffineQuantization* quant_struct,
                       size_t bytes_size, TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  const float* input_data = GetTensorData<float>(input);
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  const bool need_non_dilated_im2col =
      params->stride_width != 1 || params->stride_height != 1 ||
      filter->dims->data[2] != 1 || filter->dims->data[1] != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // std::cout << "codes runs here #-1" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #-2" << std::endl;
  bool has_bias = false;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));
  // TfLiteTensor* filter;
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  auto filter_input_channel = filter->dims->data[3];
  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;
  // std::cout << "codes runs here #-3" << std::endl;
  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }
  // std::cout << "codes runs here #-4" << std::endl;
  const TfLiteTensor* bias = nullptr;

  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.
  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }
  // std::cout << "codes runs here #-5" << std::endl;
  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8));

  if (is_hybrid && filter->type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  // std::cout << "codes runs here #-6" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  data->supports_multithreaded_kernel =
      (kernel_type == kMultithreadOptimized) &&
      (context->recommended_num_threads != 1) && !is_hybrid &&
      (params->dilation_width_factor == 1) &&
      (params->dilation_height_factor == 1) &&
      (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);

  int channels_in = filter->dims->data[3];
  int channels_out = filter->dims->data[0];
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  // std::cout << "codes runs here #-7" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #-8" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    // std::cout << "affine_quantization->scale->size: " << affine_quantization->scale->size << std::endl;
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #-9" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter->type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }

  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }

  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #-10" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }

    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }

    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #-11" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  // std::cout << "codes runs here #4" << std::endl;
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }
  // std::cout << "codes runs here #5" << std::endl;
  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }
  // std::cout << "codes runs here #6" << std::endl;
  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // std::cout << "codes runs here #7" << std::endl;
      const float* filter_data;
      if (data->need_hwcn_weights) {
        filter_data = GetTensorData<float>(hwcn_weights);
      } else {
        filter_data = GetTensorData<float>(filter);
      }
      // int index;
      // for (index = 0; index < 432; index++){
      //   // std::cout << "filter_data[" << index << "] = " << filter_data[index] << std::endl;
      //   std::cout << filter_data[index] << ", ";
      // }
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(filter), filter_data, GetTensorShape(bias),
          GetTensorData<float>(bias), GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = params->stride_width;
      op_params.stride_height = params->stride_height;
      op_params.dilation_width_factor = params->dilation_width_factor;
      op_params.dilation_height_factor = params->dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #0" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #1" << std::endl;
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;

  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data), 
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;
  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  // std::cout << "codes runs here #10" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_wpjdeq_REF() {
  static TfLiteRegistration r = {wpjdeq::Init, wpjdeq::Free,
                                 wpjdeq::Prepare<wpjdeq::kReference>,
                                 wpjdeq::Eval<wpjdeq::kReference>};
  return &r;
}

TfLiteRegistration* Register_wpjdeq_GENERIC_OPT() {
  static TfLiteRegistration r = {wpjdeq::Init, wpjdeq::Free,
                                 wpjdeq::Prepare<wpjdeq::kGenericOptimized>,
                                 wpjdeq::Eval<wpjdeq::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_wpjdeq_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {wpjdeq::Init, wpjdeq::Free,
                                 wpjdeq::Prepare<wpjdeq::kMultithreadOptimized>,
                                 wpjdeq::Eval<wpjdeq::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_wpjdeq_CBLAS_OPT() {
//   static TfLiteRegistration r = {wpjdeq::Init, wpjdeq::Free,
//                                  wpjdeq::Prepare<wpjdeq::kCblasOptimized>,
//                                  wpjdeq::Eval<wpjdeq::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_wpjdeq() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_wpjdeq_MULTITHREADED_OPT();
#else
  return Register_wpjdeq_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
