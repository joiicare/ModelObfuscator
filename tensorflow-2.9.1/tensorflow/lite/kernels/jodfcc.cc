/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h"

#include <stddef.h>
#include <stdint.h>
#include <vector>

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/optimized/cpu_check.h"
#include "tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h"
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h"
#include "tensorflow/lite/kernels/internal/optimized/neon_check.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/internal/types.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"

namespace tflite {
namespace ops {
namespace custom {
namespace jodfcc {

constexpr int kInputTensor = 0;
constexpr int kFilterTensor = 1;
constexpr int kBiasTensor = 2;
constexpr int kOutputTensor = 0;

// This file has three implementation of DepthwiseConv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  kNeonOptimized,
};

const int kTensorNotAllocated = -1;

int8_t filter_r   aw[10368]={11, 18, -4, -13, 24, 58, 29, 55, -23, -18, 4, 11, 32, 10, -2, -12, 32, 2, 26, 6, 0, 44, -4, 18, 24, 13, -5, 23, 12, 5, 4, 11, 2, 17, 25, -16, -23, 12, 37, -6, -6, 19, 5, 14, 71, -24, 51, 15, 10, 11, 56, 40, -12, -64, -1, 12, 35, 47, 10, 3, 61, 0, -4, 0, -96, 11, 5, 22, 22, 5, 5, -42, 25, 9, -15, -20, 54, -24, -62, -81, 14, -23, 14, -5, 12, 21, 14, 28, -23, 37, 26, 5, 9, 13, -9, 3, 12, 66, -70, -5, -70, -27, 12, 77, -14, -5, 25, -33, 0, 29, 18, 14, 40, -56, 1, -8, -12, -3, 12, 7, 8, -4, 9, 40, 3, -6, 34, 12, 36, 36, 9, -69, 20, 4, 26, 53, 17, 36, 11, -14, 28, 9, -35, -5, 17, -7, -38, 26, 13, 19, 39, 35, 66, 13, 92, 30, 18, -9, 6, -6, 3, 16, -22, -7, -24, -28, -31, 14, 5, -9, 122, 1, 83, 1, 13, 15, 13, -7, 11, -61, 93, 6, 4, 21, 21, -27, 10, 16, -3, 55, 20, -23, -52, 42, 13, 17, 3, -1, 29, 20, 97, -8, 6, -22, 8, 10, -12, 0, 77, 2, 25, 0, 15, -7, -7, 36, -19, 12, 39, 2, -17, 19, 33, 16, -7, 23, -6, -15, 8, -19, 50, 36, -12, 18, 32, 4, 51, 19, 42, -13, 5, 5, 9, 13, 10, 45, 18, 9, 52, -23, 6, 17, 10, 29, 26, 41, 23, 27, -14, -20, 8, -24, -48, 32, -15, 5, 34, -12, 12, -22, -19, 54, 11, 7, -18, -2, -3, -53, 13, 27, 22, 5, 12, 30, 104, 20, -4, 15, 9, 34, 6, -7, -56, -24, -4, 26, 45, 27, -4, 12, 15, 10, 42, 28, -66, 50, -1, 13, -3, 26, 20, 23, 14, -31, 46, 12, 4, 21, 23, 83, -5, 10, 4, -23, -12, -10, 6, -3, -24, -42, 2, 8, 30, 15, 12, 66, -3, -6, 17, 20, 3, 17, 19, 26, 4, -53, 77, 17, 18, 14, 18, -5, 23, 3, -9, 30, 2, 17, 35, 10, -12, -4, 6, 11, -9, 12, -1, 46, -17, -78, 5, 47, 23, 37, 54, -44, -24, 14, 0, 12, 14, 17, 35, 35, -8, 17, 34, 0, 2, 7, 0, -70, 4, 45, 39, -10, 0, -2, 3, 6, 24, 11, -51, 49, 27, -12, -58, 13, -1, -6, 0, 26, 6, 2, -11, 34, 14, 40, 3, 27, 86, -21, 110, 15, -5, -9, 7, 10, -11, 15, 1, -8, 27, 21, 11, 1, -12, -32, 23, -10, 20, -8, 17, 11, 10, 2, 33, 29, 4, -68, 38, 13, 24, 45, 32, 47, 11, 41, 42, 23, 20, -29, 25, 16, 17, 15, 15, 7, 23, 12, 39, 28, 28, 50, -38, 10, -5, 21, -50, 12, 41, 11, 35, 28, -6, 94, 13, 28, -8, 2, 11, 25, -10, 19, -2, 3, -19, 10, -49, -32, 22, 39, 21, -21, -80, -77, 10, -18, -24, -32, -120, 46, 11, 24, -51, -5, 23, -9, 78, 6, 50, -22, 9, 14, 15, 28, 3, 12, 29, 43, -8, 12, 21, 17, 19, 15, 73, 11, -12, 11, -9, 16, 9, 25, 9, 32, 1, -19, 13, 7, 34, 42, -4, 13, 20, 13, 92, 48, 29, 66, -51, 0, -24, 33, -7, 73, -64, -46, 5, 59, 18, 11, 15, 17, 92, 36, -30, -6, -1, 18, 19, 17, 4, 19, 0, -14, -34, 15, 39, 34, -52, -25, 20, 4, 27, 19, -5, 15, 12, 21, -21, 62, 10, 27, -5, -12, 27, -12, 25, 3, -70, 21, -7, 28, 9, 38, -17, -27, -43, -2, 21, 1, -24, 11, 48, -9, 15, 2, -13, 10, 15, 36, 4, 12, 12, 97, 18, 28, 21, 12, 6, 35, 28, 9, -3, 44, 16, -15, 3, -3, -15, -19, -23, 14, 4, 33, -4, -2, -17, 119, 0, -32, 3, 12, 11, 12, 37, 32, -4, 20, 33, 4, 35, -3, -59, -2, 35, 3, 57, 23, -12, 31, 6, 2, 2, -23, -15, -4, 14, 86, -4, 33, 28, -24, 27, 19, 22, -8, 31, 11, 3, 79, 18, 24, 17, 28, 1, 30, -24, 17, 8, -84, -5, 13, 31, 38, 3, -20, 12, -17, -10, 11, -34, 9, -4, 6, 10, 22, -2, 18, 53, 5, -34, 85, 51, -8, 27, 80, 31, 27, 28, -7, 21, 2, -2, 37, 85, 10, 15, 43, 12, 31, -22, 26, -21, 21, -55, -13, 23, 6, -71, 47, 18, 9, -18, 1, 16, -7, 33, 6, -28, 54, 13, 19, 60, -11, -9, 29, 3, 30, 5, 1, 12, 22, 32, -18, 11, 76, 66, 37, -25, 14, 14, -11, -26, -27, -4, 15, -1, -9, 17, 48, 39, 39, 11, -5, 18, 17, -61, 15, 12, 53, 6, 69, -7, 5, 22, 2, 2, 2, 15, 52, 65, 39, -6, -1, -28, 37, 15, 17, 28, -14, 42, 34, 17, 17, 22, 26, 5, 0, 22, 10, 25, 32, 64, -26, 18, 12, 92, 0, 12, 22, -14, 6, 13, 7, 14, -66, 25, 25, 1, 15, 16, -26, 14, 28, 22, -34, -4, 0, 1, 61, 28, -10, 21, 41, 17, 1, 4, 25, 6, -27, 13, -4, 20, -12, 17, -41, 29, -12, 22, -70, -2, -8, 58, 27, 26, -29, 46, 32, -4, -9, -69, 2, -5, 69, -13, 14, 45, 5, 4, 33, -23, 41, 16, -18, -13, -6, -52, 9, 26, 40, 4, 39, -49, 27, -44, 6, 3, 7, -3, 48, 46, 44, -35, 27, 4, -31, 0, -15, 24, -3, 35, 25, 3, 40, 24, 37, -9, 34, 20, 2, 40, 5, -20, 76, -7, 71, 67, -4, -19, 25, 28, 34, -20, 14, 13, 56, -5, -23, 15, 12, 18, 81, 89, -4, -2, -14, -71, 10, -29, 13, 16, -2, 21, 42, 3, -66, 56, -33, 17, 14, 1, -19, 33, 49, 30, -10, -84, 31, 21, 40, 18, 16, 23, -1, 10, -7, -1, 40, 8, -18, -1, 20, 16, 41, -26, 8, -9, 97, 0, 15, 21, 20, 41, 34, 83, 15, 43, 49, 99, 41, 23, 20, 20, 23, 6, 31, -2, -5, 38, 37, 9, 37, 13, -5, -34, 6, -21, 12, 0, 29, -9, -6, 2, 41, 8, -87, 12, -127, -50, 25, 5, -20, 32, 8, 22, -18, 28, 18, 18, 16, -12, 7, 22, 98, 33, 2, -58, -25, 2, 15, -14, 7, -36, 10, 30, 22, 48, 22, 44, 12, 47, -23, 14, 43, -9, 3, 14, 12, -13, 38, -22, 37, 12, 46, 19, -10, 16, -35, 35, 2, 15, 6, 17, -29, -12, 11, 31, -42, 5, 18, 7, -10, 5, 29, 19, 46, 5, 6, -1, -40, 25, 24, -1, 26, 43, -4, 16, 10, 18, -13, 46, -6, -10, 36, -4, 15, 102, 41, 6, 14, 56, 70, 59, 57, 75, 95, 83, 71, -63, 44, -45, 61, 115, 59, -76, 20, 68, 76, 76, 63, -63, 85, 56, 58, 65, 70, 70, 75, 73, 61, 58, 48, 62, 65, 53, 46, -43, -32, 74, 53, 65, 63, -29, -16, 127, -83, 68, 64, 70, -57, 110, 83, -102, -127, 63, -28, 70, 81, 76, -53, 104, -24, 56, -37, -109, -38, 68, -17, 65, 67, 53, -75, 67, -40, 49, -60, 127, -55, -96, -92, 0, 51, 59, -76, 63, 88, 64, 87, -83, 67, 81, 35, 70, 70, 82, 52, -44, 43, -112, 64, -102, -71, 61, 127, 62, 65, 119, -83, -22, 70, 50, 56, 66, -101, 64, -51, -48, -56, 64, 67, 81, -41, -40, 87, -39, 82, 86, 50, -11, 60, 45, -78, 67, 72, 68, 98, 38, 50, 57, 75, 82, 58, -76, 63, 89, 68, -91, 61, 42, 79, 60, 60, 127, 71, 116, 94, 56, -57, -37, -52, 64, 64, -62, -51, 117, -89, -90, 67, 50, 50, 113, -45, 92, 60, 82, -19, -37, 54, 68, -9, 127, 55, 101, 60, 64, -57, 49, 59, -60, 43, 60, 95, -71, 82, 66, 63, 54, 50, 80, 65, 127, 63, 67, -60, -25, 53, 47, 53, 127, -41, 77, 61, 59, -56, 80, 92, 73, 97, 76, 53, -30, 61, 72, -36, 75, 54, 76, 47, 59, -69, 104, 90, -71, -21, 73, 64, 103, 61, 76, -40, 90, 53, 77, 66, -44, 66, 44, 76, 75, 30, -95, 83, 90, 60, 61, 98, -3, 85, 47, 5, -26, -73, -51, 65, 58, 81, 63, 67, 67, -70, -58, 105, 72, 44, 42, 60, 72, -78, 64, 73, 81, 55, 59, 86, 38, 57, 81, -22, 86, 72, 78, 75, -54, 61, -53, -11, 119, 111, 53, 82, 40, 71, 94, 71, -15, 105, 65, 56, 49, 81, 82, 64, 127, -91, 89, 67, 57, -56, 66, 106, 25, 46, 79, -66, 107, -71, 66, -51, -66, -4, 73, 53, 95, 81, -35, 83, 67, 70, 93, 58, -24, 75, 65, 63, 42, -81, 127, 75, 74, -23, 72, 91, 64, 52, 67, 69, 59, 71, 49, 54, -89, 46, -19, 50, -51, 69, -45, 104, -65, -111, 127, 95, 57, 52, 127, -100, 34, 61, 63, -36, -31, 57, 80, 89, -58, 102, 59, -44, 69, -42, 62, -62, 65, 79, 95, 79, -44, 90, -49, 72, 81, 64, -99, 38, 67, 44, -41, 46, 42, 58, 65, 63, -49, 80, 30, 80, 77, 65, 67, 80, 127, -83, 127, -30, -40, 57, 76, -50, 76, 68, 51, -53, 55, 77, 64, 46, 53, 127, -12, -65, 62, 49, 78, -21, 62, 111, 60, 63, -24, -44, 108, 58, 70, 92, 53, 63, 61, 75, 53, 89, 71, -81, 76, 113, 72, -27, 57, 62, 73, 65, 58, -97, 80, 95, 34, 68, 52, 88, -113, 48, 90, 60, 87, 86, 60, 127, 40, 65, 49, 49, 74, 88, 47, 70, 80, 50, -44, 93, -104, -64, 61, 50, 92, -58, -105, 85, 71, -79, -78, -73, -126, 96, 109, 76, -82, -61, 26, 59, 127, 79, 127, -61, 70, 55, 54, 74, 76, 63, 62, 96, 52, 46, 76, 56, 66, -25, 127, 43, -64, 71, 52, 68, -39, 62, 64, 23, 66, -57, 67, 57, 66, 65, -40, 80, 66, 91, 127, 93, 70, 101, -116, 55, -97, 98, 57, 96, -45, -42, -34, 83, -33, 68, -39, 62, 127, 77, -84, 75, 70, 88, 84, 78, 77, 67, -50, 44, -68, -21, 75, 59, -91, -60, 70, -19, 86, 87, -23, 71, 56, 74, 43, 99, 89, 62, -43, -80, 79, 53, 86, -54, -62, 77, -42, -22, 70, 84, -59, -53, -56, 75, 62, 109, 13, 68, 123, -105, -24, 58, -3, 78, 75, 79, 73, -57, 55, 115, -5, 75, 68, 71, 75, 80, 55, 71, 56, 94, 72, 57, -38, -54, -50, 51, -68, 62, 53, 79, 69, -41, -69, 117, 53, -55, 73, 61, 63, 111, 76, 23, -30, 54, 96, 66, 106, 89, -104, -37, 72, 89, 104, 76, 58, 71, 53, 81, 65, -111, -54, 66, 70, 127, 61, 83, 93, 44, 69, 68, 69, -61, 57, 54, 54, 63, 71, 72, 70, 62, -49, 85, -82, -25, 64, -97, -41, 85, 85, 99, 51, -73, 66, 47, 32, 63, -72, 64, -58, -38, 62, -19, -37, 69, 72, 46, -116, 122, 119, -55, 51, 127, 86, 65, 86, 13, 82, 45, 50, 76, 105, 71, 47, 23, 64, 89, -63, 68, -66, 92, -92, -53, 32, 69, -127, 80, 74, -74, 21, 55, 58, -18, 88, 71, 56, 110, 58, 73, 91, -92, -61, 43, 55, 78, -40, 51, 68, 65, 74, -81, -68, 84, 96, 72, -71, 65, 51, -51, 8, 95, -38, 73, -37, 62, 49, 85, 97, 20, 51, 55, 59, -32, -84, 68, 74, 90, 38, 109, -63, 78, 60, 78, -36, 56, 65, 88, 100, 90, 84, 72, -75, 96, 77, 72, 80, -35, 86, 68, -27, 74, 55, 87, 67, 67, 82, 59, -26, -17, 81, 84, 56, 65, 54, 65, -47, 51, 68, 62, 45, 79, -46, -127, 67, 58, 59, 44, 97, -58, 69, 71, 43, 31, -54, 63, 40, 112, 81, 29, 121, 44, 71, -64, 55, 101, 58, -56, -16, 65, 75, -57, -23, -80, 82, -67, 47, -70, 60, 54, 101, 85, 66, -47, 117, 77, 69, -70, -60, -23, 35, 127, -51, 61, 75, -34, 82, 77, 94, 48, -29, -59, 29, 61, -50, 67, 56, 65, 51, 91, -70, 75, -76, 59, 23, 52, -32, 101, 89, 88, -2, 79, -35, -84, -42, -64, 61, 64, 78, 80, -63, 93, 50, 88, -43, 72, 73, 61, 82, 65, 55, 127, 28, 95, 84, 71, 37, 42, 91, 83, 59, 65, -9, 96, 68, -91, 58, -21, 76, 127, 93, -58, -48, 59, -28, 62, 71, 55, 65, 80, 105, 68, 68, -102, 86, 38, 65, 96, -58, -55, 73, 85, 79, 26, -127, 82, 67, 89, 71, 67, 68, 76, -40, 29, 54, 79, 57, -74, 68, -23, 76, 82, 83, 72, -72, 99, 66, 69, 82, 89, 98, 73, 119, -26, 68, 108, 127, 77, 81, 64, 95, 58, 68, 82, -47, 58, 69, 66, 50, 62, 81, 61, 36, -45, -66, 69, -70, 80, 47, 75, 72, 53, 49, -101, 68, -107, -69, 63, 95, -58, 71, -30, 99, 73, 85, 68, 84, 51, 75, 74, 66, 123, 96, 99, -60, -61, 75, 70, -80, 61, -84, 64, 61, 61, 35, 87, 84, 68, 99, 33, 59, 48, 49, 56, -29, 67, 68, 84, 80, 79, -26, 82, 71, 78, 59, -70, 57, 75, 93, 79, -16, 95, -58, 69, 59, -81, 64, 69, 63, 45, 73, 68, 76, 88, 61, 61, -28, -86, 52, 81, 55, 71, 77, 67, 102, -26, -27, 39, 99, -49, 60, 54, 37, 67, 127, 80, -48, -32, 12, 22, -3, -14, 22, 62, 35, 59, -24, -16, 2, 18, 33, 11, -2, -10, 30, 3, 21, 8, -2, 42, 2, 24, 22, 2, -9, 20, 13, 8, 8, 12, 1, 11, 27, -18, -23, 14, 36, -3, -5, 18, 8, 12, 70, -27, 48, 16, 9, 14, 55, 36, -11, -65, -2, 12, 33, 47, 9, 0, 58, 1, -6, 1, -100, 12, 6, 22, 24, 7, 4, -39, 22, 13, -16, -17, 52, -26, -58, -70, 15, -21, 13, -6, 11, 18, 18, 28, -27, 36, 29, 3, 11, 12, -10, 5, 11, 66, -80, -6, -69, -31, 17, 77, -15, -5, 26, -30, -2, 25, 16, 19, 39, -47, 4, -9, -11, -6, 19, 11, 12, -4, 9, 40, 3, -7, 34, 11, 35, 33, 10, -71, 19, 1, 30, 52, 13, 32, 11, -12, 27, 9, -37, -5, 16, -6, -42, 26, 13, 18, 37, 37, 65, 15, 90, 32, 18, -5, 8, -8, 4, 5, -23, -5, -20, -25, -39, 13, 4, -6, 124, -2, 95, 2, 15, 16, 12, -7, 12, -61, 90, 4, 3, 20, 19, -26, 16, 21, -3, 50, 22, -12, -48, 40, 11, 18, 7, -8, 28, 19, 98, -6, 3, -19, 7, 5, -14, -2, 78, 3, 23, 3, 13, -6, -2, 34, -19, 12, 45, -1, -15, 17, 34, 16, -1, 22, -1, -17, 4, -15, 50, 35, -15, 18, 27, 11, 51, 18, 45, -11, 5, 5, 10, 17, 11, 46, 24, 8, 48, -19, 9, 18, 6, 29, 26, 41, 22, 29, -14, -16, 6, -25, -54, 29, -16, 7, 32, -15, 16, -25, -17, 57, 14, 10, -13, 0, 2, -64, 15, 26, 19, 8, 15, 30, 100, 19, 1, 15, 9, 38, 9, -6, -50, -25, -1, 26, 40, 26, 1, 17, 15, 9, 42, 30, -72, 48, -1, 15, -4, 29, 17, 21, 37, -26, 43, 10, 1, 22, 24, 79, -3, 14, 8, -22, 0, -9, 6, 2, -23, -43, 7, 7, 26, 14, 10, 69, 0, 0, 18, 19, 6, 16, 20, 26, 3, -45, 83, 15, 18, 15, 16, -6, 24, 1, -7, 29, -1, 17, 37, 7, -12, -6, 6, 11, -12, 9, -1, 45, -18, -67, 9, 40, 19, 36, 56, -40, -26, 19, -5, 11, 12, 15, 36, 32, -11, 23, 34, -1, -5, 8, 1, -75, 1, 43, 38, -8, 1, 1, 3, 3, 22, 9, -51, 50, 28, -11, -62, 11, 0, -4, 3, 24, 3, 9, -9, 39, 20, 34, 3, 22, 82, -18, 111, 15, -6, -8, 7, 8, -16, 14, 5, -9, 27, 21, 14, 2, -5, -34, 22, -5, 17, -8, 19, 9, 9, 6, 31, 35, 3, -60, 40, 15, 24, 42, 31, 52, 12, 40, 45, 20, 20, -31, 21, 17, 17, 14, 10, 11, 20, 12, 43, 91, 30, 48, -45, 11, -3, 15, -50, 8, 43, 10, 38, 33, -7, 94, 15, 29, -15, 2, 23, 22, -9, 20, -6, 10, -17, 13, -47, -25, 24, 39, 21, -20, -73, 119, 13, -19, -22, -27, -120, 45, 10, 24, -43, -7, 24, -8, 75, 5, 47, -25, 16, 13, 16, 24, 4, 13, 33, 39, -8, 11, 20, 18, 22, 15, 77, 6, -7, 8, -8, 14, 11, 23, 10, 32, 1, -22, 12, 12, 33, 41, -5, 12, 14, 3, 92, 50, 33, 66, -48, -3, -20, 34, -8, 72, -68, -45, 8, 64, 19, 18, 13, 17, 90, 31, -30, -6, 4, 24, 18, 19, 2, 19, 1, -20, -39, 15, 40, 33, -46, -24, 15, 7, 26, 17, -4, 15, 14, 28, -23, 54, 5, 34, -2, -25, 26, -12, 25, 5, -65, 19, -5, 26, 8, 44, -21, -25, -47, -3, 27, 3, -21, 16, 51, -19, 15, 6, -13, 14, 19, 35, 12, 13, 16, 98, 18, 30, 24, 8, 8, 35, 29, 9, -3, 44, 17, -10, 4, -1, -15, -20, -22, 17, 5, 28, -2, 0, -15, 120, 1, -33, 3, 11, 14, 21, 34, -54, -1, 17, 33, 3, 34, 3, -58, -5, 35, 1, 56, 24, -12, 27, 9, 0, -1, -24, -20, -1, 19, 83, -1, 37, 29, -26, 26, 20, 26, -8, 35, 13, 1, 67, 16, 20, 14, 34, 4, 30, -27, 18, 8, -90, 0, 17, 33, 35, 1, -16, 16, -17, -11, 12, -27, 6, -9, 6, 7, 21, 0, 22, 49, 2, -39, 84, 49, -8, 32, 80, 37, 26, 33, -8, 20, 3, 2, 38, 86, 13, 16, 40, 16, 31, -19, 24, -21, 21, -42, -15, 29, 7, -73, 46, 14, 7, -18, 4, 16, -7, 33, 6, -28, 53, 9, 17, 66, -2, -5, 22, -5, 35, 5, -1, 9, 23, 29, -19, 10, 78, 63, 40, -22, 16, 23, -14, -25, -32, -6, 14, 0, -10, 17, 48, 51, 34, 13, -6, 21, 17, -58, 15, 8, 55, 8, 69, -9, 1, 18, -5, 1, 7, 20, 53, 62, 36, -5, 2, -27, 35, 14, 15, 26, -11, 33, 34, 16, 20, 19, 29, 5, 0, 24, 12, 25, 34, 64, -14, 14, 12, 95, 3, 8, 20, -9, 6, 13, 9, 14, -73, 24, 25, -1, 17, 15, -26, 17, 29, 29, -34, -6, -9, 1, 60, 29, -11, 23, 42, 16, 2, 11, 22, 6, -24, 15, 0, 17, -8, 19, -44, 27, -11, 14, -68, 0, -6, 56, 24, 23, -31, 38, 30, -2, -9, -73, 4, -8, 71, -13, 12, 45, 6, 6, 32, -25, 39, 17, -20, -11, -3, -45, 12, 25, 40, 8, 41, -52, 29, -44, 7, 6, 3, -3, 50, 46, 43, -36, 32, 1, -26, 0, -12, 23, -4, 35, 26, 5, 43, 25, 35, -7, 33, 19, 10, 36, 7, -19, 74, -6, 78, 73, -2, -20, 26, 31, 28, -20, 17, 12, 54, -4, -20, 10, 12, 22, 82, 102, -5, -4, -14, -70, 13, -28, 13, 17, 1, 18, 38, 3, -70, 57, -26, 19, 8, 0, -17, 30, 52, 31, -4, -83, 27, 24, 38, 18, 16, 24, -5, 10, -6, 2, 47, 11, -21, -3, 18, 20, 35, -26, 8, -14, 84, -3, 23, 17, 21, 39, 43, 87, 14, 43, 46, 95, 41, 24, 22, 20, 20, 6, 26, -2, -2, 42, 42, 8, 35, 16, -2, -35, 4, -19, 11, 6, 28, -7, -2, 2, 39, 9, -93, 13, -116, -41, 31, 5, -19, 30, 7, 23, -20, 29, 21, 13, 19, -10, 8, 24, 97, 37, 1, -61, -21, 0, 21, -10, 7, -36, 7, 30, 17, 50, 21, 43, 14, 52, -18, 12, 45, -4, 2, 14, 12, -9, 41, -24, 36, 13, 43, 21, -11, 22, -31, 33, -6, 14, 4, 17, -14, -14, 12, 28, -44, 6, 17, 8, -9, 2, 28, 19, 47, -2, 8, -3, -35, 24, 23, -1, 27, 42, -10, 6, 9, 17, -13, 47, -7, -13, 28, -3, 14, 104, 47, 8, 12, 71, 64, 76, 61, 76, 98, 45, 60, -76, 58, -37, 92, 106, 69, -105, 27, 108, 65, 84, 76, -72, 73, 53, 75, 73, 89, 74, 93, 68, 75, 64, 54, 68, 93, 62, 50, -67, -41, 95, 54, 83, 71, -43, -28, 66, -76, 83, 87, 87, -69, 102, 79, -98, -118, 59, -49, 72, 75, 68, -60, 116, -36, 83, -54, -23, -42, 63, -35, 64, 75, 67, -127, 74, -53, 53, -67, 60, -73, -93, -119, -13, 58, 48, -53, 63, 98, 62, 94, -123, 96, 84, 47, 69, 73, 66, 76, -37, -21, -95, 68, -96, -73, 85, 98, 75, 67, 127, -77, -56, 89, 107, 56, 86, -127, 71, -67, -81, -83, 62, 77, 73, -40, -40, 104, -59, 87, 81, 60, -33, 48, 49, 50, 89, 85, 81, 84, 41, 46, 66, 76, 73, 80, -68, 71, 105, 55, -113, 77, 56, 69, 78, 47, 88, 70, 97, 61, 61, -67, -42, -56, 70, 102, -65, -51, 122, -90, -99, 81, 54, 44, 105, -49, 88, 59, 71, -15, -64, 69, 66, 102, 75, 57, 105, 82, 69, -82, 65, 73, -62, 74, 65, 123, -75, 93, 70, 66, 65, 70, 75, 69, 39, 65, 61, -67, -34, 105, 79, 62, 61, -32, 84, 87, 43, -54, 64, 93, 81, 71, 88, 64, -39, 54, 71, -34, 79, 51, 83, 60, 76, -78, 79, 91, -79, -30, 75, 71, 92, 75, 83, -48, 58, 69, 78, 70, -30, 76, 63, 78, 79, 66, -102, 76, 78, 56, 84, 90, -51, 80, 50, 11, -40, -42, 57, 87, 71, 84, 102, 77, 55, -94, -37, 84, 84, 62, 80, 68, 78, -95, 78, 76, 88, 56, 75, 85, 127, 57, 117, -26, 86, 77, 78, 73, 23, 65, -52, -12, 78, 93, 78, 80, 53, 104, 86, 52, -22, 83, 75, 94, 60, 73, 84, 72, 123, -96, 71, 63, 68, -62, 76, 127, 48, 94, 82, -98, 93, -63, 78, -66, -78, 65, 87, 56, 77, 83, -35, 83, 64, 72, 115, 67, -13, 69, 58, 74, 95, -112, 126, 74, 67, -39, 77, 81, 73, 55, 71, 73, 60, 80, 47, 90, -88, 47, -14, 71, -84, 70, -77, 89, -71, -95, 28, 102, 63, 91, -27, -123, 92, 68, 67, -58, -30, 70, 72, 97, -53, 101, 78, -40, 72, -51, 86, 31, 80, 83, 76, 88, -62, 56, -39, 67, 99, 63, -125, 71, 94, 75, 40, 61, 78, 73, 62, 83, -53, 79, 26, 92, 85, 71, 58, 87, 83, -39, 45, -22, -41, 62, 80, -55, 80, 68, 50, -60, 72, 86, 75, 52, 59, 113, -31, -76, 71, 74, 86, -17, 64, 108, 76, 94, -46, 99, 94, 61, 51, 87, 65, 96, 69, 79, 76, 89, 75, -86, 76, 112, 74, -36, 69, 72, 74, 71, 83, 104, 111, 97, 65, 76, 72, 93, -121, 72, 107, 61, 104, 89, 62, 10, 80, 73, 58, 51, 73, 69, 91, 67, 80, 53, -74, 76, -118, -53, 76, 69, 77, -71, -127, 58, 86, -74, -78, -79, 11, 87, 127, 80, -92, -56, 23, 69, 81, 89, 61, -70, 78, 75, 61, 77, 84, 66, 74, 120, 55, 70, 72, 50, 106, -30, 89, 83, -66, 66, 52, 67, -53, 69, 55, 60, 82, -80, 71, 62, 82, 61, -48, 78, 77, 69, 102, 107, 70, -26, -114, 66, -103, 105, 63, 96, 52, 34, -47, 105, -41, 69, -39, 62, 53, 80, -99, 75, 82, 97, 85, 77, 70, 102, -59, 81, -78, -26, 93, 63, -90, -78, 64, -38, 80, 74, -28, 77, 64, 93, 54, 64, 31, 64, -61, 104, 72, 87, 95, -54, 54, 78, -46, -42, 79, 84, -75, -62, 41, 68, 48, 93, 76, 110, 85, 38, -34, 66, 73, 68, 79, 75, 95, -64, 75, 43, -23, 94, 84, 83, 70, 41, 59, 70, 70, 102, 84, 71, -60, -51, -67, 68, -85, 72, 54, 65, 68, -47, -42, -29, 58, -77, 78, 61, 80, 126, 74, 71, -63, 49, 85, 76, 78, 109, -127, -52, 64, 89, 101, 72, 67, 78, 65, 86, 64, -111, -55, 56, 82, 7, 61, 81, 105, 77, 88, 89, 74, -65, 83, 58, 85, 83, 88, 94, 67, 89, -46, 74, -103, -48, 68, -127, -40, 82, 76, 117, 67, -78, 65, 44, 23, 90, -94, 94, -62, -34, 65, -28, -41, 71, 63, 47, -109, 91, 75, -63, 109, 88, 75, 79, 57, 22, 92, 56, 78, 88, 78, 77, 52, -85, 68, 103, -77, 101, -77, 88, -79, -53, 48, 67, -66, 105, 81, -46, 31, 58, 69, -26, 85, 49, 90, 104, 73, 73, 80, -87, -85, 78, 84, 75, -49, 59, 67, 82, 76, -68, -56, 127, 111, 84, -83, 62, 83, -65, 65, 94, -56, 88, -46, 66, 60, 80, 90, 46, 64, 67, 63, -26, -111, 72, 80, 84, 60, 103, -56, 76, 81, 78, -48, 72, 95, 74, 121, 112, 80, 75, -74, 78, 72, 69, 98, -70, 97, 74, -39, 77, 58, 95, 81, 71, 86, 66, -33, -28, 72, 63, 95, 84, 127, 88, -52, 93, 79, 58, 61, 100, -38, 18, 77, 69, 63, 85, 72, -59, 70, 69, 51, 44, -50, 62, 50, 73, 76, 107, 100, 29, 73, -45, 71, 83, 61, -50, -19, 64, 82, -72, -50, -79, 78, -51, 62, -118, 69, 71, 108, 57, 75, 29, 55, 68, 70, -53, 28, -29, 23, 57, -68, 76, 82, -51, 78, 92, 93, 108, -43, -90, 37, 73, 70, 91, 62, 64, 55, 91, -109, 75, -75, 69, 22, 67, -53, 87, 91, 127, 55, 85, -44, -81, -43, -53, 70, 83, 80, 68, -41, 91, 64, 68, -67, 69, 67, 54, 111, 72, 43, 67, 24, 127, 86, 76, 68, 47, 71, 77, 73, 84, -37, 107, 64, -77, 64, -21, 77, 66, 127, -60, -61, 70, 72, 54, 82, 74, 81, 101, 108, 87, 64, -96, 90, 66, 65, 95, -61, -86, 79, 95, 70, 79, -45, 77, 70, 84, 70, 67, 83, 71, -43, 36, 69, 75, 58, -51, 81, -30, 61, 62, 84, 75, -85, 119, 68, 79, 73, 70, 97, 78, 96, -43, 91, 83, 6, 84, 87, 69, 98, 63, 85, 76, -63, 72, 91, 81, 55, 79, 75, 62, 68, -42, -70, 69, -71, 99, 51, 81, 72, 61, 56, -120, 57, -104, -102, 84, 84, -45, 83, -33, 77, 65, 69, 82, 100, 43, 85, 80, 85, 111, 126, 86, 17, -83, 84, 72, -82, 57, -74, 82, 78, 56, -46, 88, 63, 80, 97, 86, 71, 80, 78, 77, -36, 64, 62, 114, 88, 100, -26, 76, 72, 80, 90, -66, 67, 61, 84, 81, -24, 77, -56, 57, 70, -94, 72, 95, 62, 70, 72, 95, 68, 95, 73, 69, -46, 17, 80, 74, 68, 79, 77, 77, 78, -32, -22, 49, 109, -65, 64, 83, 46, 82, 38, 79, -53, -34, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, -127, 127, 127, 127, -127, 127, 123, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, 127, 127, -127, -127, 77, -127, 127, 127, 127, -127, 127, 127, -127, -116, 127, -127, 127, 127, 127, -127, 127, -127, 127, -127, -22, -127, 127, -127, 127, 127, 127, -114, 127, -127, 127, -127, 42, -127, -127, -127, -127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, -127, -127, 127, -127, -127, 127, 96, 127, 127, -117, -127, -127, 127, 127, 127, 127, -88, 127, -127, -127, -127, 127, 127, 127, -127, -127, 127, -127, 127, 127, 127, -127, 127, 127, 66, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 118, 127, 127, 127, 127, -127, -127, -127, 127, 127, -127, -127, 127, -127, -127, 127, 127, 127, -122, -127, 127, 127, 127, -127, -127, 127, 127, 123, 103, 127, 127, 127, 127, -127, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 43, 127, 127, -127, -127, 127, 127, 127, 76, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, -127, 127, 127, -127, 127, 127, 127, 127, 127, -127, 127, 127, -127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, -127, -127, 48, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, 127, 127, 127, 127, -124, 127, 127, 127, 127, 127, 127, -110, 127, 127, -127, 127, 127, 127, 127, 33, 127, -127, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 116, -127, 127, 127, 127, -127, 127, 114, 84, 127, 127, -127, 127, -127, 127, -127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 117, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, -127, 127, -127, 127, -127, 127, -127, -127, 75, 127, 127, 127, -9, -117, 127, 127, 127, -127, -127, 127, 127, 127, -127, 114, 127, -127, 127, -127, 127, 28, 127, 127, 127, 127, -127, 127, -127, 127, 127, 127, -113, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 125, -127, 56, -127, -127, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 121, -127, -127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 101, 127, 127, 127, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 32, 127, 127, 127, 127, 127, 127, -112, 127, 127, 127, 127, 127, 127, -32, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, -127, -127, 127, 127, 127, -127, -93, 127, 127, -127, -127, -127, 12, 127, 104, 127, -127, -127, 127, 127, 111, 127, 110, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 124, 123, -127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, -42, -115, 127, -127, 120, 127, 127, 57, 53, -127, 70, -127, 127, -127, 127, -3, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, 127, -127, -127, 127, 127, -127, -127, 127, -127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, 84, 127, 127, 127, -127, 23, 127, -127, -127, 127, 127, -127, -127, 52, 127, 127, 127, 127, 127, 127, -114, -127, 127, 102, 127, 127, 127, 127, -127, 127, 70, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, -127, -127, 127, -127, 127, 127, 127, 127, -127, -127, -127, 127, -127, 127, 127, 127, 117, 127, 18, -127, 127, 127, 127, 127, 127, -109, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, 36, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, -127, -127, 127, -71, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, -127, -127, 127, -127, -127, 127, 127, 127, -109, 127, 127, -127, 127, 66, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -119, 127, 127, -127, 127, -127, 127, -127, -127, 127, 127, -115, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, 127, -127, 127, 127, 127, 127, -127, -127, 113, 127, 127, -127, 127, 127, -127, 127, 127, -127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, -46, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, 127, 127, 84, 127, -127, 127, 127, 127, 127, 127, -127, -32, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, -127, -127, 127, 127, -127, -127, -127, 127, -127, 127, -121, 127, 127, 127, 127, 127, 68, 127, 127, 127, -127, 47, -127, 127, 89, -127, 127, 127, -127, 127, 127, 127, 127, -127, -127, 127, 127, 77, 127, 127, 127, 127, 127, -127, 127, -127, 127, 127, 127, -127, 127, 127, 124, 127, 127, -127, -127, -127, -127, 127, 127, 127, 127, -127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 88, 127, -31, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, -127, 127, -127, 127, 87, 123, -127, -127, 127, 85, 127, 127, 127, 127, 127, 127, 127, 127, -108, 127, 127, 127, 127, -127, -127, 127, 127, 127, 127, -84, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, -127, 127, 127, 127, 127, -127, 123, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, -33, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, -127, 127, 127, 127, 127, 127, 127, -99, 127, -70, -127, 127, 127, -127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 35, -127, 127, 127, -127, 127, -127, 127, 127, 127, -82, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, 127, 127, 127, -127, 127, -127, 127, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, -127, -108, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 127, -127, 127, 127, 127, 127, -39, 127, -127, -127, 69, 66, 74, 60, 77, 98, 42, 69, -79, 57, -37, 91, 97, 69, -106, 28, 105, 63, 86, 76, -72, 78, 52, 69, 69, 87, 76, 91, 67, 75, 62, 55, 66, 94, 62, 47, -71, -42, 95, 60, 80, 72, -39, -27, 66, -69, 77, 85, 86, -64, 100, 82, -96, -117, 55, -49, 77, 71, 75, -57, 121, -36, 83, -57, -22, -44, 66, -34, 64, 75, 70, -124, 73, -51, 49, -69, 61, -73, -89, -111, -14, 59, 49, -51, 60, 93, 59, 93, -117, 94, 83, 47, 68, 72, 74, 73, -39, -25, -101, 69, -102, -67, 81, 93, 75, 65, 123, -74, -55, 86, 110, 52, 84, -112, 68, -64, -81, -78, 62, 79, 74, -40, -40, 109, -60, 86, 76, 59, -33, 50, 48, 53, 88, 85, 79, 88, 41, 47, 68, 74, 70, 79, -71, 69, 102, 58, -108, 75, 61, 67, 80, 43, 93, 72, 100, 60, 61, -64, -42, -54, 69, 102, -66, -53, 120, -86, -94, 79, 53, 46, 108, -47, 69, 59, 70, -15, -64, 69, 67, 101, 76, 58, 106, 81, 70, -85, 60, 72, -61, 74, 66, 119, -79, 92, 72, 63, 64, 65, 79, 66, 41, 63, 65, -67, -32, 109, 78, 61, 58, -30, 86, 83, 43, -49, 60, 95, 79, 70, 84, 63, -36, 55, 76, -30, 80, 50, 82, 60, 73, -81, 77, 89, -77, -30, 81, 67, 90, 76, 83, -50, 59, 70, 74, 70, -31, 69, 63, 76, 76, 64, -106, 76, 80, 47, 81, 99, -50, 76, 52, 7, -38, -40, 50, 83, 72, 81, 100, 75, 49, -96, -38, 84, 81, 60, 80, 68, 75, -91, 76, 74, 85, 56, 76, 86, 124, 56, 110, -27, 84, 75, 78, 72, 22, 67, -52, -13, 80, 94, 77, 73, 50, 102, 82, 47, -17, 82, 74, 89, 61, 72, 88, 71, 97, -90, 72, 65, 68, -60, 72, 125, 53, 99, 79, -100, 86, -63, 79, -64, -72, 65, 91, 57, 76, 84, -36, 76, 65, 66, 112, 64, -12, 70, 55, 72, 96, -105, 122, 75, 69, -39, 80, 76, 74, 52, 71, 72, 59, 75, 50, 91, -90, 46, -14, 70, -84, 69, -77, 89, -72, -97, 24, 102, 63, 89, -24, -127, 92, 65, 67, -57, -30, 71, 69, 99, -54, 90, 74, -39, 70, -53, 83, 28, 80, 84, 78, 87, -65, 55, -38, 66, 94, 62, -127, 72, 92, 72, 40, 63, 78, 73, 62, 84, -47, 80, 23, 85, 87, 72, 58, 88, 81, -38, 53, -22, -40, 63, 82, -59, 78, 68, 45, -57, 74, 85, 72, 51, 56, 120, -31, -74, 69, 73, 83, -20, 66, 112, 74, 87, -44, 107, 101, 59, 56, 87, 65, 95, 70, 77, 73, 91, 75, -83, 77, 109, 78, -37, 68, 73, 69, 71, 80, 67, 104, 89, 73, 79, 74, 92, -127, 74, 98, 63, 101, 86, 61, 12, 81, 75, 64, 52, 67, 68, 90, 69, 81, 50, -75, 73, -118, -59, 79, 68, 80, -73, -126, 15, 78, -72, -75, -76, 9, 91, 127, 76, -96, -55, 24, 67, 83, 87, 59, -69, 80, 75, 61, 82, 81, 63, 72, 125, 55, 66, 67, 50, 98, -31, 79, 82, -69, 63, 54, 67, -52, 68, 55, 64, 80, -80, 70, 58, 84, 59, -48, 77, 81, 71, 107, 107, 63, -29, -127, 63, -104, 107, 61, 93, 54, 40, -48, 98, -39, 63, -39, 59, 51, 84, -101, 75, 79, 95, 84, 80, 71, 100, -60, 79, -78, -27, 91, 66, -89, -76, 66, -38, 76, 75, -30, 78, 61, 90, 55, 64, 32, 60, -58, 101, 71, 85, 97, -54, 53, 79, -44, -42, 80, 86, -78, -60, 48, 66, 49, 90, 73, 115, 80, 38, -36, 65, 75, 67, 81, 79, 94, -56, 72, 44, -24, 88, 83, 85, 65, 44, 54, 68, 71, 100, 82, 69, -62, -53, -64, 66, -89, 74, 53, 68, 67, -50, -41, -21, 61, -75, 83, 63, 77, 111, 76, -127, -61, 52, 84, 79, 72, 109, -125, -51, 61, 92, 102, 72, 67, 74, 63, 84, 65, -105, -56, 57, 83, 9, 63, 81, 101, 76, 88, 86, 69, -62, 86, 59, 88, 90, 88, 97, 73, 88, -51, 76, -102, -51, 68, -121, -43, 71, 72, 116, 67, -76, 68, 44, 24, 92, -103, 94, -61, -37, 67, -28, -43, 68, 61, 45, -110, 84, 73, -62, 112, 89, 70, 79, 53, 22, 97, 55, 82, 87, 75, 76, 53, -79, 65, 102, -79, 109, -75, 89, -72, -54, 69, 66, -70, 101, 80, -45, 30, 58, 68, -25, 80, 53, 96, 105, 71, 70, 84, -91, -82, 79, 86, 71, -48, 63, 66, 84, 78, -62, -54, 127, 110, 84, -82, 60, 69, -62, 66, 92, -56, 83, -47, 66, 58, 78, 83, 46, 62, 66, 63, -25, -113, 74, 80, 86, 59, 98, -53, 80, 80, 81, -50, 71, 96, 70, 120, 127, 81, 73, -75, 76, 70, 68, 94, -69, 96, 74, -37, 74, 60, 89, 79, 72, 85, 65, -33, -26, 76, 55, 92, 81, 115, 87, -49, 92, 73, 56, 57, 98, -37, 22, 74, 70, 63, 83, 73, -61, 69, 69, 46, 43, -48, 65, 51, 69, 75, 106, 97, 27, 72, -45, 70, 86, 61, -52, -17, 61, 78, -74, -51, -75, 79, -53, 66, -127, 69, 74, 107, 59, 79, 29, 59, 71, 71, -56, 30, -29, 24, 61, -64, 76, 79, -50, 80, 92, 90, 102, -43, -79, 35, 71, 66, 90, 57, 65, 57, 88, -105, 74, -77, 68, 9, 67, -54, 87, 95, 118, 59, 81, -41, -85, -40, -52, 72, 84, 78, 69, -42, 88, 66, 63, -69, 65, 67, 48, 108, 71, 42, 67, 24, 115, 91, 75, 65, 46, 70, 75, 72, 80, -37, 104, 65, -73, 68, -21, 77, 64, 124, -60, -58, 70, 69, 53, 87, 76, 81, 102, 110, 85, 65, -87, 91, 58, 65, 92, -60, -83, 77, 89, 73, 75, -51, 74, 69, 87, 69, 67, 83, 70, -41, 34, 69, 68, 58, -55, 80, -29, 60, 62, 82, 73, -83, 114, 67, 76, 71, 70, 98, 77, 101, -47, 89, 84, 12, 83, 83, 69, 101, 64, 83, 78, -60, 72, 92, 78, 58, 80, 75, 60, 66, -40, -73, 70, -75, 94, 52, 83, 69, 60, 56, -110, 57, -93, -111, 82, 84, -46, 72, -37, 74, 66, 67, 80, 101, 40, 87, 81, 88, 118, 119, 86, 15, -71, 85, 74, -79, 58, -79, 81, 78, 54, -46, 89, 64, 79, 90, 86, 74, 79, 78, 79, -36, 67, 60, 111, 84, 99, -26, 73, 71, 81, 86, -72, 68, 67, 87, 77, -24, 76, -55, 53, 63, -89, 69, 92, 61, 69, 70, 94, 69, 92, 79, 68, -47, 8, 79, 77, 68, 77, 74, 78, 81, -30, -21, 47, 106, -68, 64, 87, 44, 80, 35, 79, -51, -33, 4, 17, 5, -14, 24, 52, 34, 84, -35, -3, 0, 33, 36, 13, -8, -9, 81, -40, 4, 7, -4, 38, 5, 25, 11, 7, 0, 5, -4, -6, 12, 12, 7, 34, 26, -11, -14, 15, 18, -3, 9, 25, 13, 13, -25, -13, 71, -3, 17, 15, -21, 31, -13, -59, -8, 6, 32, 51, -11, -14, 46, -3, 50, -11, 65, 10, 5, 17, 17, 16, 15, -26, -22, 8, -2, -15, -50, -19, -45, -72, 10, -2, 12, 7, 11, 46, 14, 0, -40, 50, -10, -7, 3, 13, -10, 12, 10, -13, -75, -4, -43, -26, 7, -52, -11, -16, 42, -33, -4, 22, -7, 17, 47, -73, 20, -9, -23, -9, 7, 29, 4, 4, 11, 31, -4, 5, 0, 18, 24, 24, 23, 93, 27, 37, 8, 9, 15, 26, 10, 15, 0, 15, -37, -6, 13, 8, -29, 19, 5, 22, 3, 38, -67, 1, -65, 20, 17, -19, 6, 2, 4, 15, -13, -21, 39, -38, -47, 20, -11, 8, 127, 0, 89, -8, 8, 18, 6, -9, 20, 68, -30, 11, 16, 24, 30, -17, 4, 28, -18, 48, 26, 18, -40, 34, 4, 7, 18, -8, 10, 30, -69, -3, 14, -19, 8, 62, 18, -3, -30, -1, 18, 14, 0, -19, -8, 23, -38, 7, 38, -7, -22, 17, 38, 15, 3, 16, 10, -10, -20, -26, 4, 41, -14, 5, 19, 18, -10, 23, -20, 3, 18, -3, 10, 11, 24, 47, 30, 7, 38, 16, 15, 16, 14, 12, 39, 50, -2, -24, -5, -11, -1, 30, 99, 32, -10, 6, 59, -4, 29, -25, 30, 21, 13, 29, 19, 13, -2, -97, 16, 30, 22, 6, 17, 17, 91, 26, 25, 9, 6, 37, 14, -5, 78, -16, -7, 18, -33, 34, 37, -13, 21, 15, 2, 42, -67, 41, 10, 6, -4, -4, 4, 26, 48, -55, 18, 22, 17, -7, 32, 78, 56, 53, 7, -43, -3, -14, 18, -4, -13, 46, 28, 8, -2, -5, 11, 58, -7, -6, 37, 21, 9, 16, 11, 26, 70, -44, -64, 7, -11, 16, 30, 8, 29, -3, 2, 26, -1, 5, 27, 27, -18, -5, 10, 16, -5, 8, -3, 2, -25, -61, -36, 23, 28, 23, -100, -45, 71, 12, 11, 5, 11, 16, 24, 23, -2, 7, 34, -1, 8, 5, 13, 109, -39, 34, 8, -7, -8, -11, 40, -1, 20, 16, -66, 50, 45, 17, 2, 2, 81, 18, -1, 25, -6, 4, -1, 0, 24, 41, -1, 29, -59, 49, -58, 10, -1, 3, 15, 8, -23, 3, -5, 1, 17, 8, 15, 5, 16, 37, 14, -11, 17, 26, 11, 17, 5, 18, 14, 11, 0, 80, 38, 3, 14, 27, 31, 107, 10, 36, 32, 39, 12, -13, 28, 16, 13, 12, 14, -1, 26, 6, 40, 72, 30, 41, 45, 12, 2, 1, -35, 1, 28, 7, 29, 27, -5, -66, 66, 33, 8, 3, 26, 16, 28, 19, -2, 18, -19, 23, -45, -22, 37, 36, 37, -42, -66, 56, 14, -28, -6, -40, 112, 54, -1, 18, -45, 1, 26, 4, -8, 7, -44, -17, 3, 26, 14, 29, -5, -1, -10, 59, -5, 21, 22, 14, 39, 16, 48, 76, -15, 9, 1, 20, 12, 24, 18, 17, 5, -26, 27, 20, 32, 49, -10, -5, 26, 24, -19, 24, 21, -89, -18, -6, -23, 1, 11, 8, 97, 93, -1, 91, 15, 9, 15, 9, -29, 19, -21, -15, 14, 22, 11, 13, 12, 5, 2, 48, -31, 14, -3, 35, -40, -20, 22, 8, 30, -12, -11, 27, 26, 6, -17, 63, -3, 16, -7, -18, 52, 21, 21, -4, 95, 14, -3, 2, 5, 23, -38, -19, 93, -8, 37, 4, 59, 82, -4, 118, 9, 2, 85, 1, 8, 47, 18, 12, 56, -110, 3, 18, 14, 18, 13, 46, 3, 18, 31, 49, 9, 21, -12, -16, -3, -8, -16, 20, -1, 35, 22, 1, 15, -65, 1, -25, -5, -4, 35, 15, 16, 45, 3, 26, 15, -11, 62, -15, -69, -16, 31, 22, 54, -2, -14, 19, 7, 12, -1, -20, -13, 32, 11, -45, -9, 27, 26, 26, 21, 0, 22, -3, 2, 6, 27, 94, 11, 15, 8, 19, 4, 25, -20, 7, 19, -74, -3, 46, 14, 19, 6, -16, 15, -17, -20, 2, -26, 19, -17, -2, 10, 23, -7, 20, 40, -3, -33, -28, 53, -4, 10, -28, 37, 10, -18, 1, -27, 3, 14, 45, 91, 11, 11, -88, 17, 48, -23, 47, -21, 28, -56, 8, 27, 3, 31, 51, 4, 5, -10, -5, 25, -2, 17, 1, 53, 78, 2, 6, 74, -16, 5, 48, 40, 20, 9, 2, 14, 21, 28, -18, 12, 67, 4, 24, -28, 24, -22, -15, 29, -2, -8, 20, 3, 7, 14, 29, 39, 37, 14, 6, 33, 17, -62, 22, 18, 29, 11, 66, -2, 4, 5, -4, -6, 3, 24, 24, 42, 42, -5, -1, -29, -7, 10, -13, 58, -7, 45, 40, 14, 10, 20, 13, 9, -3, 34, 17, 18, 25, 58, 17, 35, 24, 91, 48, 1, 53, 21, 2, 21, 54, 11, 92, 35, 20, 5, 31, 12, -21, 23, 23, 34, -21, -5, 22, -4, -6, 16, 77, 27, 29, 23, 15, -4, -1, 6, -37, 13, 19, 32, -12, 8, -26, 3, -4, 27, -16, 4, 13, 55, -2, 19, 75, 31, 35, 5, -13, 79, 0, -11, -17, -19, 21, 36, 8, 23, 27, 6, 50, 20, -23, -17, 16, 70, 27, 25, 41, 11, 43, -53, -36, -38, 21, 10, 16, 4, 43, -10, 61, 60, 29, 6, -27, 17, -8, 30, -15, 45, -4, 22, 37, 44, 65, 0, 26, 12, 16, 43, 9, -23, -23, -7, 84, 74, 10, -3, 24, -5, 35, -17, 29, -5, 13, 4, -19, 13, 16, 9, -42, 108, -8, -7, 5, 69, -5, 23, 39, 37, 1, 15, 38, 11, -49, -26, -25, 25, 6, 4, -22, 39, 17, 32, 33, 42, 21, 13, -24, 26, 21, 18, 11, 7, -2, -5, 47, 10, -13, 9, 8, 31, 10, 23, 19, -11, 48, -8, 3, 29, -2, 21, 37, -43, 11, 34, 20, -113, 27, 9, 20, 9, 9, 37, 13, -8, 12, 40, 40, 14, 50, 17, 8, -42, 6, -23, 0, 9, 51, -12, 33, 10, 16, -3, -72, 4, -127, -63, 28, 30, -19, 24, 3, 17, -29, 0, 40, 22, 32, -3, -4, 19, 49, 79, -21, 52, -20, 25, 42, -10, 3, -23, 7, 3, 8, -103, 27, 36, 24, 57, 49, 7, 34, 18, 21, 12, -1, -25, 49, -25, 36, 8, 43, 16, 12, 46, -25, 43, -6, 29, -9, 11, 32, 16, -4, -18, -29, 12, 14, 0, 17, 7, 51, -8, -19, 35, 15, -17, 114, 31, 30, 11, 23, 55, 3, -2, 9, 13, -11, 40, -19, 2, 46, 0, 22, -65, 46, -3, 12, 54, 54, 81, 63, 71, 79, 88, 73, -67, 68, -41, 60, 113, 70, -83, 32, 127, 43, 71, 64, -68, 89, 54, 66, 62, 71, 64, 76, 69, 73, 61, 44, 65, 94, 43, 46, -55, -42, 67, 56, 74, 73, -21, -32, -47, -78, 99, 56, 86, -71, -7, 62, -78, -109, 55, -43, 66, 49, 57, -74, 117, -26, 75, -61, 127, -27, 57, -22, 59, 69, 63, -63, 55, -38, 56, -46, -81, -46, -92, -92, -16, 63, 46, -59, 59, 108, 50, 101, -105, 101, 54, 37, 64, 68, 81, 66, -38, -67, -94, 63, -107, -74, 79, -14, 63, 48, 116, -81, -27, 75, 85, 60, 75, -90, 81, -60, -56, -50, 61, 73, 75, -45, -36, 95, -50, 97, 56, 54, -50, 40, 54, 127, 83, 82, 62, 48, 26, 55, 50, 97, 59, 68, -85, 69, 103, 49, -108, 71, 46, 61, 70, 38, -20, 65, -66, 69, 55, -64, -38, -37, 62, 103, -59, -69, 126, -80, -97, 67, 31, 51, 122, -53, 95, 49, 70, -18, -58, 68, 69, 127, -22, 52, 94, 73, 76, -55, 59, 58, -58, 84, 62, 102, -77, 76, 78, 55, 69, 68, 65, 72, -65, 62, 62, -56, -28, 119, 85, 59, -14, -38, 91, 65, 31, -80, 69, 61, 70, 70, 77, 54, -46, 55, 65, -28, 77, 48, 72, 55, 65, -70, 51, 78, -55, -44, 61, 65, 52, 67, 51, -22, 60, 54, 51, 77, -17, 80, 51, 73, 63, 75, -105, 70, 74, 45, 69, 81, -41, 39, 45, 6, -47, 11, 127, 71, 69, 72, 103, 67, 66, -79, -7, 120, 68, 61, 96, 65, 69, -127, 64, 70, 58, 54, 82, 70, 65, 51, 124, -24, 72, 86, 44, 64, 127, 61, -50, -23, 35, 76, 81, 77, 59, 67, 51, 58, -9, 75, 72, 77, 58, 59, 74, 74, 73, -107, 50, 57, 54, -53, 75, 101, 127, 84, 84, -94, 85, -56, 81, -55, -76, 98, 79, 51, 55, 68, -31, 106, 49, 64, 98, 64, -27, 79, 58, 64, 121, -74, 39, 65, 42, -35, 64, 82, 73, 36, 79, 68, 55, 71, 37, 68, -82, 49, -26, 52, -36, 67, -53, 42, -70, -69, -55, 77, 66, 49, -111, -103, 126, 65, 70, -54, -25, 70, 60, 88, -46, 127, 46, -42, 68, -44, 75, 127, 58, 91, 52, 87, -52, 36, 6, 65, 73, 49, -113, 68, 75, 79, 73, 86, 123, 73, 48, 66, -59, 71, 20, 59, 86, 68, 65, 81, 16, 7, -89, -21, -39, 56, 70, -60, 70, 63, 42, -54, 65, 73, 63, 55, 55, 106, -25, -71, 69, 74, 80, -9, 67, 101, 65, 97, -39, 121, 91, 54, 53, 64, 45, 127, 64, 74, 48, 84, 62, -78, 83, 100, 91, -30, 72, 53, 78, 76, 76, 127, 79, 101, 104, 73, 71, 88, -104, 53, 82, 70, 78, 86, 52, -97, 111, 65, 60, 51, 61, 93, 80, 63, 64, 41, -54, 93, -102, -72, 68, 68, 95, -80, -113, 57, 85, -73, -62, -78, 127, 89, 99, 74, -66, -47, 26, 64, 40, 71, -6, -50, 77, 55, 71, 78, 75, 59, 67, 105, 44, 61, 68, 45, 103, -22, 65, 127, -61, 67, 45, 73, -39, 72, 74, 54, 73, -76, 68, 66, 84, 67, -56, 70, 72, 94, 19, 75, 64, -127, -113, 62, -120, 127, 63, 43, 127, 127, -42, 127, -32, 62, -30, 64, -54, 85, -95, 69, 70, 86, 56, 75, 57, 68, -57, 107, -61, -29, 42, 55, -84, -63, 83, -31, 85, 60, -19, 78, 63, 68, 52, 90, 92, 67, -50, 127, 116, 78, 74, -49, 127, 79, -47, -48, 60, 89, -84, -57, 127, 65, 51, 94, 114, 124, 42, 127, -28, 66, 127, 58, 59, 83, 72, -42, 91, -127, -27, 87, 82, 76, 57, 97, 43, 88, 79, 77, 72, 67, -52, -66, -40, 61, -73, 66, 53, 67, 58, -46, -29, -96, 52, -60, 85, 59, 83, 127, 54, 22, -28, 71, 86, 62, 95, 78, -80, -59, 70, 76, 86, 63, 49, 80, 53, 86, 65, -118, -42, 87, 70, -41, 46, 71, 94, 103, 50, 70, 76, -71, 69, 54, 91, 62, 90, 78, 62, 73, -63, 68, -89, -35, 50, -100, -55, 73, 66, 83, 58, -69, 63, 34, 19, 69, -81, 76, -50, -40, 70, -13, -57, 57, 61, 39, -127, 12, 98, -46, 62, -11, 61, 57, 66, 39, 54, 52, 75, 74, 103, 63, 52, -127, 53, 89, -64, 92, -70, 102, -74, -35, 36, 53, 3, 49, 65, -71, 41, 58, 70, -29, 72, 42, 98, 127, 56, 58, 101, -68, -51, 86, 92, 71, -39, 52, 69, 71, 72, -61, -58, 86, 61, 68, -86, 65, 80, -48, 98, 80, -47, 82, -37, 60, 66, 89, 86, 35, 54, 53, 59, -29, -93, 68, 81, 51, 62, 109, -56, 76, 44, 68, -50, 59, 84, 51, 55, 103, 68, 71, -74, 56, 77, 49, 116, -35, 64, 73, -33, 70, 65, 65, 69, 76, 74, 59, -36, -26, 92, 42, 80, 79, 61, 108, -43, 90, 75, 56, 62, 107, -36, 108, 65, 54, 59, 58, 80, -60, 69, 79, 53, 32, -55, 59, 46, 27, 71, 116, 103, 43, 71, -62, 71, 42, 57, -65, -15, 76, 88, -61, -28, -65, 64, -84, 63, -77, 65, 74, 103, 71, 69, 127, 64, 92, 73, -65, 127, -39, 12, -9, -48, 69, 63, -49, 89, 73, 83, 92, -34, -70, 27, 66, 127, 66, 51, 75, 66, 78, -85, 33, -75, 61, 5, 80, -45, 86, 58, 122, 101, 75, -40, -80, -32, -48, 71, 57, 84, 89, -28, 87, 71, 120, -41, 54, 66, 51, 80, 73, 36, 21, 30, -20, 74, 77, 80, 51, 54, 82, 63, 90, -41, 74, 67, -77, 62, -23, 57, 28, 110, -54, -65, 71, 127, 47, 76, 77, 70, 97, 120, 62, 69, -127, 31, 64, 54, 78, -56, -69, 75, 57, 77, 87, 67, 66, 56, 47, 68, 66, 62, 72, -41, 47, 48, 83, 58, -53, 66, -27, 69, 66, 59, 83, -77, 127, 66, 59, 91, 54, 91, 59, -14, -35, 58, 64, -123, 73, 57, 69, 92, 57, 83, 78, -50, 61, 76, 84, 53, 85, 71, 64, 42, -44, -69, 50, -89, 101, 39, 99, 70, 49, 47, -127, 65, -105, -87, 65, 94, -67, 50, -31, 82, 68, 67, 91, 82, 49, 82, 63, 80, 115, 101, 60, 127, -64, 73, 79, -64, 59, -79, 69, 61, 77, -127, 85, 52, 69, 106, 86, 61, 87, 76, 75, -38, 65, 50, 71, 98, 105, -28, 85, 63, 83, 81, -64, 73, 53, 106, 67, -12, 45, -19, 56, 67, -90, 68, 88, 52, 90, 71, 117, 60, 41, 53, 61, -43, 127, 76, 82, 62, 72, 95, 75, 96, -40, -28, 43, 110, -65, 61, 88, 41, 81, -126, 95, -52, -26, 2, 15, 8, -13, 24, 54, 35, 82, -35, -4, 3, 44, 36, 10, -4, -9, 82, -37, 0, 5, -4, 33, 2, 26, 11, 8, -2, 8, -6, -3, 8, 9, 10, 33, 22, -7, -15, 14, 11, -5, 9, 22, 10, 13, -21, -11, 70, -6, 17, 11, -21, 31, -18, -57, -6, 5, 31, 51, -14, -18, 46, -3, 50, -8, 67, 11, 3, 17, 21, 15, 12, -27, -24, 4, -1, -14, -51, -14, -45, -68, 14, -2, 9, 7, 10, 48, 14, 3, -40, 49, -16, -6, 6, 13, -23, 15, 13, -5, -74, -1, -37, -26, 7, -39, -11, -15, 42, -34, -5, 26, -8, 14, 45, -61, 17, -6, -16, -12, 4, 29, 4, 5, 10, 34, -6, 7, 1, 19, 24, 26, 25, 89, 30, 33, 8, 8, 13, 28, 9, 11, 3, 14, -36, -9, 13, 2, -32, 23, 3, 28, 8, 39, -66, 0, -72, 16, 17, -16, 4, 1, -2, 7, -13, -21, 44, -34, -48, 16, -12, 6, 125, 2, 91, -7, 11, 17, 6, -8, 23, 68, -33, 11, 19, 19, 28, -16, 6, 28, -17, 48, 27, 17, -33, 29, 6, 9, 17, -4, 8, 35, -75, -7, 11, -18, 7, 61, 17, -6, -32, -2, 22, 13, 0, -22, -7, 20, -33, 9, 39, -5, -23, 14, 40, 15, 4, 14, 14, -8, -24, -21, -7, 41, -12, 4, 14, 18, -9, 23, -20, 1, 14, -4, 11, 10, 24, 50, 25, 9, 38, 16, 19, 15, 10, 16, 39, 42, 0, -24, -8, -10, 0, 29, 98, 34, -13, -5, 59, -4, 25, -25, 33, 19, 14, 29, 22, 12, 0, -108, 15, 30, 22, 4, 14, 16, 96, 25, 26, 8, 0, 38, 16, -5, 78, -20, -6, 18, -27, 31, 35, -1, 24, 22, 4, 40, -71, 43, 6, 10, -3, 0, 1, 30, 55, -61, 21, 18, 19, -4, 34, 66, 57, 57, 14, -45, 2, -13, 19, -4, -10, 47, 23, 6, -5, -9, 11, 64, -11, -9, 28, 21, 10, 17, 9, 21, 70, -46, -51, 3, -12, 19, 32, 13, 23, -2, 3, 25, 5, 6, 27, 26, -13, -6, 10, 16, -1, 6, 0, 2, -25, -66, -38, 22, 26, 23, -103, -43, 71, 14, 6, 3, 11, 18, 25, 20, 1, 19, 34, 0, 5, 3, 12, 106, -38, 26, 7, 0, -6, -10, 39, -1, 23, 13, -65, 50, 44, 23, 0, 3, 79, 19, -6, 22, -7, -3, 0, 5, 22, 42, -5, 29, -65, 48, -59, 8, -3, 4, 17, 9, -15, 3, -3, -2, 12, 8, 14, 2, 15, 33, 15, -9, 15, 28, 4, 17, 1, 9, 18, 16, -1, 79, 34, 1, 17, 25, 29, 107, 9, 38, 34, 39, 12, -19, 24, 22, 9, 16, 19, -2, 23, 0, 42, 84, 30, 48, 44, 9, 5, 8, -41, 3, 30, 9, 35, 27, -2, -61, 68, 32, 3, 1, 28, 17, 28, 16, -2, 17, -17, 25, -47, -20, 38, 34, 40, -39, -69, -75, 22, -29, -4, -40, 107, 51, 1, 19, -48, 1, 27, 2, -3, 12, -48, -17, 3, 28, 8, 31, -5, 5, -17, 51, -6, 13, 28, 15, 49, 17, 54, 73, -17, 16, 3, 16, 12, 26, 22, 15, 4, -24, 25, 17, 31, 51, -7, -12, 27, 23, -20, 18, 27, -83, -23, -2, -26, 4, 11, 9, 97, 92, -2, 93, 12, 7, 14, 10, -27, 12, -22, -17, 12, 21, 6, 9, 13, 8, -1, 50, -30, 15, -5, 32, -36, -27, 25, 7, 24, -12, -8, 26, 28, 9, -14, 60, 0, 14, -5, -21, 52, 20, 22, -7, 96, 17, -5, 2, 7, 28, -38, -18, 94, -7, 34, 5, 59, 78, -3, 113, 10, 4, 85, 3, 6, 39, 21, 7, 53, -115, 1, 22, 15, 18, 14, 48, 4, 22, 29, 53, 13, 29, -10, -15, 0, -8, -16, 18, -1, 26, 20, 3, 15, -73, -2, -27, -6, -6, 35, 30, 16, -37, 2, 23, 14, -6, 62, -10, -70, -15, 30, 20, 54, -7, -16, 23, 5, 8, -3, -26, -14, 25, 8, -44, -13, 23, 27, 27, 20, -1, 23, -3, 6, 6, 26, 95, 10, 8, 9, 27, 9, 21, -29, 5, 13, -79, -4, 54, 14, 23, 10, -9, 10, -18, -19, 1, -26, 16, -13, -2, 7, 23, -5, 19, 38, 0, -33, -30, 54, -4, 13, -27, 37, 9, -14, 2, -27, 2, 15, 41, 88, 13, 11, -89, 13, 44, -23, 44, -18, 22, -61, 9, 30, 2, 32, 52, 3, 5, -10, -7, 26, -2, 18, -6, 48, 76, 4, 10, 71, -14, 4, 42, 37, 17, 7, 0, 15, 20, 28, -19, 9, 70, 5, 28, -29, 23, -8, -16, 26, 0, -4, 20, 1, 7, 13, 33, 45, 33, 12, 8, 33, 15, -59, 23, 14, 30, 12, 60, -4, 2, 7, -5, -7, 7, 28, 22, 47, 28, -11, 4, -30, -12, 7, -13, 61, -6, 50, 36, 13, 12, 20, 12, 13, -4, 35, 15, 16, 24, 48, 19, 34, 23, 95, 47, -1, 59, 24, 0, 22, 50, 10, 90, 33, 23, 6, 30, 7, -21, 25, 29, 35, -21, -6, 26, -4, -2, 15, 75, 39, 29, 23, 14, -5, -8, 5, -36, 12, 21, 37, -12, 9, -32, 3, -5, 23, -14, 4, 14, 58, -8, 16, 75, 29, 36, 3, -13, 84, -2, -11, -20, -15, 20, 35, 5, 20, 24, 17, 50, 19, -27, -14, 16, 70, 26, 24, 37, 10, 43, -54, -34, -34, 21, 19, 17, 5, 39, -28, 60, 53, 32, 4, -30, 17, -9, 30, -14, 44, -11, 24, 36, 43, 66, 1, 24, 13, 17, 45, 12, -22, -24, -9, 78, 66, 8, -3, 23, -4, 32, -17, 28, -6, 13, -1, -18, 11, 14, 8, -42, 106, -4, -7, 7, 69, -3, 14, 34, 35, -2, 8, 32, 9, -58, -15, -25, 25, 10, -1, -20, 38, 21, 31, 30, 48, 27, 12, -20, 26, 21, 10, 14, 6, 0, -5, 50, 7, -12, 10, 9, 35, 10, 22, 19, -8, 57, -10, 1, 27, -1, 21, 38, -47, 13, 33, 20, -115, 28, 15, 14, -3, 4, 36, 18, -7, 16, 41, 45, 15, 47, 17, 7, -42, 6, -18, 0, 11, 50, -11, 30, 13, 21, -3, -80, 8, -115, -63, 30, 26, -19, 25, 6, 19, -31, 4, 38, 31, 33, -4, 3, 21, 38, 81, -22, 51, -22, 20, 39, -9, 5, -23, 13, 4, 11, -107, 21, 39, 20, 62, 46, 4, 39, 19, 21, 12, -3, -26, 52, -27, 41, 6, 40, 15, 13, 46, -21, 40, -11, 27, -5, 11, 29, 15, 0, 3, -35, 14, 14, 1, 17, 8, 47, -11, -20, 37, 13, -13, 119, 31, 28, 9, 24, 55, 0, 0, 7, 15, -11, 44, -14, -2, 38, -3, 23, -68, 51, -5, 13};

float bias_raw[1152]={-0.4658365547657013, 0.14087393879890442, 2.395437240600586, -0.7283818125724792, 1.1909458637237549, 1.478224754333496, 0.8242764472961426, 1.0822710990905762, 0.6135643720626831, 0.27189671993255615, 6.4781694412231445, 1.3160312175750732, -0.2839508056640625, 0.547749400138855, 1.6895471811294556, -0.4188823699951172, 0.4991641342639923, 0.4419991672039032, 1.0983128547668457, 1.348038673400879, 3.8137378692626953, 0.5922834873199463, 1.1459072828292847, -0.07387101650238037, -0.2769353985786438, 0.8924362659454346, -0.9636209011077881, 0.4234044551849365, 0.9940102696418762, -0.09723880887031555, -0.16780602931976318, -0.07272666692733765, -0.8815554976463318, -0.7591965198516846, 1.7990162372589111, -0.692366898059845, 0.5556184649467468, 4.997280597686768, -0.37228381633758545, 0.8920678496360779, 0.618133544921875, 0.24736523628234863, 1.94077467918396, 1.2624446153640747, 0.39306506514549255, 1.194002628326416, -0.5878996849060059, 0.9480777978897095, 0.42993462085723877, 2.3534789085388184, -0.30097055435180664, 1.4884954690933228, 1.9066630601882935, 1.6575210094451904, -3.144357442855835, 1.4692587852478027, 1.748928427696228, 1.0971803665161133, 0.7277633547782898, 2.6713829040527344, -2.275394916534424, 1.6537134647369385, 1.5521148443222046, 1.7054321765899658, 0.5276915431022644, 1.710280179977417, 0.7580674886703491, 1.7565230131149292, 0.15260982513427734, 0.7755368947982788, -1.03299081325531, 1.6596001386642456, 0.7824504375457764, 1.434503436088562, -1.500593662261963, 1.7430357933044434, -0.09315499663352966, 0.2009422779083252, 1.2712632417678833, 1.3271384239196777, 1.733758807182312, -0.3416537046432495, 1.6622183322906494, 1.1907744407653809, -0.8076900243759155, -0.8054640293121338, 0.13708221912384033, 1.927793025970459, 1.0763707160949707, -0.2312089204788208, 0.12498730421066284, -1.0447239875793457, -0.35604560375213623, -0.44356298446655273, 1.1624765396118164, 1.3693149089813232, 3.147465705871582, 0.20931632816791534, 0.11383795738220215, 2.0041637420654297, 0.32446134090423584, 0.938372015953064, 1.0961649417877197, -0.33363479375839233, 0.43599075078964233, 0.6791036128997803, 0.667363703250885, 1.9749555587768555, 0.6458566188812256, 2.715282678604126, 1.773237943649292, -0.9799200296401978, -0.4664686918258667, 0.9669931530952454, 0.9144340753555298, 1.2091326713562012, 1.6661169528961182, 2.493818759918213, 1.3788111209869385, 0.47952109575271606, -0.04729902744293213, 1.9920611381530762, 6.630308628082275, 0.8161782026290894, 0.8929746150970459, 2.0092146396636963, 1.8485524654388428, -1.5348937511444092, 1.2201807498931885, 1.2676985263824463, 0.7022600173950195, 0.23839178681373596, 0.6434531807899475, 1.3612275123596191, -0.7882688045501709, 0.7049016356468201, -0.42943501472473145, 1.0182549953460693, 2.257704734802246, -0.06250667572021484, 1.6080528497695923, 1.30656898021698, 1.2263706922531128, -0.029771238565444946, 0.1607666015625, 3.2448458671569824, 1.6494191884994507, 1.1514921188354492, 2.0884430408477783, 0.12905621528625488, -0.800487756729126, 1.313293695449829, 0.1404920071363449, 1.5961594581604004, -0.08458061516284943, 1.8546174764633179, 0.14354193210601807, 1.6412997245788574, 2.0216946601867676, 0.6493293046951294, 1.297826886177063, 0.7222045660018921, 1.3903323411941528, 0.2797884941101074, 1.2331109046936035, 1.1870390176773071, 1.036529779434204, -0.7513622641563416, 0.7281120419502258, -0.8329188823699951, 0.3782788813114166, 2.7765135765075684, 1.0666759014129639, 0.8375458717346191, 2.311638355255127, 1.569362759590149, 1.918646216392517, 0.9464499950408936, 1.839989423751831, -0.24093002080917358, -0.9856480956077576, 1.850768804550171, -0.4374099373817444, 2.2613744735717773, 0.7834732532501221, 0.1921919882297516, 1.5348193645477295, 0.36643505096435547, 2.002124071121216, 0.11011195182800293, 1.9497809410095215, 1.0016698837280273, 1.4953255653381348, 1.640852928161621, -0.9345706701278687, 1.2767088413238525, 1.958660364151001, 0.44790172576904297, 1.163724422454834, 0.9101762771606445, 0.10403542220592499, 0.19205597043037415, 0.8921135663986206, 2.654656171798706, 2.130011558532715, 0.5404993891716003, 1.3912733793258667, 3.2468292713165283, -0.07288485765457153, 4.1593451499938965, 0.20829713344573975, 1.0828975439071655, -1.2164051532745361, 2.453040599822998, -0.826024055480957, 1.0248264074325562, 2.9987287521362305, -0.8473079204559326, -0.0707935094833374, 0.667881965637207, -1.5668152570724487, 2.0766139030456543, 0.3916962146759033, 2.7581324577331543, 0.6562525033950806, 2.1263294219970703, 2.524509906768799, -0.6401779651641846, 1.2565200328826904, 1.3357501029968262, 1.6438432931900024, 0.364929735660553, 2.8862481117248535, 1.2812951803207397, 2.0094287395477295, -0.41597771644592285, -0.34909069538116455, 1.0356775522232056, 0.10982012748718262, 1.2608013153076172, 2.3851025104522705, -0.5415871143341064, 1.0084103345870972, 1.4263784885406494, 1.5996983051300049, 0.5749495029449463, 0.041126251220703125, 0.8759130835533142, 0.5676367282867432, 0.26981234550476074, 0.6175435781478882, -0.20747649669647217, 1.9741408824920654, 3.286848783493042, 0.5146543979644775, 1.7120883464813232, 0.8642303347587585, 0.5482728481292725, 0.6127017140388489, 0.4651236832141876, 4.742917537689209, 0.6362346410751343, 0.22993722558021545, -1.2527124881744385, -0.27624180912971497, 0.6947653293609619, 1.6031310558319092, 1.7376739978790283, -0.16729331016540527, 2.1489670276641846, 0.6534880995750427, -0.06254005432128906, 0.9748252630233765, -2.183973550796509, -0.23635970056056976, 2.249171495437622, 0.3274410367012024, 0.9566160440444946, 1.9224019050598145, -0.0647357702255249, -1.0141746997833252, 0.9910887479782104, -0.7711220979690552, 1.9000215530395508, -1.023592472076416, 1.887067198753357, 0.33204030990600586, 1.6234326362609863, 1.5323190689086914, 2.1507465839385986, 1.162049651145935, 2.463881015777588, 0.4632978141307831, 0.16552558541297913, 0.8797847032546997, 1.0356931686401367, 0.5715873837471008, -0.2806893587112427, 0.9767827987670898, 1.7587207555770874, 2.6765646934509277, -0.03925108164548874, 1.0692555904388428, 1.8493804931640625, 0.9342784881591797, 0.6102648973464966, 0.8724795579910278, -1.5581032037734985, 0.007980972528457642, 0.826911211013794, 0.8919283151626587, 1.3266422748565674, -2.585146427154541, 3.1398797035217285, 2.5435855388641357, 1.069493055343628, 0.40252137184143066, -0.6708693504333496, 1.9283723831176758, 0.6981421709060669, 1.4228719472885132, 1.665006399154663, -2.7811803817749023, 1.1996512413024902, -0.19321227073669434, 2.357685089111328, -0.10599818825721741, 1.6180353164672852, 1.0520591735839844, -0.6609815955162048, 1.0699514150619507, 0.28070956468582153, 1.2584468126296997, 1.6026195287704468, 4.401209354400635, 0.3070516586303711, 2.8780617713928223, 1.4959295988082886, 1.590968132019043, 1.9422194957733154, 2.1537673473358154, -0.6271371841430664, 1.3819013833999634, 2.2461998462677, 0.3772263526916504, 0.5874501466751099, -0.18925923109054565, 1.41438627243042, 0.20623284578323364, 2.3201522827148438, 0.5614416599273682, 2.2886054515838623, 1.3221817016601562, 0.35989949107170105, -0.706558346748352, 1.6447722911834717, 1.809682846069336, 0.8714147806167603, 1.6108779907226562, 1.6282908916473389, 1.3037002086639404, -5.657578468322754, 4.614706516265869, -0.6357783079147339, 0.9862270951271057, 1.0822685956954956, 1.7329349517822266, 0.487945020198822, 0.040969133377075195, 1.2750418186187744, -4.14240026473999, 0.9168940782546997, 2.7894442081451416, -0.5543448328971863, 0.8983722925186157, 0.7825799584388733, 0.5902575254440308, 0.8124001026153564, 1.5852402448654175, 1.5181909799575806, 2.020784616470337, -0.7600646018981934, -0.03424859046936035, 1.2031662464141846, 0.9989483952522278, 0.780461311340332, 1.2279874086380005, 0.9942781925201416, 2.8088364601135254, 1.512970209121704, 0.8836098313331604, 0.12582308053970337, 0.8367599844932556, 1.4754292964935303, 1.2371026277542114, -2.990489959716797, 0.8345742225646973, 0.46756482124328613, 0.6886584162712097, -1.515660285949707, 0.3987852931022644, 2.1109189987182617, 0.8177541494369507, -0.4930018186569214, 0.9773645997047424, -0.20269250869750977, -0.8752392530441284, -0.31120944023132324, 1.2119853496551514, 0.5569642782211304, 2.0501670837402344, 1.1171777248382568, 5.153809070587158, -0.3464241027832031, 1.1210358142852783, 0.13214558362960815, -0.23509562015533447, 0.4373464584350586, 1.517409086227417, 0.72404545545578, 0.0849878340959549, 1.0416665077209473, 0.1397877037525177, 3.8454527854919434, 4.306446075439453, 0.4489142596721649, 0.7583866119384766, 2.1613636016845703, 2.1161084175109863, 1.049405574798584, -0.2887086868286133, 1.5562973022460938, 2.0058798789978027, 1.3540749549865723, 1.93644380569458, 0.49608105421066284, 2.59196400642395, 2.607386589050293, 1.2348577976226807, 2.2489399909973145, 1.5187807083129883, 0.9454365372657776, 2.6045923233032227, 1.4988605976104736, 0.4406176805496216, -0.6102544069290161, 0.49834227561950684, 0.09382510185241699, 1.0837006568908691, 0.3507942259311676, 0.18673372268676758, 1.0547953844070435, 2.085284948348999, 1.019532322883606, 1.7632371187210083, 1.210547685623169, 0.03496062755584717, -1.035590410232544, -0.924598217010498, 0.4869198799133301, 0.6749757528305054, 1.538337230682373, 0.11525458097457886, -0.4721970558166504, -0.43639063835144043, 1.9418689012527466, -1.1295868158340454, 0.4268442988395691, 1.675580620765686, -2.983097791671753, 1.3956042528152466, 0.44752228260040283, 1.204182505607605, -0.4657151699066162, -0.08189746737480164, -0.547578752040863, -0.02376362681388855, 0.7302895784378052, 0.805455207824707, 0.22473716735839844, 0.8199841976165771, -0.5395483374595642, 1.0638227462768555, 0.00545048713684082, 0.42411187291145325, -0.11469847708940506, 1.4886481761932373, 0.27343320846557617, -0.8337947726249695, -1.8465379476547241, 0.36685025691986084, -1.0789220333099365, -0.5603097677230835, 1.3524246215820312, 2.2587432861328125, -0.03345656394958496, 0.9751474261283875, 1.3847994804382324, 1.046473741531372, 1.5559161901474, 1.2880895137786865, 1.1038155555725098, 0.40970468521118164, 0.41535842418670654, 1.1085619926452637, -0.9384485483169556, -0.012106895446777344, 2.046757221221924, 1.6036863327026367, 0.6257568597793579, 0.15427739918231964, 1.1601040363311768, 3.3807473182678223, 1.2151299715042114, 1.4198265075683594, 4.639687538146973, 0.5241086483001709, 1.7591526508331299, 0.9872804880142212, 1.3441146612167358, -0.09019604325294495, 0.22446191310882568, -0.47512269020080566, -0.3172276020050049, 1.5629336833953857, 1.9140739440917969, 0.39850980043411255, 2.4274163246154785, 1.6491894721984863, -0.725921630859375, 0.5357932448387146, 0.6171031594276428, -0.9803190231323242, 2.1963322162628174, 0.5361880660057068, 2.64725661277771, 0.8984388113021851, 0.6199718117713928, 3.330106258392334, 1.8015152215957642, 0.2985233664512634, -0.12469351291656494, 1.3482513427734375, -0.08888792991638184, -0.39967167377471924, -2.9833900928497314, 1.441953182220459, 1.0727487802505493, 0.12937796115875244, 1.55228853225708, 2.0057883262634277, 0.5877403020858765, 2.886770725250244, 0.9609090089797974, 1.9080151319503784, 1.5556974411010742, 0.8188601136207581, 0.18357053399085999, 1.1403145790100098, 1.1406664848327637, 1.1702194213867188, 1.9803107976913452, 1.4112261533737183, 0.7879762649536133, 1.4186489582061768, 0.5762824416160583, -0.15277276933193207, 0.042959123849868774, 1.431652307510376, 1.2848730087280273, 4.763579845428467, 1.1249045133590698, 1.9911324977874756, 2.1661171913146973, 0.1697562038898468, 0.9784340858459473, 1.0770809650421143, -2.291295051574707, 1.3365458250045776, 1.596803903579712, 1.5814512968063354, 1.7604517936706543, 2.8817386627197266, 0.1221468448638916, 3.3902535438537598, 0.3863234519958496, 0.5256391763687134, 2.7602524757385254, 0.2248864769935608, 2.604957103729248, 2.5396504402160645, 1.2188565731048584, 0.9258899688720703, 3.5408105850219727, 1.7952594757080078, 1.4386303424835205, -2.119335651397705, -0.28649449348449707, 1.7565555572509766, 0.9693975448608398, -0.41292107105255127, -0.29337334632873535, -2.0762085914611816, 2.279796600341797, 2.5205769538879395, -2.6922593116760254, 0.466188907623291, 0.8746857643127441, -1.051867961883545, 3.8545923233032227, 0.1103854775428772, 0.9433735609054565, 3.772824287414551, 1.119627594947815, 0.9617709517478943, 0.7348982095718384, 0.21267765760421753, 0.800413966178894, 0.2012649029493332, 0.5924311280250549, 0.9476146697998047, 0.9479780197143555, 0.4108315706253052, 0.8604291677474976, -0.9071230888366699, -0.3074965476989746, 1.8260741233825684, 0.5974175930023193, -0.009289860725402832, 1.468860387802124, -0.7483851313591003, 0.6653333902359009, 0.44135865569114685, 5.378528594970703, 1.0868979692459106, 0.07197849452495575, 0.6037753224372864, 1.3843499422073364, -0.24702996015548706, -0.23137903213500977, -0.8355423808097839, 1.4079086780548096, 0.6907198429107666, 0.11416506767272949, 1.2492780685424805, 0.14796602725982666, 0.5848437547683716, 1.0807387828826904, 0.728544294834137, 1.3827571868896484, 1.0236737728118896, 0.11799359321594238, 1.9101892709732056, 1.1171765327453613, -0.11410921812057495, 0.5958552360534668, 1.255449652671814, 2.078655242919922, 0.9555584192276001, -0.20030520856380463, 0.6027421951293945, 0.05327272415161133, -0.405619740486145, 2.2307889461517334, 0.609444260597229, -0.4893977642059326, 0.924102783203125, -1.5999454259872437, 1.7538199424743652, -0.5416419506072998, 0.8590003252029419, 1.2612899541854858, 0.3441653251647949, 0.07413223385810852, 0.89980149269104, 3.1097872257232666, 0.43056321144104004, 0.5485435724258423, 1.500597596168518, 1.8138957023620605, -0.008733093738555908, 1.8548210859298706, -0.9544967412948608, -2.6181674003601074, -1.0585845708847046, 2.9560163021087646, 1.4142670631408691, 1.6652076244354248, 1.6971182823181152, -0.20589330792427063, 0.14663639664649963, -0.964157223701477, -0.9269253611564636, -0.15196484327316284, 0.2960522770881653, -0.34924060106277466, -0.31409913301467896, 2.758737564086914, 0.7996387481689453, -0.7625333070755005, 0.8823814392089844, -0.17572832107543945, -0.19122225046157837, -0.48666125535964966, -0.5101609230041504, -0.32871484756469727, 3.3188064098358154, 0.9427242279052734, 1.3673834800720215, 1.6338911056518555, 0.06362307071685791, 0.8380300998687744, 2.736745595932007, -0.9153425693511963, -0.7381095886230469, 1.6895753145217896, 2.681514024734497, 2.197629451751709, 1.996488094329834, -0.31237924098968506, -2.1094729900360107, 0.5705397129058838, 1.710740089416504, 1.146080732345581, 3.1904566287994385, 2.8979125022888184, 2.347160577774048, 1.8752926588058472, 3.166480541229248, 1.7078571319580078, 1.0582780838012695, -1.1214252710342407, 1.4628212451934814, 0.5292017459869385, -0.48306143283843994, 0.6544448733329773, 1.3098405599594116, -0.41772758960723877, -0.788457989692688, -0.14836537837982178, 1.4513468742370605, -1.8233634233474731, -0.38355034589767456, 0.4644801616668701, -0.691077470779419, 1.0361318588256836, -0.14084935188293457, 1.3460956811904907, 0.6540277004241943, -0.4510003924369812, 1.9583027362823486, 0.9011083245277405, 1.5164296627044678, 0.9187374711036682, 2.4759066104888916, 0.5259032249450684, 2.1775879859924316, 1.074107050895691, -0.6248619556427002, 0.29623955488204956, 0.05160600692033768, 1.6382545232772827, 0.9049836993217468, 4.490422248840332, -1.9430956840515137, 2.707237958908081, -0.37946319580078125, 2.744525194168091, 1.6775901317596436, 0.6112972497940063, -0.47552376985549927, -0.22868776321411133, 0.5299264192581177, 0.6592863202095032, -0.015321493148803711, 3.591747999191284, 1.2506840229034424, 2.2657036781311035, 0.8206304907798767, 1.6653399467468262, 1.9636197090148926, 1.1950559616088867, 1.9745486974716187, 0.6466915607452393, -0.12115442752838135, 2.728085994720459, 5.531288146972656, 1.2540887594223022, 0.5902547240257263, 0.6684565544128418, 1.992130994796753, 1.8191969394683838, 1.7845282554626465, 0.4258425831794739, -0.15464186668395996, -1.2557001113891602, 2.10506010055542, 0.48763665556907654, 0.39197641611099243, 1.7605743408203125, 0.6324770450592041, 1.3719508647918701, 0.3070247173309326, -0.435046911239624, -0.10133373737335205, 1.0062816143035889, 2.2719883918762207, 3.623109817504883, 1.53260338306427, 0.14106076955795288, 0.5267214775085449, 1.3468488454818726, -0.5687954425811768, 0.7363913059234619, 4.7675676345825195, 0.3162088096141815, 0.15318739414215088, 0.3082222044467926, 0.7860136032104492, 0.9135432243347168, 0.9217089414596558, 1.224522352218628, 1.2842504978179932, 2.2883665561676025, 1.2064907550811768, 1.2581510543823242, 1.1670641899108887, 1.5272105932235718, 1.3834861516952515, 1.4727435111999512, 1.4268717765808105, 0.21884924173355103, 1.3701345920562744, 0.7171204090118408, 2.23764705657959, -0.06315815448760986, 1.0890729427337646, 0.9315727949142456, 0.48283928632736206, 1.7214888334274292, 0.2231283187866211, 0.9562087059020996, 2.2336788177490234, 1.3137487173080444, 0.8609445095062256, 1.4917563199996948, -0.01588541269302368, 0.7151115536689758, 0.12434959411621094, 0.6216162443161011, 5.544990539550781, 0.5012170076370239, 1.0005769729614258, 1.9335250854492188, -0.06739354133605957, 1.522295594215393, 5.782712459564209, 0.2568456530570984, 2.0757105350494385, 1.037636160850525, 1.6833500862121582, -0.49739551544189453, 0.34097933769226074, 0.33741044998168945, 0.05067595839500427, 2.1933178901672363, 2.8275132179260254, 1.1358550786972046, 0.7521304488182068, 2.0046677589416504, 0.1970854252576828, -0.8751717209815979, 1.0397177934646606, 0.7267736196517944, -0.07326602935791016, 2.0716166496276855, 0.7391015291213989, 1.2819535732269287, 1.8806788921356201, -0.1417379379272461, 0.7948025465011597, -0.36858999729156494, 1.8305031061172485, 1.2618005275726318, 0.44066011905670166, 0.5483741760253906, 1.2682936191558838, 1.6802926063537598, 0.3239668607711792, 2.136866569519043, 0.6006190776824951, 1.8516846895217896, 0.4787307381629944, 1.1042945384979248, -0.9573936462402344, 0.664219856262207, -0.4166349768638611, -0.12322989106178284, 1.5147651433944702, 2.120885133743286, -0.3268095850944519, 3.0341248512268066, 0.20494715869426727, 3.2229082584381104, -3.875143527984619, 0.6986827850341797, 0.7045558094978333, 1.7978451251983643, 1.7036678791046143, 1.8893870115280151, 0.3585205078125, 0.8317303657531738, 1.4137229919433594, -1.566267967224121, 1.5780284404754639, 0.7797986268997192, 1.1567375659942627, 0.2694516181945801, 0.6882154941558838, 1.1691014766693115, -0.09944641590118408, 2.3530962467193604, -0.03805118799209595, 0.9104090929031372, 1.113706350326538, -0.6242235898971558, 1.8574442863464355, 1.3484044075012207, 0.41164523363113403, 0.8688105344772339, 1.444384217262268, 1.1765996217727661, -0.004773616790771484, 1.9703326225280762, -0.6320374608039856, 0.23190879821777344, 5.501354694366455, 1.868499517440796, 2.275753974914551, 3.4311344623565674, 1.8109424114227295, -0.26104024052619934, 0.21721303462982178, 0.5777587890625, 0.9752647876739502, 1.0686155557632446, 1.6516696214675903, 1.6603907346725464, 1.1296355724334717, 0.1379697322845459, 2.2711973190307617, 1.8458901643753052, 0.9677828550338745, -0.3185462951660156, -0.838404655456543, 0.021001189947128296, -0.7395657896995544, -0.008458375930786133, 0.7544388771057129, 1.4087591171264648, -0.4061482548713684, 1.3878982067108154, 0.6747389435768127, 2.315455913543701, -0.11931578069925308, 1.206782579421997, 1.3294031620025635, 0.8040039539337158, 1.0105191469192505, 1.362112283706665, 0.15563535690307617, 2.326672315597534, 2.0012214183807373, 0.7282813191413879, -0.7671347260475159, 3.182027578353882, 4.572751998901367, -2.4915173053741455, 0.19280225038528442, 1.512851595878601, 3.0891144275665283, 0.35935866832733154, 1.3962907791137695, 1.2290375232696533, 1.8358685970306396, 1.7741479873657227, 0.9465277194976807, 1.099041223526001, 0.28676584362983704, 0.6700015068054199, -0.4017701745033264, 0.40094542503356934, 2.9218590259552, 1.5848414897918701, 0.039491355419158936, 0.36179089546203613, 0.9368162155151367, 0.26036301255226135, 1.1422829627990723, -0.005722016096115112, 2.1451172828674316, -0.3367815613746643, 1.771735668182373, 0.38745594024658203, 2.3260786533355713, -0.39572152495384216, 4.083766460418701, -1.413710355758667, -0.7202761769294739, 0.36904263496398926, 0.7561839818954468, 0.34857940673828125, 1.3878287076950073, 1.417159080505371, 0.3637959957122803, 2.471919298171997, -0.3899393081665039, -0.2627580165863037, 3.597412586212158, 0.588975191116333, -0.16326893866062164, 2.1538710594177246, 1.0809803009033203, 1.494539737701416, 0.13191434741020203, 1.409813642501831, -0.5067781805992126, 1.4145233631134033, 0.0827183723449707, -0.42074209451675415, -0.04411301016807556, -0.424843966960907, 0.36135122179985046, -0.3612516522407532, 0.2661869525909424, -0.3011171817779541, 1.0800927877426147, 0.2724449038505554, 1.5091869831085205, 0.09233641624450684, -0.2902700901031494, 1.6986626386642456, 0.4222838878631592, 1.0880945920944214, 0.4361942708492279, 3.3362936973571777, -0.2539767920970917, 2.2125954627990723, 1.9836682081222534, 1.5004974603652954, 2.6486120223999023, 1.7972315549850464, -0.8391148447990417, 1.0854518413543701, -0.3046683967113495, 2.713566780090332, 1.465918779373169, 1.4219374656677246, 0.7295318841934204, 0.9959157705307007, 1.5276901721954346, -0.8720015287399292, 1.6302173137664795, 1.3037779331207275, 2.1327385902404785, 3.8836231231689453, -0.28161442279815674, 0.7456149458885193, 0.9579511880874634, -0.0988473892211914, 0.3825792670249939, 0.5488462448120117, 1.4810823202133179, 1.1519532203674316, -0.3467763662338257, 0.8425544500350952, -0.23431062698364258, 1.117100715637207, -0.14865756034851074, 0.75563645362854, 0.9703177213668823, 0.40357279777526855, 2.1477692127227783, 0.2590903043746948, 1.3926831483840942, -0.10251057147979736, -0.49826741218566895, 2.4640519618988037, 0.44726690649986267, 1.0467240810394287, 2.148183584213257, 1.538655400276184, -0.5677955150604248, 0.45878371596336365, -0.016321688890457153, 1.4306749105453491, 0.13771390914916992, 0.1344495415687561, 2.016929864883423, 0.18380171060562134, 0.5792974233627319, 0.27541887760162354, 0.5658328533172607, 0.1653679609298706, 2.8237826824188232, -0.12012374401092529, -0.38656139373779297, 0.5488192439079285, 0.48653268814086914, 1.9239747524261475, 1.8266303539276123, -0.2789442539215088, 0.15522819757461548, 1.0834834575653076, 2.074620008468628, 1.98604154586792, 1.1384930610656738, 0.5632998943328857, 2.687987804412842, 0.9585021138191223, 1.3646240234375, 0.7753446102142334, 0.2552270293235779, -1.0153206586837769, 1.3898155689239502, 1.158046007156372, 2.18558669090271, 0.8172803521156311, 0.3083609342575073, 1.2375954389572144, 0.6613365411758423, -0.6320258378982544, -2.1558849811553955, 0.39290523529052734, 1.804912805557251, 0.5309641361236572, 0.11691737174987793, 2.2680859565734863, 1.238520622253418, 5.600407600402832, 2.767620325088501, -0.5070046782493591, 0.6863069534301758, 0.5418985486030579, 1.1470861434936523, 0.15436577796936035, -2.1139097213745117, 0.4167083501815796, -0.2786996364593506, 0.08361673355102539, 4.076259136199951, 3.343430280685425};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
int stride_width=1;
int stride_height=1;
TfLiteFusedActivation activation=kTfLiteActNone;
int dilation_width_factor=1;
int dilation_height_factor=1;
const int filter_dims_size=4;
const int filter_dims_raw[4]={1,3,3,1152};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={1152};
TfLitePadding paddings=kTfLitePaddingSame;
TfLiteType filter_type=kTfLiteInt8;
TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;

struct OpData {
  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;
  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // Hybrid per channel temporary tensors.
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t input_offset_index;
};

void ExtractDepthConvParams(TfLitePadding padding, int stride_width, int stride_height,
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteDepthwiseConvParams* data_params) {
  // TfLiteDepthwiseConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetDepthConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params, char* tensor_data,
                       TfLiteAffineQuantization* quant_struct, size_t bytes_size,
                       TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}
void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to carry information from Prepare() to
  // Eval().
  return new OpData;
}

void Free(TfLiteContext* context, void* buffer) {
  delete reinterpret_cast<OpData*>(buffer);
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;

  // TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;
  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  const TfLiteTensor* bias = nullptr;

  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));

  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);
  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);
  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);
  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);

  const TfLiteType data_type = input->type;

  const TfLiteType filter_type = filter->type;
  const bool is_hybrid =
      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;
  TF_LITE_ENSURE(context,
                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||
                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);
  if (!is_hybrid) {
    TF_LITE_ENSURE(context,
                   filter->type == data_type || data_type == kTfLiteInt16);
  }

  if (data_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }

  // Filter in DepthwiseConv is expected to be [1, H, W, O].
  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));
    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (data_type == kTfLiteInt16) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);
    }
    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);
    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),
                      SizeOfDimension(bias, 0));
  }

  int channels_out = SizeOfDimension(filter, 3);
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int batches = SizeOfDimension(input, 0);

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (data_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }

  if (is_hybrid) {
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE_EQ(
        context, affine_quantization->scale->size,
        filter->dims->data[affine_quantization->quantized_dimension]);

    int temporaries_count = 0;
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;
    data->input_offset_index = temporaries_count;
    if (data->input_offset_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_offset_id));
    }
    ++temporaries_count;

    TfLiteIntArrayFree(node->temporaries);
    node->temporaries = TfLiteIntArrayCreate(temporaries_count);

    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    const int batch_size = SizeOfDimension(input, 0);
    int scaling_dims[1] = {batch_size};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    node->temporaries->data[data->input_offset_index] = data->input_offset_id;
    TfLiteTensor* input_offsets;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->input_offset_index,
                                       &input_offsets));
    input_offsets->type = kTfLiteInt32;
    input_offsets->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {
      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
      input_offsets_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                       input_offsets_size));
    }
  }

  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);
  outputSize->data[0] = batches;
  outputSize->data[1] = out_height;
  outputSize->data[2] = out_width;
  outputSize->data[3] = channels_out;
  return context->ResizeTensor(context, output, outputSize);
}

TfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    const TfLiteTensor* filter,
                                    int16* depth_multiplier) {
  int num_filter_channels = SizeOfDimension(filter, 3);
  int num_input_channels = SizeOfDimension(input, 3);
  TF_LITE_ENSURE(context, num_input_channels != 0);
  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);
  *depth_multiplier = num_filter_channels / num_input_channels;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
                       TfLiteDepthwiseConvParams* params, OpData* data,
                       const TfLiteTensor* input, const TfLiteTensor* filter,
                       const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output));
  } else {
    optimized_ops::DepthwiseConv<float, float>(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                           TfLiteDepthwiseConvParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  } else {
    optimized_ops::DepthwiseConv<uint8, int32>(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                                     TfLiteDepthwiseConvParams* params,
                                     OpData* data, const TfLiteTensor* input,
                                     const TfLiteTensor* filter,
                                     const TfLiteTensor* bias,
                                     TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));

  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output));
  } else {
    optimized_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

TfLiteStatus EvalQuantizedPerChannel16x8(
    const TfLiteDepthwiseConvParams* params, const OpData* data,
    const TfLiteTensor* input, const TfLiteTensor* filter,
    const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.weights_offset = 0;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier.data(),
      data->per_channel_output_shift.data(), GetTensorShape(input),
      GetTensorData<int16>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<std::int64_t>(bias), GetTensorShape(output),
      GetTensorData<int16>(output));

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteDepthwiseConvParams* params,
                                  OpData* data, const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* input_quantized;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &input_quantized));
  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;

  op_params.weights_offset = 0;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr);
  } else {
    optimized_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr, CpuBackendContext::GetFromContext(context));
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // const TfLiteTensor* bias =
  //     (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetDepthConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TFLITE_DCHECK_EQ(input_type, input->type);

  switch (input_type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteFloat32) {
        return EvalFloat<kernel_type>(context, node, params, data, input,
                                      filter, bias, output);
      } else if (filter->type == kTfLiteInt8) {
        return EvalHybridPerChannel<kernel_type>(context, node, params, data,
                                                 input, filter, bias, output);
      } else {
        TF_LITE_KERNEL_LOG(
            context, "Type %s with filter type %s not currently supported.",
            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));
        return kTfLiteError;
      }
      break;
    case kTfLiteUInt8:
      return EvalQuantized<kernel_type>(context, node, params, data, input,
                                        filter, bias, output);
      break;
    case kTfLiteInt8:
      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,
                                                  input, filter, bias, output);
      break;
    case kTfLiteInt16:
      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,
                                         output);
      break;
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));

  switch (input->type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

}  // namespace jodfcc

TfLiteRegistration* Register_jodfcc_REF() {
  static TfLiteRegistration r = {
      jodfcc::Init, jodfcc::Free, jodfcc::Prepare,
      jodfcc::Eval<jodfcc::kReference>};
  return &r;
}

TfLiteRegistration* Register_jodfcc_GENERIC_OPT() {
  static TfLiteRegistration r = {
      jodfcc::Init, jodfcc::Free, jodfcc::Prepare,
      jodfcc::Eval<jodfcc::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_jodfcc_NEON_OPT() {
  static TfLiteRegistration r = {
      jodfcc::Init, jodfcc::Free, jodfcc::Prepare,
      jodfcc::Eval<jodfcc::kNeonOptimized>};
  return &r;
}

TfLiteRegistration* Register_jodfcc_NEON_OPT_UINT8() {
  static TfLiteRegistration r = {
      jodfcc::Init, jodfcc::Free, jodfcc::Prepare,
      jodfcc::EvalImpl<jodfcc::kNeonOptimized, kTfLiteUInt8>};
  return &r;
}

TfLiteRegistration* Register_jodfcc() {
#ifdef USE_NEON
  return Register_jodfcc_NEON_OPT();
#else
  return Register_jodfcc_GENERIC_OPT();
#endif
}

// Warning: Clients using this variant are responsible for ensuring that their
// models only need the UINT8 type. TFLite's op registration mechanism doesn't
// yet allow for more nuanced registration mechanisms.
TfLiteRegistration* Register_jodfcc_UINT8() {
#ifdef USE_NEON
  return Register_jodfcc_NEON_OPT_UINT8();
#else
  return Register_jodfcc();
#endif
}

}  // namespace builtin
}  // namespace ops
}  // namespace tflite
