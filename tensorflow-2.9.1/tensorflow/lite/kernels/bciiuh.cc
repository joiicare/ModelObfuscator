/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace bciiuh {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

int8_t filter_r   aw[18816]={-2, 60, -3, -3, -3, -3, -3, -3, -3, 62, -3, -2, 81, 127, -3, -3, -3, -24, -3, -3, 57, -7, -86, 14, -2, -2, -3, 79, -2, 34, -9, -9, -9, -9, -9, -9, -9, 91, -8, -5, 127, 118, -9, -8, -9, 49, -9, -9, 84, -1, 24, -30, -9, -9, -6, 98, -5, 5, -8, -8, -8, -8, -8, -8, -8, 24, -8, -8, 127, 102, -8, -8, -8, 22, -8, -8, 93, 29, 78, -72, -8, -8, -8, 38, -2, -1, -3, -3, -4, -3, -3, -3, -3, -16, -3, -3, 53, 127, -3, -3, -3, 59, -3, -3, 9, 55, -11, -28, -3, -3, -3, 48, 3, 66, 5, 5, 5, 5, 5, 5, 5, 48, 4, 4, 105, 51, 5, 4, 5, 34, 5, 5, 69, 74, 6, 73, 5, 5, 4, 127, 2, 52, 3, 3, 3, 3, 3, 3, 3, 57, 3, 3, 127, 27, 3, 3, 3, 50, 3, 3, 47, 12, -7, -23, 3, 3, 3, 50, -3, 39, -4, -4, -4, -3, -3, -3, -4, 127, -5, -6, -5, 10, -4, -5, -4, 1, -4, -4, 57, 114, 21, 58, -3, -3, -5, 121, -2, 78, -7, -7, -7, -8, -8, -8, -7, 63, -6, -3, 127, 98, -7, -5, -7, 71, -7, -7, 32, -97, 33, -72, -8, -8, -4, -36, -3, 111, -7, -7, -7, -7, -7, -7, -7, 74, -7, -7, 50, 74, -7, -7, -7, 45, -7, -7, 13, 26, 4, 12, -7, -7, -7, 127, -4, 22, -6, -5, -5, -5, -5, -5, -5, 47, -6, -6, 76, 87, -5, -6, -6, 126, -5, -5, 89, 21, -4, -127, -5, -5, -6, 95, -5, -92, -7, -7, -7, -7, -7, -7, -7, -6, -7, -8, 54, 23, -7, -8, -7, 29, -7, -7, -7, -27, -71, 53, -7, -7, -8, 127, -3, 54, -2, -2, -2, -2, -2, -2, -2, 10, -3, -3, 127, -7, -2, -3, -2, -41, -2, -2, 43, 13, 2, 18, -2, -2, -3, -16, -1, 33, -6, -7, -7, -7, -7, -7, -7, -19, -5, -3, 27, 127, -6, -5, -6, 24, -7, -7, -38, -30, -37, 119, -7, -7, -4, 113, 3, 48, 3, 4, 3, 4, 4, 4, 4, 16, 3, 2, -53, 127, 3, 3, 3, 7, 4, 4, -5, 61, -35, -5, 4, 4, 2, -1, -5, 14, -11, -11, -11, -11, -11, -11, -11, 78, -10, -9, 21, 88, -11, -10, -11, 60, -11, -11, 30, 61, 57, -35, -11, -11, -10, 127, 5, 30, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 74, 127, 1, 1, 1, 36, 1, 1, 54, 125, -96, 14, 2, 2, 2, -4, -3, -8, -1, 0, 0, 0, 0, 0, 0, 54, -2, -4, 117, 66, -1, -3, -1, 82, 0, 0, 127, -70, 10, -103, 0, 0, -4, 95, -5, 75, -2, -2, -1, -2, -2, -2, -2, 66, -3, -7, 1, 31, -2, -4, -2, 41, -1, -1, 127, 93, 49, 13, -2, -2, -6, 60, -4, -33, -8, -8, -8, -7, -8, -8, -8, 33, -8, -6, 110, 93, -8, -7, -8, 94, -8, -8, 127, 58, 50, -7, -7, -7, -7, -1, 1, 16, 4, 4, 5, 5, 5, 5, 4, 127, 3, 1, 41, 61, 4, 3, 4, 115, 4, 4, -69, 35, -26, -9, 4, 4, 2, -55, -3, 65, -6, -6, -6, -6, -6, -6, -6, 54, -6, -5, 62, 127, -6, -6, -6, 65, -6, -6, 11, 13, 13, -29, -6, -6, -5, 63, -3, 59, -5, -5, -5, -5, -5, -5, -5, 27, -6, -6, 90, 71, -5, -6, -5, 96, -5, -5, 5, 123, 51, 127, -5, -5, -6, 13, -1, 45, -2, -2, -2, -2, -2, -2, -2, 5, -2, -2, 3, 53, -2, -2, -2, 127, -2, -2, 44, 23, 40, 39, -2, -2, -2, -1, -6, 16, -13, -14, -14, -14, -14, -14, -14, 27, -13, -10, 126, 69, -14, -12, -14, 89, -14, -14, 127, 118, 57, 32, -14, -14, -11, 55, -1, 64, 2, 2, 2, 2, 2, 2, 2, 67, 1, -1, 87, 127, 2, 0, 2, 91, 2, 2, 57, -93, 55, 84, 2, 2, -1, 9, 4, 55, 6, 6, 6, 6, 6, 6, 6, 3, 6, 5, 127, -9, 6, 6, 6, 10, 6, 6, 13, -1, 21, 41, 6, 6, 6, 64, -1, 64, 1, 2, 2, 2, 2, 2, 2, 22, 1, -1, 45, 19, 2, 0, 2, 127, 2, 2, 7, -33, 38, 52, 2, 2, -1, -20, -6, 113, -9, -9, -9, -9, -9, -9, -9, 82, -9, -9, 56, 127, -9, -9, -9, 86, -9, -9, 68, 55, 44, -36, -9, -9, -9, 57, -3, -76, -4, -4, -3, -4, -4, -4, -4, 43, -4, -5, 30, 77, -4, -4, -4, -37, -4, -4, 59, -4, -98, 8, -4, -4, -5, 127, -3, 127, -4, -4, -4, -4, -4, -4, -4, 32, -4, -4, -17, 14, -4, -4, -4, -5, -4, -4, 16, 9, 1, -6, -4, -4, -4, 47, -2, 11, -4, -4, -4, -3, -3, -4, -3, 12, -4, -4, 127, 6, -4, -4, -4, 48, -4, -4, 46, 105, 49, 25, -3, -3, -4, 68, 1, -12, -2, -2, -2, -1, -2, -2, -2, 9, -2, -1, -54, 52, -2, -2, -2, 23, -2, -2, 33, -17, -24, 127, -1, -1, -1, 63, -2, 46, -2, -2, -2, -1, -2, -2, -2, 46, -3, -5, 127, 99, -2, -4, -2, 46, -2, -2, 72, 49, -64, -75, -1, -1, -5, 11, -1, 72, 0, 0, 0, 1, 0, 0, 0, 127, -1, -2, 23, 88, 0, -1, 0, 12, 0, 0, 64, 86, 5, 22, 1, 0, -2, 42, -1, 50, 1, 2, 2, 3, 2, 2, 2, 61, 0, -2, -19, 127, 2, -1, 2, 33, 2, 2, -11, 56, -33, 33, 3, 2, -2, 13, 0, 4, 2, 2, 3, 2, 2, 2, 2, 114, 1, -1, 54, 127, 2, 1, 2, 109, 2, 2, 35, 49, 13, -20, 2, 2, -1, 26, -4, 49, -5, -5, -5, -5, -5, -5, -5, 50, -5, -6, 127, 69, -5, -6, -5, 12, -5, -5, 53, 76, 16, 54, -5, -5, -6, 0, 3, -39, 2, 3, 2, 3, 3, 3, 3, 77, 2, 3, -122, 71, 2, 2, 3, 90, 3, 3, 12, 19, 49, 127, 3, 3, 3, 50, -1, 55, 3, 4, 4, 4, 4, 4, 4, 60, 2, -1, 70, 127, 3, 2, 3, 44, 4, 4, 36, -70, -29, 65, 4, 4, 0, 56, -4, 104, -10, -10, -10, -10, -10, -10, -10, 72, -9, -7, 73, 47, -10, -9, -10, 105, -10, -10, -22, 95, -40, -127, -10, -10, -8, 110, -5, 127, -8, -8, -8, -8, -8, -8, -8, 57, -8, -7, 103, 59, -8, -7, -8, 10, -8, -8, 90, 20, 20, 11, -8, -8, -7, 30, -4, 82, -3, -3, -3, -3, -3, -3, -3, 20, -3, -4, 127, 60, -3, -4, -3, 112, -3, -3, 86, 12, 48, -20, -3, -3, -4, 101, -3, 59, -5, -6, -6, -6, -6, -6, -6, 26, -5, -5, 5, 93, -6, -5, -6, 127, -6, -6, 43, 103, 38, 8, -6, -6, -5, 56, -2, -3, -2, -1, -2, -1, -1, -1, -1, 47, -2, -2, 127, -6, -2, -2, -2, 4, -2, -2, 64, -23, 33, 48, -1, -1, -2, 69, -1, 33, -4, -4, -4, -4, -4, -4, -4, 90, -4, -4, -99, 74, -4, -4, -4, 127, -4, -4, 97, -35, -2, 4, -4, -4, -4, 66, -4, 40, -7, -6, -7, -6, -6, -6, -6, 18, -7, -6, 119, 36, -6, -7, -6, 127, -6, -6, -15, 39, -25, -8, -6, -6, -7, 62, -5, 15, -9, -9, -9, -9, -9, -9, -9, 73, -9, -8, 1, 127, -9, -9, -9, 61, -9, -9, 38, 56, -88, 6, -9, -9, -9, 107, -6, -41, -14, -14, -15, -14, -14, -15, -14, 127, -13, -9, 58, 82, -14, -13, -14, 59, -15, -15, 4, 110, 66, -39, -14, -14, -11, 88, -6, 127, -8, -8, -8, -8, -8, -8, -8, 47, -8, -8, 29, 60, -8, -8, -8, 22, -8, -8, 122, -15, 13, 21, -8, -8, -8, 37, -5, 124, -9, -9, -8, -9, -9, -8, -9, 52, -9, -9, 90, 51, -9, -9, -9, 24, -8, -8, 45, 127, 16, -8, -9, -9, -10, 111, -3, 36, -6, -6, -6, -6, -6, -6, -6, 45, -6, -6, 16, 74, -6, -6, -6, -1, -6, -6, -15, 27, -53, 6, -6, -6, -6, 127, -3, 53, -10, -9, -10, -9, -10, -10, -9, -112, -9, -6, 13, 127, -10, -9, -9, 79, -10, -10, 77, 52, 40, 97, -9, -9, -7, 97, -5, 127, -6, -6, -6, -6, -6, -6, -6, 125, -7, -7, 81, 31, -6, -7, -6, -10, -6, -6, 52, 27, -13, -67, -6, -6, -7, 0, -7, -31, -12, -12, -12, -12, -12, -12, -12, 24, -12, -10, 127, 104, -12, -11, -12, 1, -12, -12, 83, -1, 11, 83, -12, -12, -11, 111, -1, 45, 0, 0, 0, 1, 0, 0, 0, 15, 0, -1, 127, -4, 0, -1, 0, -3, 0, 0, 21, -27, 45, 21, 0, 0, -1, 37, 5, 10, 10, 11, 11, 12, 12, 12, 11, 8, 9, 6, 95, 57, 10, 8, 11, 1, 11, 11, 84, 26, 18, -7, 12, 12, 7, 127, -2, 34, -3, -3, -2, -3, -3, -3, -3, 127, -3, -4, 51, 117, -3, -3, -3, 13, -3, -3, 78, 40, 55, 3, -3, -3, -4, 70, -4, 78, -9, -9, -9, -9, -9, -9, -9, 55, -8, -7, 127, 95, -9, -8, -8, 109, -9, -9, 47, 19, -6, 40, -8, -8, -7, 125, -4, 23, -7, -7, -7, -7, -7, -7, -7, 15, -7, -6, 127, 76, -7, -7, -7, 62, -7, -7, 52, 42, 12, 11, -7, -7, -7, 55, 2, 76, 9, 10, 10, 11, 11, 11, 10, -22, 7, 3, 114, 43, 9, 6, 9, 21, 10, 10, 70, 45, 38, 54, 11, 11, 4, 127, 2, 42, 3, 3, 4, 3, 3, 3, 3, 45, 3, 1, 127, 43, 3, 3, 3, 47, 3, 3, 6, -101, -71, 14, 2, 2, 2, 120, -5, 14, -9, -9, -9, -9, -9, -9, -9, -23, -9, -8, 21, 19, -9, -9, -9, 14, -9, -9, 48, 38, -127, 9, -9, -9, -9, 34, -4, 27, -5, -5, -5, -5, -5, -5, -5, 52, -5, -6, 127, 53, -5, -5, -5, 10, -5, -5, -7, 34, -20, 62, -5, -5, -6, 24, -3, 63, -3, -3, -3, -3, -3, -3, -3, 9, -3, -3, 127, 46, -3, -3, -3, 36, -3, -3, 33, -4, 34, 9, -3, -3, -3, -19, -5, 72, -7, -7, -7, -7, -7, -7, -7, 99, -7, -7, 60, 127, -7, -8, -7, 68, -7, -7, 70, 78, -19, -36, -7, -7, -8, 62, 0, 24, 3, 4, 4, 4, 4, 4, 4, 26, 2, 0, 49, 49, 3, 2, 3, 40, 4, 4, 8, 64, -1, -127, 4, 4, 1, 45, -4, 11, -7, -6, -7, -6, -6, -7, -6, 104, -7, -6, -4, 85, -7, -7, -7, 73, -7, -7, -45, 127, -30, 82, -6, -6, -7, -2, -8, -123, -19, -19, -19, -19, -19, -19, -19, 68, -18, -13, 17, 76, -19, -17, -19, 71, -19, -19, 99, -4, 47, 70, -19, -19, -15, 127, -1, -10, -4, -4, -3, -4, -4, -4, -4, 101, -4, -4, 50, 108, -4, -4, -4, 84, -4, -4, 75, -127, -30, 27, -4, -4, -5, 78, 0, 76, -5, -5, -5, -5, -5, -5, -5, 62, -4, -2, 124, 21, -5, -4, -5, 60, -5, -5, 52, 20, 127, -47, -4, -4, -3, 55, -2, 50, -5, -5, -5, -5, -5, -5, -5, 97, -5, -3, 11, 47, -5, -5, -5, 100, -5, -5, 103, -79, 127, 81, -5, -5, -4, 64, 7, -61, 9, 9, 9, 9, 9, 9, 9, 99, 9, 8, 49, 55, 9, 9, 9, 67, 9, 9, -8, 64, -127, 78, 9, 9, 9, -22, -2, 23, -4, -4, -4, -4, -4, -4, -4, 39, -4, -3, -33, 43, -4, -4, -4, -73, -4, -4, 109, 110, 8, 49, -4, -4, -4, 127, -5, 10, -10, -11, -11, -11, -11, -11, -11, 60, -10, -8, -28, 127, -10, -9, -10, 6, -11, -11, -52, 87, -32, 54, -11, -11, -9, 102, -2, 44, -3, -4, -3, -4, -4, -3, -4, 59, -3, -3, 127, -3, -3, -3, -3, 64, -3, -3, 18, -17, -15, 13, -4, -4, -3, 100, 3, -35, -2, -2, -3, -2, -2, -2, -2, 44, -1, 2, -25, 34, -2, -1, -2, 127, -2, -2, -88, 36, -127, 97, -2, -2, 1, 38, -5, 102, -8, -8, -8, -7, -7, -8, -7, -16, -9, -8, 123, 127, -8, -9, -8, 75, -8, -8, 43, 44, 106, 35, -7, -7, -9, -13, -4, 43, -7, -8, -7, -8, -8, -8, -8, 89, -7, -6, 53, 127, -7, -7, -8, 56, -8, -8, 35, -24, 71, 105, -8, -8, -7, -42, -1, 9, -2, -2, -2, -2, -2, -2, -2, 63, -2, -1, 127, 119, -2, -1, -2, 18, -2, -2, 23, -5, 11, -16, -2, -2, -1, 62, -5, 72, -11, -11, -11, -11, -11, -11, -11, -57, -10, -9, 127, 70, -11, -10, -11, 26, -11, -11, 35, 76, 12, 65, -11, -11, -9, 41, -3, 127, -8, -8, -9, -8, -8, -8, -8, 19, -8, -5, 82, -14, -8, -7, -8, -2, -8, -8, 61, 81, 25, -33, -8, -8, -6, 102, -1, 53, -3, -3, -3, -3, -3, -3, -3, -74, -3, -3, -72, 113, -3, -3, -3, 26, -3, -3, 99, 127, 17, 74, -3, -3, -3, -26, 2, 35, 3, 3, 3, 3, 3, 3, 3, 12, 3, 3, 127, 54, 3, 2, 3, 89, 3, 3, 78, -66, -27, -15, 3, 3, 2, 125, 1, 84, 0, -1, 0, -1, -1, -1, -1, 60, 0, 0, 27, 56, 0, 1, 0, 41, 0, 0, 49, -88, -18, -127, -1, -1, 0, 76, -3, 66, -3, -2, -3, -2, -2, -2, -2, 127, -4, -5, 116, 114, -3, -4, -3, 22, -3, -3, -14, 62, 31, 95, -2, -2, -5, 89, -1, 77, -1, 0, -1, 0, 0, 0, 0, 52, -1, -2, -44, 118, -1, -2, -1, 22, -1, -1, 60, 127, 63, -52, 0, 0, -2, 52, -5, 28, -5, -5, -5, -5, -5, -5, -5, 127, -6, -7, 54, 3, -5, -6, -5, 15, -5, -5, 1, 49, 0, -28, -5, -6, -7, 66, 1, -7, 0, 0, 0, 0, 0, 0, 0, 17, 0, 1, -4, -17, 0, 0, 0, 31, 0, 0, 46, 42, -127, 40, 0, 0, 0, 20, -5, -11, -15, -15, -15, -15, -15, -15, -15, 48, -13, -9, 51, 40, -15, -12, -14, 120, -15, -15, 35, 102, -1, 54, -15, -15, -10, 127, 0, -40, 1, 1, 1, 1, 1, 1, 1, 8, 0, -1, 108, 84, 1, 0, 1, 90, 1, 1, 127, 3, -17, 37, 1, 1, 0, -37, -5, 4, -2, -1, -1, -1, -1, -1, -1, 66, -3, -6, 52, 59, -1, -3, -1, 59, -1, -1, 127, -65, 29, 39, -1, -1, -5, 92, -5, 127, -7, -7, -7, -7, -7, -7, -7, 113, -7, -7, -37, 63, -7, -7, -7, 37, -7, -7, 69, -35, 36, -3, -7, -7, -7, 11, 2, 82, -1, -1, -2, -1, -1, -1, -1, 29, -1, 2, 127, 85, -1, 0, -1, -10, -2, -1, 24, 62, 21, 92, -1, -1, 1, 72, -4, 46, -9, -9, -9, -9, -9, -9, -9, 56, -8, -7, 4, 127, -9, -8, -9, 32, -9, -9, 109, 64, 1, -13, -9, -9, -8, 28, -6, 58, -11, -11, -12, -12, -11, -12, -11, 42, -11, -8, 64, 81, -11, -10, -11, 120, -11, -12, 49, 42, 127, 73, -11, -11, -9, 74, -3, 127, 0, 0, 1, 0, 0, 1, 0, 55, -1, -3, 21, 63, 0, -2, 0, 1, 0, 0, 30, 18, 16, 13, 0, 0, -3, 17, -4, 109, -9, -9, -9, -9, -9, -9, -9, 83, -9, -7, 14, 36, -9, -8, -9, 127, -9, -9, 59, 41, 31, -31, -9, -9, -8, -31, 1, 72, -3, -3, -3, -3, -3, -3, -3, 42, -2, 0, 127, 8, -3, -2, -3, 47, -3, -3, 10, 39, 12, 49, -3, -3, -1, 38, -4, 65, -9, -9, -9, -9, -9, -9, -9, 121, -9, -7, -40, 118, -9, -8, -9, 127, -9, -9, -44, 25, 34, 7, -9, -9, -8, 91, -10, 54, -16, -17, -16, -17, -17, -16, -17, 54, -16, -15, 1, -34, -16, -15, -16, 79, -16, -16, 39, -13, 38, 32, -17, -17, -15, 127, 2, -13, 1, 1, 1, 1, 1, 1, 1, 25, 1, 2, 127, -61, 1, 1, 1, 29, 1, 1, 84, 31, 8, -43, 1, 1, 2, 57, 0, 85, 1, 1, 1, 1, 1, 1, 1, 35, 0, -2, -15, 127, 1, 0, 0, -11, 1, 1, 46, 29, 1, 10, 0, 0, -1, 86, -1, 127, 1, 1, 1, 1, 1, 1, 1, 22, 0, -1, 30, 65, 1, 0, 1, 81, 1, 1, 43, -6, 15, -1, 1, 1, 0, 34, -5, 74, -6, -6, -6, -6, -6, -6, -6, 16, -6, -7, 99, 127, -6, -6, -6, -8, -6, -6, 92, 82, 84, -2, -7, -7, -6, 45, -2, 63, -5, -5, -6, -5, -5, -5, -5, 76, -5, -3, 127, 60, -5, -5, -5, -13, -5, -5, 5, -68, 82, 58, -5, -5, -4, -3, -3, 54, -7, -7, -7, -7, -7, -7, -7, 56, -6, -5, 113, 43, -7, -6, -7, 65, -7, -7, 8, 32, -7, -63, -7, -7, -6, 127, -5, 127, -6, -6, -6, -6, -6, -6, -6, 48, -7, -6, 56, 61, -6, -7, -6, 35, -6, -6, 29, 40, 26, 12, -6, -6, -7, 42, -5, 81, -12, -12, -12, -12, -12, -12, -12, 50, -11, -8, 30, 68, -12, -11, -12, 127, -12, -12, 19, 11, 26, 40, -12, -12, -9, 56, -8, 35, -14, -14, -14, -14, -14, -14, -14, 19, -14, -12, -13, -15, -14, -13, -14, 105, -14, -14, -58, 83, -57, -61, -14, -14, -13, 127, -2, 43, 0, 0, 1, 0, 0, 1, 0, 33, -1, -3, 47, -1, 0, -1, 0, 42, 1, 1, 9, -8, 17, 4, 0, 0, -2, 127, -2, 62, -4, -5, -5, -5, -5, -5, -5, 15, -4, -3, 61, 56, -5, -4, -5, 40, -5, -5, 37, 127, 9, -26, -5, -5, -3, 43, 7, 44, 9, 9, 9, 9, 9, 9, 9, 49, 9, 8, 112, 52, 9, 9, 9, 77, 9, 9, 104, -127, 20, 26, 9, 9, 8, 89, 0, -17, -6, -6, -7, -6, -6, -7, -6, 76, -5, 0, -10, 45, -6, -4, -6, 36, -7, -7, 24, -2, 42, 118, -6, -6, -2, 127, -5, 48, -7, -7, -7, -7, -7, -7, -7, 55, -7, -7, 44, 127, -7, -7, -7, 124, -7, -7, 52, 4, 24, 8, -7, -7, -7, 95, -2, 30, -4, -4, -4, -4, -4, -4, -4, 71, -5, -4, 6, 69, -4, -5, -4, 127, -4, -4, 16, 15, 9, 104, -4, -4, -4, 65, -5, 44, -5, -4, -4, -4, -4, -4, -4, 53, -6, -7, 23, 41, -5, -6, -5, 20, -4, -4, 13, 49, 23, 43, -4, -4, -7, 127, -6, 127, -10, -10, -10, -10, -10, -10, -10, 118, -10, -10, -61, 121, -10, -10, -10, 75, -10, -10, 2, 82, -7, 35, -10, -10, -10, 3, -7, 90, -11, -11, -11, -11, -11, -11, -11, 127, -12, -11, 101, 51, -11, -12, -11, 110, -11, -11, -42, 66, 21, 13, -11, -11, -12, 61, 1, 92, 4, 5, 5, 5, 5, 5, 5, 99, 2, -1, -31, 104, 4, 1, 4, 121, 5, 5, 62, -37, -19, -72, 5, 5, 0, 127, -9, 26, -12, -13, -12, -13, -13, -13, -13, 32, -11, -11, 127, 93, -12, -11, -12, 50, -12, -12, 27, 103, 66, -34, -14, -13, -12, -10, -5, 7, -6, -6, -6, -5, -6, -6, -6, 72, -7, -7, 100, 74, -6, -7, -6, 62, -6, -6, 22, 46, 63, -64, -5, -5, -7, 127, -1, -18, -2, -1, -2, -1, -1, -2, -1, 87, -3, -2, 41, 12, -2, -3, -2, 10, -2, -2, 82, 25, 45, 70, -1, -1, -3, 127, 0, 126, 4, 6, 5, 7, 6, 6, 6, 51, 3, 0, -19, 26, 5, 2, 5, -1, 5, 6, -36, 14, 2, 6, 7, 7, 0, -127, -1, 31, -4, -4, -4, -4, -4, -4, -4, 78, -3, -1, -27, 52, -4, -3, -4, 127, -4, -4, 56, -36, -16, -40, -4, -4, -2, 36, -4, 100, -6, -6, -6, -6, -6, -6, -6, 70, -6, -6, -30, 111, -6, -6, -6, 45, -6, -6, 17, -12, 119, 86, -6, -6, -6, 127, -2, 25, -5, -5, -5, -5, -5, -5, -5, 127, -6, -5, 20, 89, -5, -6, -5, 119, -5, -5, 89, 37, 53, 78, -5, -5, -6, 11, 2, -127, -5, -6, -6, -6, -6, -6, -5, -10, -4, 0, 3, 61, -5, -3, -5, 45, -6, -6, 67, 40, -80, 93, -5, -5, -1, 98, -4, 127, -5, -4, -4, -4, -4, -4, -4, 63, -6, -7, 101, 32, -4, -6, -5, 25, -4, -4, 44, 74, 48, 4, -4, -4, -7, 39, -5, 70, -6, -7, -6, -7, -7, -7, -7, 110, -6, -6, 15, 127, -6, -6, -7, 123, -7, -7, 71, 36, 18, 51, -7, -7, -6, 21, 2, 14, -2, -3, -3, -2, -3, -3, -2, -28, -2, 0, -27, 6, -3, -2, -2, 127, -3, -3, 57, 53, -60, 21, -2, -2, -1, 56, 1, 38, 2, 3, 3, 3, 3, 3, 3, 46, 1, -1, 105, 44, 2, 1, 2, 18, 3, 3, 96, 34, 92, -127, 3, 3, 0, 30, -7, 66, -10, -9, -10, -9, -9, -9, -9, 78, -11, -10, 117, 48, -10, -11, -10, 4, -10, -10, 39, 127, 36, 25, -9, -9, -11, 121, -2, 76, -4, -3, -4, -3, -3, -3, -3, 117, -4, -4, 114, 95, -4, -5, -4, 83, -3, -3, 68, -75, 3, -118, -3, -3, -5, 127, -5, 54, -14, -15, -14, -15, -15, -15, -15, 127, -13, -10, 50, 4, -14, -12, -14, 64, -14, -14, 58, 83, -13, -20, -15, -15, -12, -62, -4, 37, -4, -4, -4, -4, -4, -4, -4, 65, -4, -4, 8, 127, -4, -5, -4, 60, -4, -4, 100, -17, 78, 26, -4, -4, -4, 55, -3, 127, -6, -5, -6, -5, -5, -5, -5, 67, -6, -5, 87, 20, -6, -6, -6, 76, -6, -6, -8, 103, 24, 8, -5, -5, -6, 37, -1, 23, 3, 3, 4, 3, 3, 3, 3, 58, 2, -1, 82, 76, 3, 2, 3, 71, 3, 3, 120, -127, 90, 33, 3, 3, 0, 57, -5, 18, -5, -5, -5, -5, -5, -5, -5, 53, -5, -6, 127, 77, -5, -6, -5, 46, -5, -5, 101, 47, 18, -116, -5, -5, -6, 79, 3, 127, 17, 16, 18, 16, 16, 17, 16, -31, 15, 7, -47, -12, 17, 14, 16, -47, 17, 17, -50, -49, 1, -10, 16, 16, 10, -49, -5, 34, -7, -7, -7, -7, -7, -7, -7, 58, -7, -8, 35, 62, -7, -8, -7, -2, -7, -7, 24, 59, -19, -18, -7, -7, -8, 127, -4, 117, -6, -6, -6, -6, -6, -6, -6, 127, -6, -6, 85, 42, -6, -6, -6, -1, -6, -6, 93, 47, -8, 44, -6, -6, -6, 44, -8, 1, -12, -12, -12, -12, -12, -12, -12, 49, -12, -11, 3, 127, -12, -12, -12, -24, -12, -12, -71, 16, -58, 47, -12, -12, -12, 101, -3, 21, -8, -8, -8, -8, -8, -8, -8, 15, -7, -5, 127, 36, -8, -6, -8, 2, -8, -8, 14, -4, -22, -22, -8, -8, -6, 45, -5, 75, -9, -8, -9, -8, -8, -8, -8, 66, -9, -8, 127, 12, -8, -9, -8, 42, -8, -8, 13, 14, 22, 14, -8, -8, -8, 84, -3, 76, -4, -5, -4, -5, -5, -5, -5, 74, -4, -5, -127, 65, -4, -4, -5, 70, -4, -5, 85, -37, 29, -8, -5, -5, -5, 31, -1, 83, -5, -5, -5, -5, -5, -5, -5, 84, -4, -3, -36, 102, -5, -4, -5, 63, -5, -5, 36, 1, 85, 6, -5, -5, -3, 127, -7, 105, -10, -10, -10, -10, -10, -10, -10, 85, -10, -10, -32, 127, -10, -10, -10, -23, -10, -10, 13, 15, -14, 66, -10, -10, -10, 36, -2, 115, -2, -2, -2, -2, -2, -2, -2, 34, -3, -4, 65, 127, -2, -3, -2, 86, -2, -2, 70, -9, 11, 49, -2, -2, -4, 42, 0, 32, -3, -3, -3, -3, -3, -3, -3, -64, -3, -2, 9, 40, -3, -3, -3, 127, -3, -3, 12, 64, 8, 27, -2, -2, -2, 47, -5, 127, -5, -5, -4, -5, -4, -4, -5, 31, -5, -7, 123, 70, -4, -6, -5, -33, -4, -4, 4, 79, 3, 58, -5, -5, -7, 89, -3, 76, -5, -5, -5, -5, -5, -5, -5, -7, -5, -5, 19, 115, -5, -5, -5, 72, -5, -5, 79, 127, 117, -36, -5, -5, -5, 41, -4, -67, -13, -13, -14, -13, -13, -13, -13, 72, -11, -6, 4, 127, -13, -11, -12, 64, -13, -13, 16, 23, 7, 6, -13, -12, -8, 99, -8, 102, -12, -11, -12, -11, -11, -11, -11, 127, -12, -12, 43, 108, -12, -12, -12, 34, -11, -11, 67, 119, 10, 33, -11, -11, -13, 105, 1, 28, -4, -4, -5, -4, -4, -4, -4, -45, -4, -1, 91, 127, -4, -4, -4, 14, -4, -4, 29, 81, -64, -32, -3, -3, -2, 57, 1, 89, 3, 4, 4, 3, 4, 4, 3, 65, 3, 2, 125, 71, 4, 3, 3, 123, 4, 4, 127, -42, 8, -52, 3, 3, 2, 56, -5, 68, -5, -4, -4, -4, -4, -4, -4, -15, -6, -7, 127, 14, -5, -6, -5, 4, -4, -4, 41, 55, 38, 28, -4, -4, -7, 68, -2, 22, -2, -2, -2, -2, -2, -2, -2, -39, -2, -1, 8, -127, -2, -2, -2, 120, -2, -2, 88, -1, -94, -11, -2, -2, -1, -55, 1, -76, 3, 4, 3, 4, 4, 4, 4, 37, 2, 1, 44, 127, 3, 1, 3, 95, 3, 3, 96, 77, 83, -10, 4, 4, 1, 106, -7, 59, -12, -12, -12, -12, -12, -12, -12, 18, -12, -10, 55, 78, -12, -12, -12, 69, -12, -12, 65, 127, 12, -106, -12, -12, -11, 68, -2, 47, -4, -4, -4, -4, -4, -4, -4, 44, -4, -4, -52, 127, -4, -5, -4, 85, -4, -4, 14, 51, 66, 21, -4, -4, -5, 86, 5, -34, 10, 12, 11, 12, 12, 12, 12, 127, 10, 8, 39, 64, 11, 9, 11, 42, 11, 11, 121, -76, 2, 123, 12, 12, 8, 116, -5, 93, -11, -12, -12, -12, -12, -12, -12, 120, -10, -7, 95, 100, -11, -10, -11, 127, -12, -12, 42, 48, 100, 22, -12, -12, -9, -72, -4, 34, -12, -12, -12, -13, -13, -13, -12, 127, -10, -6, -14, 46, -12, -9, -12, 125, -12, -12, 83, 6, 17, -56, -13, -12, -8, 97, -4, 115, 0, 1, 1, 2, 1, 1, 1, 116, -2, -5, 12, 74, 0, -3, 0, -23, 1, 1, 88, 29, -83, 94, 2, 1, -4, 127, 0, -12, 0, 0, 0, 0, 0, 0, 0, 12, -1, -1, -31, -14, 0, -1, 0, 119, 0, 0, 37, 45, -127, 34, 0, 0, -1, 13, 0, 37, 2, 2, 2, 2, 2, 2, 2, 54, 1, 0, 127, 15, 2, 1, 2, 55, 2, 2, 36, 36, -37, 31, 2, 2, 1, 64, -7, 46, -12, -12, -12, -12, -12, -12, -12, 67, -12, -11, 65, -6, -12, -12, -12, 86, -12, -12, 27, 93, 11, 9, -12, -12, -11, 127, -2, 15, 0, 0, 0, 1, 1, 1, 0, 74, -2, -4, 12, 96, 0, -3, 0, 127, 0, 0, 62, -69, 38, 35, 1, 0, -4, 111, -2, 33, -3, -3, -3, -3, -3, -3, -3, 72, -3, -3, 126, 127, -3, -3, -3, 60, -3, -3, -17, -25, 37, 32, -3, -3, -3, 39, -2, 27, -1, -1, -1, -2, -1, -1, -1, 127, -1, -3, 29, -5, -1, -2, -1, 81, -1, -1, 60, 89, 12, 58, -2, -2, -3, 52, -6, 73, -10, -10, -10, -10, -10, -10, -10, 101, -10, -9, 87, 127, -10, -9, -10, 41, -10, -10, 27, 58, 29, -67, -10, -10, -9, -33, -1, -45, -2, -2, -2, -1, -2, -2, -2, 116, -2, -3, 91, 127, -2, -3, -2, 112, -2, -2, 19, 104, -12, 6, -1, -1, -3, -18, -3, 11, -7, -7, -7, -7, -7, -7, -7, 66, -7, -5, 49, 127, -7, -7, -7, 85, -7, -7, 95, 29, 39, 120, -7, -7, -6, 105, -6, 127, -8, -8, -8, -8, -8, -8, -8, 62, -9, -8, -30, 78, -8, -9, -8, 81, -8, -8, 97, 72, 14, 36, -9, -9, -9, 15, -5, 82, -9, -9, -9, -9, -9, -9, -9, 42, -9, -9, 52, 95, -9, -9, -9, 85, -9, -9, 45, 88, 44, 4, -9, -9, -9, 127, -4, 76, -12, -13, -13, -13, -13, -13, -13, 44, -11, -6, 72, 103, -12, -10, -12, 127, -13, -13, 71, -93, 21, 63, -13, -13, -8, 49, -3, 28, -6, -6, -6, -6, -6, -6, -6, 119, -5, -4, 127, 114, -6, -5, -5, 9, -6, -6, 23, 120, 0, 23, -5, -5, -5, 63, 3, -5, -2, -3, -3, -3, -3, -3, -2, 2, -1, 2, 111, 127, -2, -1, -2, 42, -3, -3, 71, -27, -85, 54, -2, -2, 1, 89, -2, 5, -7, -7, -8, -6, -7, -7, -6, 88, -7, -4, 127, 105, -7, -7, -7, -14, -7, -7, 112, 37, 91, 110, -6, -6, -6, 95, 0, 78, -1, 0, -1, 0, 0, 0, 0, 31, -1, -1, -127, 84, 0, -1, 0, 61, 0, 0, 39, -46, -16, 23, 0, 0, -1, 102, -2, 4, -3, -3, -2, -3, -3, -3, -3, 64, -3, -4, 5, 20, -3, -3, -3, 81, -3, -3, -61, 0, -127, 12, -3, -3, -4, 63, 0, 127, 5, 6, 5, 6, 6, 6, 6, 74, 3, 1, 16, 81, 5, 3, 5, 19, 6, 6, 36, -4, -1, -51, 6, 6, 1, 84, 0, 79, -2, -2, -2, -2, -2, -2, -2, -23, -2, -3, 102, 74, -2, -3, -2, 92, -2, -2, 57, 127, -95, 114, -3, -3, -3, -30, -3, 44, -4, -3, -4, -3, -3, -3, -3, 55, -4, -4, 127, 61, -4, -4, -3, 27, -3, -3, 16, 17, 56, 6, -3, -3, -4, 102, -3, 85, -4, -5, -5, -5, -5, -5, -5, 118, -4, -3, 110, 60, -5, -4, -5, 35, -5, -5, 84, -110, 92, 11, -5, -5, -4, 127, -6, 44, -11, -11, -11, -11, -11, -11, -11, 95, -11, -9, 76, 104, -11, -10, -11, 73, -11, -11, 71, -71, 36, -28, -11, -11, -10, 127, -4, 108, -6, -5, -5, -5, -5, -5, -5, 34, -6, -7, 62, 72, -6, -7, -6, -5, -5, -5, 26, 32, 62, -5, -5, -5, -7, 127, 6, 9, 10, 12, 11, 13, 12, 12, 12, 115, 9, 8, 40, 50, 11, 8, 11, 127, 11, 12, 110, 3, -8, 49, 13, 13, 8, 119, -5, 69, -8, -7, -8, -7, -7, -7, -7, 28, -8, -7, 41, 127, -8, -8, -8, 65, -7, -7, 41, 63, 25, 46, -7, -7, -8, 72, -1, -9, 2, 3, 3, 3, 3, 3, 2, -74, 1, -1, 13, 111, 2, 1, 2, 13, 3, 3, 48, 34, -82, 100, 2, 2, -1, 127, -5, 15, -14, -14, -15, -14, -14, -14, -14, 50, -13, -9, 105, 94, -14, -13, -14, 105, -14, -14, 48, 74, 100, 35, -14, -14, -11, 127, 1, 35, 0, -1, 0, -1, -1, -1, -1, -19, 0, 0, -36, 27, 0, 0, 0, 127, 0, 0, 28, 22, 75, -17, -1, -1, 0, 69, -6, 27, -9, -9, -9, -9, -9, -9, -9, 21, -10, -9, 61, 29, -9, -10, -9, 25, -9, -9, 25, 98, 15, 127, -9, -9, -10, 22, -9, 88, -13, -13, -12, -13, -13, -12, -13, 54, -13, -13, 116, 127, -13, -13, -13, 61, -12, -12, 60, -118, 24, -40, -13, -13, -14, 10, -6, 97, -9, -8, -8, -8, -8, -8, -8, 127, -9, -9, 100, 66, -9, -9, -9, 40, -8, -8, 41, 112, 8, 61, -8, -8, -9, 31, 5, 35, 4, 3, 4, 3, 3, 3, 3, -127, 5, 6, 17, 69, 4, 5, 4, 48, 4, 3, -3, -20, 22, -6, 3, 3, 6, -16, -5, 127, -6, -6, -5, -6, -6, -6, -6, 33, -6, -6, 15, 2, -6, -6, -6, -9, -6, -6, 47, 16, 31, -30, -6, -6, -6, -21, 2, -12, 6, 6, 6, 6, 6, 6, 6, 59, 4, 2, 21, 8, 6, 4, 6, 127, 6, 6, 36, 65, 49, 70, 6, 6, 2, 19, -6, 39, -10, -10, -10, -10, -10, -10, -10, 107, -10, -9, 41, 54, -10, -10, -10, -32, -10, -10, 127, 56, -1, 37, -10, -10, -9, 73, -3, 127, -4, -3, -3, -3, -3, -3, -3, -13, -4, -5, 120, 87, -4, -5, -4, 26, -3, -3, 28, 7, -25, -1, -3, -3, -5, 73, -5, -2, -9, -8, -9, -8, -8, -9, -8, 69, -9, -8, 127, 93, -9, -9, -9, 22, -9, -9, 47, 62, 51, 6, -8, -8, -8, 112, -7, 30, -9, -8, -9, -8, -8, -8, -8, -20, -10, -11, -2, 26, -9, -10, -9, -4, -9, -9, 19, 58, 24, 29, -8, -8, -11, 127, 0, -12, -1, -1, -1, -1, -1, -1, -1, 79, -1, -1, -127, 58, -1, -1, -1, 61, -1, -1, 65, 8, -12, 27, -1, -1, -1, 63, -1, 29, -3, -4, -4, -4, -4, -4, -4, 28, -3, -1, 127, 44, -4, -2, -4, 12, -4, -4, 20, -25, 43, -27, -4, -4, -2, -9, 4, -54, 4, 5, 4, 6, 5, 5, 5, 36, 4, 4, -20, 70, 4, 3, 5, -49, 5, 5, -13, 127, -46, 28, 6, 6, 4, 96, -7, 127, -12, -13, -13, -13, -13, -13, -12, 81, -12, -10, -58, 42, -12, -12, -12, 32, -13, -13, 21, -44, 72, 10, -13, -12, -11, 67, -1, -36, -1, -1, -1, -1, -1, -1, -1, 73, -1, -2, 69, 67, -1, -2, -1, 34, -1, -1, 49, 26, 15, -9, -1, -1, -2, 127, -10, 22, -16, -17, -16, -17, -17, -17, -17, 34, -15, -14, -127, 40, -16, -15, -16, 50, -16, -16, 52, 0, 21, -38, -17, -17, -15, 24, 0, 54, -1, 0, -1, 0, 0, 0, 0, 20, -1, -1, 43, 127, -1, -2, -1, 98, -1, -1, 83, -10, 30, -98, 0, 0, -1, 120, -5, 127, -12, -12, -12, -12, -12, -12, -12, 106, -11, -7, 79, 33, -12, -10, -12, 120, -12, -12, 120, 46, 106, 55, -12, -12, -9, 73, 2, 82, 6, 7, 7, 7, 7, 7, 7, 112, 5, 2, 39, 57, 6, 4, 6, 68, 7, 7, 124, -127, 6, 3, 7, 7, 2, 103, -7, 36, -10, -10, -10, -10, -10, -10, -10, 116, -11, -11, 73, 125, -10, -11, -10, 57, -10, -10, 38, 127, 37, 49, -10, -10, -11, 61, -5, 7, -11, -11, -11, -10, -11, -11, -10, 107, -10, -8, 82, 98, -11, -10, -11, 41, -11, -11, 62, 17, 10, -127, -10, -10, -9, 119, -2, 127, -3, -3, -3, -3, -3, -3, -3, -22, -3, -3, -15, 24, -3, -3, -3, 3, -3, -3, 33, 1, 24, 1, -3, -3, -3, 45, 2, -12, 8, 8, 9, 9, 9, 9, 8, 8, 7, 4, 98, 75, 8, 6, 8, 88, 8, 9, 100, -50, -55, 5, 8, 8, 5, 127, 1, 37, 2, 2, 2, 2, 2, 2, 2, 15, 1, 1, 127, 14, 2, 1, 2, -3, 2, 2, 0, 21, 1, 12, 2, 2, 1, 48, 0, -50, 0, 0, 0, 0, 0, 0, 0, 48, 0, 0, 30, 48, 0, -1, 0, 70, 0, 0, 102, -9, 8, 66, 0, 0, -1, 127, -2, -5, -4, -4, -4, -4, -4, -4, -4, 92, -4, -4, 8, 68, -4, -4, -4, 127, -4, -4, 60, 2, 1, -1, -4, -4, -4, 32, 0, 127, 3, 4, 4, 4, 4, 4, 4, 12, 2, 0, 54, 28, 3, 1, 3, 70, 4, 4, 27, -25, 17, -1, 4, 4, 1, 13, -2, 81, 0, 0, 0, 1, 0, 0, 0, 28, -1, -2, 127, 37, 0, -1, 0, 30, 0, 0, -21, 27, 38, -4, 1, 0, -2, 35, -6, 1, -12, -12, -12, -12, -12, -12, -12, 54, -11, -8, 55, 30, -12, -10, -12, 60, -12, -12, 127, 29, 34, 17, -12, -12, -9, 91, -1, -12, -3, -3, -3, -3, -3, -3, -3, -3, -3, -2, 127, 29, -3, -3, -3, 7, -3, -3, -14, 14, 47, -18, -3, -3, -3, 50, -4, 99, -8, -8, -8, -8, -8, -8, -8, 127, -7, -6, 89, 5, -8, -7, -8, 32, -8, -8, 55, -25, -49, 16, -8, -8, -6, 114, -3, 78, -2, -2, -2, -2, -2, -2, -2, 85, -3, -4, 44, 127, -2, -3, -2, 29, -2, -2, 83, -49, 28, 2, -2, -2, -3, 57, 2, -46, 10, 12, 12, 12, 12, 12, 12, 64, 8, 4, 90, 57, 11, 7, 11, 107, 12, 12, 12, 42, 56, 127, 12, 12, 5, 42, -8, 123, -10, -10, -10, -10, -10, -10, -10, 68, -10, -10, 58, 127, -10, -10, -10, 15, -10, -10, 61, -2, 34, -59, -10, -10, -10, 1, -5, 0, -9, -9, -9, -9, -9, -9, -9, 95, -8, -7, 127, 81, -9, -8, -9, 59, -9, -9, 56, 47, -32, -77, -9, -9, -7, 42, -5, 63, -6, -6, -6, -7, -6, -6, -6, 66, -6, -6, 40, 127, -6, -6, -6, 92, -6, -6, -23, -11, 60, 60, -7, -7, -6, 24, -1, 76, -2, -2, -2, -2, -2, -2, -2, 48, -2, -1, 127, 34, -2, -2, -2, 78, -2, -2, 23, 7, 25, -88, -2, -2, -2, 68, -2, -14, -6, -6, -6, -6, -6, -6, -6, -3, -5, -3, 127, 35, -6, -5, -6, 83, -6, -6, -22, 11, -25, -55, -6, -6, -4, 42, -1, 41, 0, 0, 1, 0, 0, 0, 0, 37, 0, -1, 127, 31, 0, 0, 0, 88, 0, 0, 58, 32, 14, 49, 0, 0, -1, 30, -6, 97, -5, -5, -4, -5, -5, -5, -5, 44, -5, -8, 127, -12, -5, -6, -5, 42, -4, -4, 3, 55, -18, -73, -5, -5, -7, 5, -7, 89, -15, -16, -16, -16, -16, -16, -16, -65, -14, -11, -32, -81, -16, -14, -15, -40, -16, -16, 31, -9, 0, 48, -17, -16, -12, 127, -7, 106, -11, -10, -11, -10, -10, -10, -10, 109, -11, -11, 104, 107, -11, -11, -11, 112, -10, -10, 127, 96, 50, 61, -10, -10, -12, 110, -4, 127, -1, 0, 0, 0, 0, 1, 0, 45, -2, -6, -60, 125, 0, -3, 0, 77, 0, 0, 49, 34, -9, -38, 0, 0, -5, 118, 0, 81, 0, 0, 0, 0, 0, 0, 0, 19, 0, 1, 127, 14, 0, 0, 0, -7, 0, 0, 57, -24, 10, 1, 0, 0, 1, 36, 2, -2, 8, 9, 8, 10, 10, 9, 9, -26, 6, 4, 120, 18, 8, 5, 8, 32, 9, 9, 69, -76, 72, 110, 10, 10, 4, 127, -4, 20, -6, -5, -5, -5, -5, -5, -5, 37, -6, -6, 106, 113, -6, -6, -6, 61, -5, -5, 63, 55, -15, 45, -5, -5, -7, 127, -1, 33, 1, 1, 1, 2, 2, 2, 1, 112, 0, -2, 12, 91, 1, -1, 1, 127, 1, 1, 100, -14, 61, 46, 2, 1, -2, -13, -3, 22, -5, -5, -5, -5, -5, -5, -5, 43, -5, -5, 41, 127, -5, -5, -5, 34, -5, -5, 41, 64, -51, -46, -5, -5, -6, 58, -3, 66, -6, -7, -7, -7, -7, -7, -7, 96, -6, -4, 85, 116, -6, -5, -6, 56, -7, -7, 31, 27, 127, 32, -7, -7, -5, -19, -1, 33, -4, -4, -4, -3, -3, -3, -4, -28, -4, -4, 125, 78, -4, -4, -4, 45, -4, -4, 47, -16, 44, 51, -3, -3, -5, 127, -3, -6, -6, -6, -6, -6, -6, -6, -6, 44, -5, -4, 74, 112, -6, -5, -6, 127, -6, -6, 48, 58, 32, 13, -6, -6, -4, -38, -6, 27, -13, -11, -13, -11, -11, -12, -11, 127, -13, -12, -64, 66, -13, -13, -12, 127, -12, -12, 42, 59, 52, 22, -11, -11, -12, 116, 1, 19, -1, -1, -1, -1, -1, -1, -1, 73, 0, 1, -89, 69, -1, 0, -1, 127, -1, -1, 42, -19, -3, 45, -1, -1, 1, 85, -2, 36, -4, -4, -4, -4, -4, -4, -4, 86, -4, -4, 9, 109, -4, -4, -4, 127, -4, -4, -17, 51, 44, 3, -4, -4, -4, -22, -6, -64, -10, -10, -10, -10, -10, -10, -10, 81, -10, -9, -59, 127, -10, -10, -10, 63, -10, -10, 70, 61, -17, -16, -10, -10, -10, 54, -5, 20, -7, -7, -7, -7, -7, -7, -7, 99, -8, -8, 40, 118, -7, -8, -7, 127, -7, -7, 57, 25, 10, 101, -7, -7, -8, 96, 0, 34, -3, -2, -3, -2, -2, -3, -2, 91, -2, -1, 127, 45, -3, -2, -2, -23, -3, -3, 8, 55, 42, 31, -2, -2, -2, 41, 0, -21, -4, -4, -4, -4, -4, -4, -4, 20, -4, -2, 1, 11, -4, -4, -4, 39, -4, -4, -2, 127, -16, 44, -3, -3, -3, -5, -7, 47, -14, -15, -15, -15, -15, -15, -15, 127, -14, -10, 68, 112, -14, -13, -14, 68, -15, -15, 118, 69, 82, -124, -15, -14, -12, 17, -3, 25, -6, -6, -6, -6, -6, -6, -6, -1, -6, -5, 44, 91, -6, -6, -6, 75, -6, -6, 47, 127, -35, 7, -6, -6, -6, 68, -1, -58, -2, -2, -2, -2, -2, -2, -2, 1, -2, -1, 127, -5, -2, -2, -2, 40, -2, -2, 29, 64, 17, -38, -2, -2, -2, 28, -4, 127, 1, 2, 2, 2, 2, 2, 2, 26, 0, -4, -6, 29, 2, -1, 2, 53, 2, 2, 8, 4, -3, 67, 2, 2, -3, -1, 2, 19, 4, 3, 4, 3, 3, 4, 3, -5, 3, 2, -2, 12, 4, 3, 4, 17, 4, 4, 7, 34, -127, 35, 3, 3, 3, 14, -1, 12, -1, -1, -1, -1, -1, -1, -1, -2, -1, -2, 7, -9, -1, -1, -1, 69, -1, -1, 82, 42, -127, 14, -1, -1, -2, -18, -3, 64, -5, -5, -5, -5, -5, -5, -5, 77, -5, -5, -27, 66, -5, -5, -5, 65, -5, -5, 127, 42, 3, 15, -5, -5, -5, 78, -7, 110, -10, -10, -10, -10, -10, -10, -10, 95, -10, -10, 53, 127, -10, -10, -10, 101, -10, -10, 29, 38, 36, 76, -10, -10, -10, 87, 3, 53, 1, 2, 1, 2, 2, 1, 2, -100, 2, 2, 55, 100, 1, 2, 2, 90, 1, 1, 94, 42, -6, 66, 2, 2, 2, 127, -4, 31, -6, -6, -6, -6, -6, -6, -6, 30, -6, -6, 54, 52, -6, -6, -6, 8, -6, -6, 28, 127, 79, -18, -6, -6, -6, 69, -5, 5, -9, -9, -9, -8, -9, -9, -9, 18, -9, -8, 127, 42, -9, -9, -9, 118, -9, -9, 47, 45, 38, 106, -8, -8, -9, 75, -1, 127, -1, -1, -1, -1, -1, -1, -1, 50, -1, -1, 119, 53, -1, 0, -1, 2, -1, -1, 64, 64, -69, 68, -1, -1, 0, 88, 0, 127, 3, 5, 4, 6, 5, 5, 5, 9, 1, -1, 92, 31, 3, 0, 3, 21, 4, 4, -28, 43, 2, -42, 6, 5, -1, 17, -2, 50, -2, -2, -2, -2, -2, -2, -2, 9, -2, -3, 127, -16, -2, -2, -2, 2, -2, -2, -8, 16, 16, 48, -2, -2, -3, 24, -2, -19, -3, -2, -3, -2, -2, -3, -2, -13, -3, -2, 13, 60, -3, -3, -3, 0, -3, -3, 27, -8, -18, 78, -2, -2, -3, 127, -7, 68, -9, -8, -8, -8, -8, -8, -9, 97, -9, -9, 51, 127, -9, -9, -9, 72, -8, -8, 12, -11, 41, 65, -8, -9, -9, -2, -4, 2, -4, -5, -4, -6, -5, -5, -5, 50, -5, -7, 42, 43, -4, -5, -5, 123, -5, -5, -127, -3, -108, 23, -6, -6, -6, 92, -1, 53, -1, 0, 0, 1, 0, 0, 0, 75, -1, -2, 13, 74, 0, -2, 0, 104, 0, 0, 80, 110, 0, -72, 1, 1, -2, 127, -5, 43, -8, -8, -8, -8, -8, -8, -8, 68, -8, -7, -8, 34, -8, -8, -8, 96, -8, -8, 30, 66, 39, -32, -8, -8, -8, 127, 0, -19, 6, 6, 7, 6, 7, 7, 6, 37, 5, 0, 67, 13, 6, 4, 6, 97, 7, 7, 35, 44, 60, 127, 6, 6, 2, 55, -3, 27, -4, -3, -3, -3, -3, -3, -3, 92, -4, -5, 87, 111, -4, -5, -4, 80, -3, -3, 8, 127, 32, -30, -3, -3, -5, -19, -4, 9, -4, -4, -4, -5, -4, -4, -4, -9, -4, -5, 127, -2, -4, -4, -4, -42, -4, -4, -12, -24, 22, 14, -5, -5, -4, -46, 4, 104, 4, 5, 4, 6, 5, 5, 5, 85, 3, 3, 87, 79, 4, 3, 4, 41, 4, 5, 118, -9, 23, 89, 6, 6, 3, 127, -1, -24, -1, -1, 0, -1, -1, -1, -1, 7, -1, -1, -19, 93, -1, -1, -1, -31, -1, -1, 47, 25, -127, 52, -1, -1, -1, -13, -8, 49, -16, -16, -16, -17, -16, -16, -16, 127, -15, -13, 105, 53, -16, -15, -16, -16, -16, -16, 15, 84, 54, 18, -17, -17, -14, 56, 7, 41, 8, 9, 8, 10, 9, 9, 9, 127, 7, 8, 61, 32, 8, 7, 9, 91, 9, 9, 43, 27, 17, 86, 10, 10, 7, 81, 0, 29, 0, 0, 0, 0, 0, 0, 0, 94, 0, 0, -41, 73, 0, 0, 0, 37, 0, 0, 127, -53, -6, 36, 0, 0, 0, 77, 5, 76, 10, 10, 10, 11, 11, 10, 10, 93, 9, 8, 127, 51, 10, 9, 10, 78, 10, 10, 59, -69, -36, 25, 11, 11, 8, 81, -3, 97, -6, -5, -6, -5, -5, -5, -5, 96, -6, -6, -2, 43, -6, -6, -6, 98, -5, -5, 127, 24, 17, 11, -5, -5, -6, 93, 0, 16, -1, 0, -1, 0, 0, 0, 0, 47, -1, -1, 55, -1, 0, -1, 0, 100, 0, 0, 10, 127, -21, -40, 1, 1, -1, 102, -4, 90, -5, -5, -4, -5, -5, -5, -5, 127, -5, -6, 71, 79, -5, -5, -5, -18, -5, -5, 44, 51, -10, 46, -6, -6, -6, 41, -1, 53, -3, -2, -3, -2, -2, -2, -2, 18, -2, -2, 127, 44, -2, -3, -2, 4, -2, -2, 22, -37, 43, -8, -2, -2, -2, 42, -2, 110, -4, -4, -4, -4, -4, -4, -4, 58, -4, -3, 93, 115, -4, -4, -4, 46, -4, -4, 59, 2, -50, 127, -4, -4, -3, 113, -4, 68, -8, -8, -8, -8, -8, -8, -8, 84, -8, -7, 127, 109, -8, -8, -8, 56, -8, -8, 13, -5, 55, 126, -8, -8, -8, 94, 0, 50, 1, 1, 1, 1, 1, 1, 1, -20, 1, -1, -37, 127, 1, 0, 1, -6, 1, 1, 19, 33, -85, 44, 1, 1, -1, -67, 3, 48, 6, 7, 7, 7, 7, 7, 7, 72, 6, 4, 81, 72, 7, 6, 7, 13, 7, 7, 75, 47, -127, -67, 7, 7, 5, 72, 4, 48, 8, 8, 9, 8, 8, 8, 8, 5, 7, 4, -127, 36, 8, 7, 8, 119, 8, 8, 99, 34, -1, 33, 8, 8, 5, 49, -7, 127, -7, -6, -6, -6, -6, -6, -6, 1, -8, -9, 55, 26, -7, -8, -7, 54, -6, -6, 3, -20, -24, 23, -6, -6, -9, 14, -1, 28, -1, -1, -1, -1, -1, -1, -1, 109, -1, -1, -127, 64, -1, -1, -1, 42, -1, -1, 6, 10, 1, 50, -1, -1, -1, 7, -5, -51, -5, -4, -5, -4, -4, -4, -4, 28, -6, -6, 50, 84, -5, -6, -5, 127, -5, -5, 70, -18, -7, 92, -4, -4, -7, 34, 0, -33, -1, 0, -1, 0, 0, 0, 0, 42, -2, -1, -12, 73, -1, -2, -1, 89, -1, 0, 54, 31, 49, -15, 0, 0, -2, 127, -1, 127, -2, -1, -2, -1, -1, -1, -2, 39, -3, -3, 46, 23, -2, -3, -2, 44, -2, -1, -18, 93, 17, 3, -1, -1, -3, -3, 1, -4, 2, 1, 1, 1, 1, 1, 1, 60, 2, 2, 33, -11, 1, 2, 1, 110, 1, 1, 27, 127, -44, 24, 1, 1, 2, 126, 3, 45, 7, 7, 7, 8, 8, 8, 7, -12, 6, 4, 127, -19, 7, 5, 7, -22, 7, 7, 68, -68, 7, 104, 8, 8, 4, 83, 5, 99, 9, 9, 9, 10, 10, 10, 9, 38, 8, 7, 127, 70, 9, 8, 9, 37, 9, 9, 84, 5, 75, 42, 10, 10, 7, 97, -1, 48, -6, -7, -7, -7, -7, -7, -7, 49, -6, -4, 127, 60, -7, -6, -6, 32, -7, -7, 118, -55, -30, -116, -7, -7, -5, 119, -3, 21, -6, -6, -6, -6, -6, -6, -6, 127, -6, -5, 78, 49, -6, -6, -6, 48, -6, -6, 14, 60, 37, 111, -6, -6, -5, 55, -3, -38, -4, -4, -4, -4, -4, -4, -4, 125, -4, -5, 74, 114, -4, -5, -4, 127, -4, -4, 66, 18, 57, 63, -4, -4, -5, -44, -3, 119, -4, -3, -4, -3, -3, -3, -3, 18, -5, -5, 88, 127, -4, -5, -4, 90, -4, -4, 50, 115, 95, 64, -3, -3, -6, 42, -2, 59, 0, 0, 1, 0, 0, 0, 0, 102, 0, -2, 9, 93, 0, -1, 0, 20, 0, 0, 53, -32, 25, -127, 0, 0, -2, 36, -1, 31, 0, 0, 1, 0, 1, 1, 0, 4, 0, -2, 0, 127, 0, -1, 0, -87, 1, 1, 38, 33, -95, 34, 0, 0, -2, -22, -3, 127, -5, -6, -5, -6, -6, -5, -6, 95, -5, -6, 32, 64, -5, -5, -5, 47, -5, -5, 68, -38, -15, -52, -6, -6, -6, 87, -6, 127, -11, -11, -11, -12, -12, -11, -12, 90, -11, -10, 3, 27, -11, -10, -11, 26, -11, -11, 84, -76, -38, 3, -12, -12, -10, -33, -2, 127, -2, -2, -2, -2, -2, -2, -2, 78, -2, -2, 106, 93, -2, -3, -2, 76, -2, -2, 123, -17, 86, -37, -1, -2, -3, 47, -3, -50, -5, -5, -5, -4, -5, -5, -5, 78, -5, -5, 63, 80, -5, -5, -5, 75, -5, -5, 4, 40, -38, 59, -4, -5, -5, 127, -7, 74, -13, -13, -13, -13, -13, -13, -13, 50, -13, -12, -3, 66, -13, -12, -13, 94, -13, -13, 17, -57, 13, -105, -13, -13, -12, 127, -2, -15, -5, -6, -6, -6, -6, -6, -6, 42, -5, -4, 42, 66, -5, -4, -5, 80, -6, -6, 56, -64, -25, 3, -6, -6, -4, 127, -3, 34, -5, -5, -5, -5, -5, -5, -5, 41, -5, -4, 36, -3, -5, -5, -5, 32, -5, -5, 49, 18, -23, 63, -5, -5, -5, 127, -4, 127, -5, -4, -4, -4, -4, -4, -4, 48, -5, -5, 47, 31, -4, -5, -4, 20, -4, -4, 18, 61, 10, 1, -4, -4, -6, 65, -10, 127, -11, -11, -11, -11, -11, -11, -11, 104, -12, -13, -14, -57, -11, -12, -11, -34, -11, -11, 40, 121, 14, 62, -11, -11, -13, 119, 3, 22, 9, 11, 10, 11, 11, 11, 11, 71, 8, 5, 6, 127, 10, 7, 10, 43, 10, 11, 69, -25, 29, 77, 11, 11, 6, 87, -1, -16, -3, -3, -3, -3, -3, -3, -3, 77, -4, -3, 58, 25, -3, -4, -3, 127, -3, -3, 14, 86, -57, 7, -3, -3, -4, 66, -1, 106, 0, 1, 1, 1, 1, 1, 1, 55, -1, -2, 85, 53, 0, -1, 0, 5, 1, 1, 24, -32, 30, 106, 1, 1, -2, 127, 1, -20, 8, 9, 9, 9, 9, 9, 9, 90, 7, 3, -76, 127, 8, 6, 8, 114, 9, 9, 36, 125, 13, -11, 9, 9, 4, 38, -4, 79, -4, -4, -4, -4, -4, -4, -4, 33, -5, -4, 23, 36, -4, -5, -4, 50, -4, -4, 60, -50, 33, 127, -4, -4, -5, 58, 3, 38, 0, 1, 0, 1, 1, 1, 1, 64, 0, 1, 47, 34, 0, 0, 1, 51, 1, 1, 70, -97, -18, -26, 2, 1, 0, 127, -1, 88, 0, 1, 1, 1, 1, 1, 0, 70, 0, -1, 91, 11, 0, -1, 0, 45, 1, 1, 38, -6, -37, 22, 1, 0, -1, 127, -10, 96, -10, -9, -9, -9, -9, -9, -9, 119, -10, -12, -88, 127, -10, -11, -10, 35, -9, -9, 92, -102, 115, -25, -9, -9, -12, 87, -2, 59, -6, -6, -7, -6, -6, -6, -6, 127, -6, -4, 62, 66, -6, -6, -6, 69, -6, -6, 93, -113, -1, 48, -6, -6, -5, 79, -4, 76, -10, -9, -10, -9, -9, -9, -9, 54, -9, -7, 127, 30, -10, -9, -9, 31, -10, -10, 62, 38, 15, 84, -9, -9, -8, 78, -3, 79, -5, -5, -5, -5, -5, -5, -5, 127, -4, -4, 38, 120, -5, -4, -5, 13, -5, -5, 47, 18, 17, 22, -5, -5, -4, 104, -3, 13, -7, -7, -8, -7, -7, -7, -7, 127, -7, -5, -2, 59, -7, -7, -7, 41, -7, -7, 73, 47, -9, 11, -7, -7, -6, 26, -2, 113, -2, -1, -1, -1, -1, -1, -1, 99, -2, -3, 126, 127, -1, -2, -1, 60, -1, -1, 85, -13, -2, -57, -1, -1, -3, 65, 4, 86, 9, 10, 9, 10, 10, 10, 10, 24, 8, 7, 0, 97, 9, 8, 9, 62, 9, 9, 45, 78, 61, 20, 10, 10, 7, 127, -4, 9, -5, -5, -5, -6, -5, -5, -6, 127, -5, -7, 75, 43, -5, -6, -5, 98, -5, -5, -68, 88, -26, 22, -6, -6, -7, 43, -5, 54, -9, -9, -9, -9, -9, -9, -9, 127, -9, -8, -2, 64, -9, -8, -9, 58, -9, -9, 2, 29, -29, 28, -9, -9, -8, 87, -4, 52, -7, -7, -7, -7, -7, -7, -7, 13, -8, -7, 40, 127, -7, -8, -7, 112, -7, -7, 40, 75, 12, 40, -7, -7, -7, 64, 0, 11, 0, 0, 0, 0, 0, 0, 0, 127, 0, 0, 42, 87, 0, 0, 0, 88, 0, 0, 121, -106, 36, 18, 0, 0, 0, 120, 1, -3, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 127, -6, 1, 1, 1, 1, 1, 1, 2, 23, 20, -10, 1, 1, 1, -2, 0, 24, 1, 0, 1, 0, 0, 1, 0, 29, 1, -1, -19, -45, 1, 1, 1, 86, 1, 1, -29, 17, -127, -18, 0, 0, 0, 24, -3, -7, -5, -5, -5, -4, -5, -5, -4, 24, -5, -4, -37, 76, -5, -5, -5, 33, -5, -5, 73, 37, 25, 59, -4, -4, -4, 127, -4, -22, -8, -8, -8, -8, -8, -8, -8, 59, -8, -7, 53, 84, -8, -8, -8, 55, -8, -8, 6, 87, 37, 47, -8, -8, -7, 127, 1, -28, 2, 2, 2, 2, 2, 2, 2, -29, 2, 2, 127, 24, 2, 2, 2, 13, 2, 2, 115, 30, -48, -27, 2, 2, 2, 78, -4, 102, -6, -6, -6, -6, -6, -6, -6, 127, -6, -5, 22, 78, -6, -6, -6, 114, -6, -6, 24, 9, 60, 46, -6, -6, -6, 52, -4, 127, -6, -6, -6, -6, -6, -6, -6, -32, -6, -6, 69, 59, -6, -6, -6, -3, -6, -6, 28, 48, 24, -28, -6, -6, -6, 83, -4, 117, -9, -9, -10, -9, -9, -9, -9, -22, -9, -7, 19, 64, -9, -9, -9, -107, -9, -9, 54, 95, 14, -8, -9, -9, -8, 127, 1, -7, 5, 6, 6, 7, 7, 7, 6, 85, 4, 2, 97, -56, 6, 3, 6, 23, 6, 6, 124, 20, 91, 66, 7, 6, 2, 127, -2, 26, -4, -3, -3, -3, -3, -3, -3, 63, -4, -4, 30, 82, -3, -4, -3, 76, -3, -3, -6, 71, 45, -127, -3, -3, -5, 30, -1, -6, -4, -4, -4, -5, -4, -4, -4, 61, -4, -2, 7, -50, -4, -3, -4, -34, -4, -4, 127, 48, 36, 22, -4, -4, -3, 108, -3, 15, -10, -10, -10, -10, -10, -10, -10, 17, -9, -7, -8, 126, -10, -9, -10, 14, -10, -10, -28, 127, 20, 79, -10, -10, -8, 51, -1, -7, -4, -4, -4, -4, -4, -4, -4, 77, -3, -2, 16, 12, -4, -3, -4, 107, -4, -4, 127, 29, 18, 23, -4, -4, -2, 29, -3, 14, -1, -1, -1, -1, -1, -1, -1, 127, -1, -1, 113, 73, -1, -1, -1, 114, -1, -1, 28, -20, 36, 118, -1, -1, -1, -15, 1, 15, 3, 3, 4, 3, 3, 3, 3, 127, 3, 1, 11, 52, 3, 3, 3, 97, 4, 3, 7, 85, -118, -4, 3, 3, 2, 43, -5, 102, -6, -6, -6, -5, -6, -6, -6, 27, -7, -8, -42, 27, -6, -7, -6, 17, -6, -6, 123, 5, 15, -5, -5, -6, -8, 127, -7, 127, -11, -10, -10, -10, -10, -10, -10, 48, -11, -11, 101, 9, -10, -11, -10, -43, -10, -10, 102, 88, -5, 12, -10, -10, -11, 120, 0, -16, -2, -2, -2, -2, -2, -2, -2, 24, -2, -1, 8, 24, -2, -2, -2, 57, -2, -2, -38, 37, -127, 20, -2, -2, -1, 34, -6, 102, -7, -7, -6, -7, -7, -7, -7, 104, -7, -8, 53, -12, -7, -7, -7, 6, -7, -7, 17, 81, -127, 26, -7, -7, -8, -6, -1, 127, -2, -2, -2, -2, -2, -2, -2, 38, -2, -2, 46, 75, -2, -2, -2, 48, -2, -2, 71, 59, 73, 73, -2, -2, -2, 62, -3, 47, -7, -8, -8, -8, -8, -8, -8, 54, -7, -5, 46, 27, -8, -7, -8, 16, -8, -8, 127, 35, 25, -14, -8, -8, -6, 97, 2, 22, 0, 0, 0, 0, 0, 0, 0, 44, 1, 2, 127, 26, 0, 1, 0, 62, 0, 0, 29, -78, 60, -7, 0, 0, 1, 23, -6, 49, -11, -11, -10, -11, -11, -11, -11, 28, -11, -10, 38, 34, -11, -11, -11, 127, -11, -11, -25, 65, 43, 28, -11, -11, -11, 24, 3, 19, 5, 6, 6, 6, 6, 6, 6, 111, 5, 4, 127, 59, 6, 5, 6, 50, 6, 6, 70, 6, 47, 21, 6, 6, 4, 38, -4, -66, -12, -11, -12, -10, -11, -11, -11, 18, -11, -6, 16, 127, -12, -10, -11, 56, -12, -11, 35, -3, 8, 40, -10, -10, -8, 75, 0, -11, 3, 4, 4, 4, 4, 4, 3, 118, 2, -1, 49, -32, 3, 2, 3, 104, 4, 4, 91, -45, 127, 56, 3, 3, 0, 97, -1, -2, -2, -3, -3, -3, -3, -3, -3, 20, -2, -2, 38, 41, -3, -2, -3, 39, -3, -3, -22, 127, 11, -21, -3, -3, -2, 33, -3, 8, -3, -3, -3, -3, -3, -3, -3, 104, -3, -4, 5, 123, -3, -3, -3, 119, -3, -3, 127, -5, 77, 64, -3, -3, -4, 76, -6, 40, -10, -10, -10, -10, -10, -10, -10, 69, -10, -9, 95, 92, -10, -10, -10, 127, -10, -10, 63, 74, 17, 36, -10, -10, -10, 76, -2, -47, -5, -5, -5, -5, -5, -5, -5, 1, -5, -4, 127, 31, -5, -5, -5, 34, -5, -5, 54, 65, 54, 53, -5, -5, -5, 37, -6, 56, -9, -9, -9, -9, -9, -9, -9, 54, -9, -9, 127, 87, -9, -9, -9, 62, -9, -9, 51, 62, -5, 29, -9, -9, -9, 42, -2, -28, -4, -4, -4, -4, -4, -4, -4, -10, -4, -3, 127, 6, -4, -3, -4, 10, -4, -4, 18, 20, 11, -21, -3, -3, -3, 38, 4, -73, 6, 7, 6, 7, 7, 7, 7, 41, 5, 4, 59, 127, 6, 4, 6, 105, 6, 6, 73, -54, 21, 0, 7, 7, 4, 118, -3, -12, -4, -4, -4, -4, -4, -4, -4, 93, -4, -4, 127, 46, -4, -4, -4, 27, -4, -4, 40, -7, 1, 95, -4, -4, -5, 94, -2, -17, -2, -2, -2, -2, -2, -2, -2, 47, -2, -3, 127, 25, -2, -3, -2, 11, -2, -2, -20, 4, 6, 14, -2, -2, -3, 35, -2, 5, -4, -5, -4, -5, -5, -5, -5, 23, -4, -3, 7, 25, -4, -4, -4, 13, -5, -5, 19, 16, -127, 25, -5, -5, -4, 18, -3, -12, -5, -5, -5, -5, -5, -5, -5, 34, -5, -4, 77, 59, -5, -5, -5, 127, -5, -5, 38, 55, -5, -6, -5, -5, -5, 48, 4, 10, 7, 6, 7, 6, 6, 7, 6, 65, 7, 5, -26, 127, 7, 6, 7, 119, 7, 7, 35, -88, 25, 22, 6, 6, 6, 23, -5, 12, -8, -8, -8, -8, -8, -8, -8, 67, -8, -9, 91, 38, -8, -9, -8, 53, -8, -8, 25, 127, 59, -6, -8, -8, -9, 110, 3, 75, 2, 1, 1, 1, 1, 1, 1, 77, 2, 5, 122, 74, 1, 3, 2, 127, 1, 1, 89, -112, 47, 64, 1, 2, 4, 12, 2, -61, -4, -4, -4, -4, -4, -4, -4, 73, -2, 2, 127, -29, -4, -1, -3, 107, -4, -4, 117, -69, 71, -13, -4, -4, 0, 21, -2, 11, 2, 2, 2, 2, 2, 2, 2, 127, 1, -1, 51, -52, 2, 1, 2, 91, 2, 2, -35, 12, 2, 37, 2, 2, 0, -9, -6, 45, -9, -9, -9, -9, -9, -9, -9, 46, -9, -8, -60, 71, -9, -9, -9, 127, -9, -9, 44, 34, 44, 91, -9, -9, -9, 32, -5, 71, -11, -11, -11, -11, -11, -11, -11, 127, -10, -8, 29, 71, -11, -10, -11, 105, -11, -11, 106, -33, 34, 93, -11, -11, -9, 6, 3, 118, 7, 9, 8, 10, 10, 9, 9, 40, 6, 2, 127, 42, 8, 4, 8, 21, 9, 9, 41, 25, -8, -12, 10, 10, 3, 65, -4, 86, -3, -3, -3, -3, -3, -3, -3, 22, -4, -6, 127, 64, -3, -5, -3, 73, -3, -3, 52, 33, 8, 103, -3, -3, -6, 90, -6, 127, -12, -12, -12, -12, -12, -12, -12, 126, -11, -10, 102, 76, -12, -11, -12, -48, -12, -12, -77, 75, 62, 46, -12, -12, -10, 120, -5, 67, -4, -4, -4, -4, -4, -4, -4, 89, -5, -6, 101, 127, -4, -5, -4, 89, -4, -4, 110, -92, -2, 96, -4, -4, -6, 70, -4, 127, -4, -3, -3, -3, -3, -3, -3, 20, -4, -5, 74, 90, -4, -5, -4, 13, -3, -3, -6, -68, 62, 38, -3, -3, -5, 56, -5, 108, -10, -9, -9, -9, -9, -9, -9, 62, -10, -10, 127, 113, -9, -10, -9, 124, -9, -9, 49, 60, 51, -7, -9, -9, -10, 82, 0, 28, -1, -2, -1, -2, -2, -2, -2, -9, -2, -2, -47, 21, -2, -2, -2, 4, -2, -2, 58, 44, -127, -1, -2, -2, -2, 44, -4, 90, -7, -7, -7, -7, -7, -7, -7, 47, -7, -7, 43, 99, -7, -7, -7, 70, -7, -7, 27, 20, 25, -69, -7, -7, -8, 127, 4, -14, 2, 2, 2, 2, 2, 2, 2, 40, 3, 5, 67, 104, 2, 4, 2, 91, 2, 2, 6, -69, 33, 126, 2, 2, 5, 127, -1, -22, -5, -5, -5, -6, -6, -6, -5, 64, -5, -3, 0, 119, -5, -4, -5, -24, -5, -5, 110, 127, -81, 74, -6, -6, -4, 70, -7, 122, -10, -10, -10, -10, -10, -10, -10, 65, -10, -9, -6, 127, -10, -10, -10, 6, -10, -10, 76, -2, 68, -101, -10, -10, -9, 106, -5, -8, -10, -10, -11, -10, -10, -10, -10, 101, -10, -8, 127, 68, -10, -10, -10, 104, -10, -10, 31, 76, 46, -13, -10, -10, -9, 16, -4, 13, -5, -5, -5, -5, -5, -5, -5, -1, -5, -6, 49, 70, -5, -5, -5, 9, -5, -5, 30, 36, 17, 18, -6, -6, -6, 127, -3, 75, -7, -7, -7, -6, -7, -7, -7, 42, -7, -6, 24, 127, -7, -7, -7, 59, -7, -7, 36, 55, 52, -19, -6, -6, -7, 77, -2, 63, -4, -4, -4, -4, -4, -4, -4, 36, -4, -4, -54, 99, -4, -4, -4, 58, -4, -4, 45, 46, -127, 29, -4, -4, -5, 18, -2, -79, -4, -3, -4, -3, -3, -3, -3, 102, -4, -4, -6, 127, -4, -5, -4, 115, -4, -4, 14, 21, 91, 86, -2, -3, -4, 57, -2, 26, 0, 0, 0, 1, 1, 0, 0, 18, -1, -2, 36, 127, 0, -1, 0, 51, 0, 0, -49, 24, 6, -4, 1, 1, -2, 55, -5, 127, -4, -3, -4, -2, -3, -3, -3, 83, -6, -7, 89, 113, -4, -6, -4, 75, -3, -3, 9, 15, 78, 34, -2, -3, -7, 4, -1, 116, 2, 3, 3, 3, 3, 3, 3, 80, 2, 0, 54, 127, 2, 1, 2, 103, 3, 3, 109, -24, 101, 124, 3, 3, 0, 64, -2, 15, -4, -4, -4, -4, -4, -4, -4, 5, -4, -4, 127, 64, -4, -4, -4, 88, -4, -4, 32, -18, 27, -58, -4, -4, -4, 44, -2, 125, -6, -6, -6, -6, -6, -6, -6, 105, -7, -6, -13, 127, -6, -7, -6, 26, -6, -6, 22, 64, 47, -53, -6, -6, -6, 58, -2, 76, -2, -2, -2, -2, -2, -2, -2, 46, -2, -3, 35, 62, -2, -3, -2, 40, -2, -2, 74, -25, 34, -127, -2, -2, -3, 28, -3, 58, -6, -6, -6, -6, -6, -6, -6, 83, -6, -5, 127, 60, -6, -6, -6, 57, -6, -6, 56, 6, -6, 19, -6, -6, -5, 84, -5, 81, -4, -4, -4, -4, -4, -4, -4, 14, -5, -7, 10, 127, -4, -6, -4, 61, -4, -4, 8, 12, 18, 45, -4, -4, -6, 43, -3, -5, -3, -3, -3, -3, -3, -3, -3, 127, -3, -3, 40, 23, -3, -4, -3, 81, -3, -3, 36, 31, 71, 65, -3, -3, -4, -2, -4, 101, -4, -3, -3, -3, -3, -3, -3, 16, -4, -5, 127, 78, -3, -5, -3, 112, -3, -3, 4, 50, 22, 95, -3, -3, -5, 76, -3, -12, -4, -4, -5, -5, -5, -5, -4, 42, -4, -3, 127, 45, -4, -4, -4, 27, -4, -4, 70, -5, 58, 18, -4, -4, -4, -14, -4, 127, -4, -4, -4, -4, -4, -4, -4, 68, -5, -6, 32, 79, -4, -5, -4, 45, -4, -4, 72, -28, 51, -7, -4, -4, -6, 66, -7, 74, -10, -10, -10, -11, -10, -10, -10, 90, -10, -10, 94, 127, -10, -10, -10, 51, -10, -10, 45, 75, -29, 90, -11, -11, -11, 80, -3, -1, -4, -3, -3, -3, -3, -3, -3, 81, -5, -6, -89, 127, -3, -5, -3, -3, -3, -3, 30, 113, 23, 48, -3, -3, -6, 73, -1, -38, -3, -3, -4, -3, -3, -3, -3, 4, -3, -2, -34, 34, -3, -3, -3, -25, -3, -3, 60, -42, 42, 23, -3, -3, -3, 127, 2, -34, 7, 6, 8, 6, 6, 7, 6, 34, 6, 2, 82, 30, 7, 6, 6, 31, 7, 7, 43, -16, 16, 9, 6, 6, 4, 127, 2, 53, 3, 3, 4, 4, 4, 4, 3, 82, 3, 2, 101, 120, 3, 3, 3, 127, 4, 3, 94, 16, -8, 57, 4, 3, 2, -123, -2, 13, -4, -5, -5, -5, -5, -5, -5, 65, -4, -3, -89, 71, -5, -4, -5, 127, -5, -5, 104, -19, 16, 55, -5, -5, -3, 25, -7, 127, -12, -12, -12, -11, -12, -12, -11, 50, -12, -11, 4, 60, -12, -12, -12, -19, -12, -12, 33, 62, 43, 29, -11, -11, -12, 101, -2, -51, -3, -3, -4, -2, -3, -3, -3, 58, -3, -2, 127, 9, -3, -3, -3, 83, -3, -3, -82, 50, -90, 9, -2, -2, -3, 84, 0, -12, 4, 5, 5, 5, 5, 5, 5, 44, 3, 0, 107, 54, 4, 2, 4, 49, 5, 5, 60, -77, 70, -21, 5, 4, 0, 127, -2, -14, -5, -6, -5, -6, -6, -6, -6, 121, -5, -4, 82, 20, -5, -4, -5, 49, -5, -5, 120, -121, -58, 17, -6, -6, -5, 127, -2, 89, -3, -3, -3, -3, -3, -3, -3, -110, -3, -3, 44, 127, -3, -3, -3, 100, -3, -3, 25, 59, 35, -11, -3, -3, -3, 23, 1, 36, 0, 0, 0, 1, 0, 0, 0, 123, -1, -1, 33, 127, 0, -1, 0, 79, 0, 0, 100, -79, -4, 65, 1, 1, -1, 34, -6, 67, -10, -9, -9, -9, -9, -9, -9, 0, -10, -9, 127, 54, -9, -10, -9, 56, -9, -9, 26, 9, 57, 118, -9, -9, -10, 42, -4, 4, -7, -6, -6, -6, -6, -6, -6, 94, -7, -7, 62, 75, -6, -7, -6, 99, -6, -6, 36, -51, -3, 58, -6, -6, -7, 127, -2, 18, 0, 0, 0, 0, 0, 0, 0, 6, -1, -2, 112, 52, 0, -1, 0, 5, 0, 0, 38, 17, -6, 17, 0, 0, -2, 127, -4, 75, -9, -9, -9, -9, -9, -9, -9, 25, -9, -7, 30, 127, -9, -9, -9, 76, -9, -9, 78, 63, 42, 2, -9, -9, -8, 85, -4, 87, -5, -5, -5, -5, -5, -5, -5, 127, -5, -6, 6, 51, -5, -6, -5, 71, -5, -5, 37, -29, 16, 33, -5, -5, -6, 52, 0, 7, -2, -2, -3, -2, -2, -2, -2, 39, -2, -2, -22, 127, -2, -2, -2, 78, -2, -2, 60, 50, -1, -39, -2, -2, -2, -9, -1, 40, -2, -2, -2, -3, -2, -2, -2, 61, -2, -2, 127, 93, -2, -2, -2, 103, -2, -2, 49, -101, 21, 16, -3, -3, -2, 10, 1, 29, 0, 1, 0, 1, 1, 1, 1, 62, 0, 0, -72, 109, 0, 0, 1, 127, 1, 1, 46, 73, -72, 12, 1, 1, 0, 29, -1, 22, -4, -3, -4, -3, -3, -3, -3, 27, -3, -2, -127, 14, -4, -4, -3, 25, -3, -3, 12, -11, -3, -36, -3, -3, -3, 45, -3, 50, -6, -6, -6, -6, -6, -6, -6, 127, -6, -6, -5, 114, -6, -6, -6, 31, -6, -6, 11, 24, 8, 42, -6, -6, -6, 106, -3, 127, -5, -5, -5, -5, -5, -5, -5, 32, -5, -5, 36, 91, -5, -5, -5, 54, -5, -5, 24, 3, 40, -25, -5, -5, -5, 49, 0, 18, 1, 1, 1, 1, 1, 1, 1, 21, 0, 0, 74, 127, 1, 0, 1, 39, 1, 1, 26, 61, 23, -11, 1, 1, 0, 16, 0, -12, 3, 4, 3, 4, 4, 4, 4, 59, 2, 1, -4, 100, 3, 2, 3, 26, 3, 3, 76, 8, 61, 15, 4, 4, 1, 127, -6, 127, -11, -12, -12, -12, -12, -12, -12, 127, -11, -10, 92, 122, -11, -11, -12, 66, -12, -12, 52, 25, 21, 62, -12, -12, -10, 36, -10, -56, -14, -15, -14, -15, -15, -15, -15, 58, -14, -14, 17, 80, -14, -14, -15, 71, -15, -15, -5, -16, 7, 9, -16, -16, -14, -127, -2, 17, -5, -6, -5, -6, -6, -6, -6, 48, -5, -4, 58, 94, -5, -5, -5, 17, -5, -6, -13, 48, 25, -127, -6, -6, -5, 77, -4, -18, -6, -6, -6, -6, -6, -6, -6, 28, -6, -7, 39, 83, -6, -7, -6, -61, -6, -6, 125, 127, 7, 57, -6, -6, -7, 45, -2, 110, -3, -3, -3, -3, -3, -3, -3, 50, -4, -4, 88, 79, -3, -4, -3, 68, -3, -3, 119, 2, 64, 46, -4, -4, -4, 127, -5, 98, -7, -7, -7, -7, -7, -7, -7, 25, -8, -8, 127, 117, -7, -8, -7, 119, -7, -7, 41, 81, 8, -48, -7, -8, -8, 78, -4, 127, -3, -2, -2, -2, -2, -2, -2, 58, -4, -5, 7, 78, -3, -4, -3, 36, -2, -2, 43, 26, 53, 100, -2, -2, -5, 91, -5, 44, -10, -10, -11, -11, -11, -11, -10, 96, -9, -7, 41, 61, -10, -9, -10, 127, -10, -10, 67, 4, 32, -14, -11, -10, -8, 6, -1, -95, -1, 0, -1, 0, 0, 0, 0, 12, -1, -1, 29, 92, -1, -1, -1, 59, -1, 0, 45, 20, 40, -11, 0, 0, -1, 127, -2, 20, -8, -8, -8, -8, -8, -8, -8, 94, -8, -6, 127, 93, -8, -8, -8, 30, -8, -8, 65, -12, 23, 27, -8, -8, -7, 1, -3, 0, -7, -8, -8, -8, -8, -8, -8, 23, -7, -5, 127, 43, -7, -6, -7, 3, -8, -8, 34, -4, 36, 22, -8, -8, -5, -5, -2, 36, -7, -8, -7, -8, -8, -8, -8, 17, -7, -5, 32, -50, -7, -6, -7, 127, -7, -8, -36, 30, -117, 11, -8, -8, -6, 10, 4, -21, 6, 7, 6, 7, 7, 7, 7, -16, 5, 4, -8, 70, 6, 5, 6, 5, 6, 7, 66, 46, -127, 54, 7, 7, 4, 31, -6, 120, -9, -9, -9, -9, -9, -9, -9, 39, -9, -8, 127, 41, -9, -9, -9, 33, -9, -9, -30, 69, 82, 41, -9, -9, -9, 23, -4, 47, -7, -7, -7, -7, -7, -7, -7, -15, -7, -7, 7, 127, -7, -7, -7, -65, -7, -7, 78, 42, -94, 27, -7, -7, -7, -63, -4, 93, -7, -7, -7, -7, -7, -7, -7, 22, -7, -5, -10, -6, -7, -6, -7, -11, -7, -7, 0, 60, -37, 127, -6, -6, -6, 63, 1, 69, 2, 3, 2, 3, 3, 3, 3, 39, 2, 2, 127, 6, 2, 2, 3, 30, 3, 3, 11, 21, 8, 2, 3, 3, 2, 53, -4, 3, -9, -10, -10, -10, -10, -10, -10, 19, -9, -6, 51, 116, -10, -8, -10, -10, -10, -10, 127, 73, -7, 36, -10, -10, -7, 44, 11, -61, 14, 16, 14, 16, 16, 15, 16, -23, 13, 12, 1, 127, 14, 12, 15, 125, 15, 15, 82, 111, 33, -88, 17, 16, 12, 64, 2, 116, 3, 3, 3, 3, 3, 3, 3, 36, 3, 2, 31, 54, 3, 2, 3, 93, 3, 3, 24, 27, 75, 121, 3, 3, 3, 127, -4, 27, -9, -9, -9, -9, -9, -9, -9, 127, -9, -7, 111, 20, -9, -9, -9, 53, -9, -9, 32, 127, -7, 59, -9, -9, -8, 112, 0, 32, 2, 3, 3, 3, 3, 3, 3, 25, 1, 0, -60, -5, 2, 1, 2, -5, 3, 3, -2, 78, -1, 127, 3, 3, 0, 99, -4, -26, -8, -8, -8, -7, -8, -8, -8, 79, -8, -7, 101, 63, -8, -8, -8, 70, -8, -8, 127, 85, 47, 47, -7, -7, -8, 114, -2, -11, -5, -6, -6, -5, -6, -6, -6, 127, -5, -5, 18, 8, -6, -5, -6, 101, -6, -6, 27, 41, 10, 123, -5, -5, -5, 12, -3, 82, -9, -9, -9, -9, -9, -9, -9, 46, -8, -6, 106, 101, -9, -8, -9, 45, -9, -9, 44, 41, 31, 14, -9, -9, -7, 127, 2, -49, 1, 2, 1, 2, 2, 2, 2, 92, 1, 2, -2, 34, 1, 1, 2, 76, 2, 2, 21, 68, -51, 127, 3, 2, 1, 99, -1, 58, 0, 0, 0, 0, 0, 0, 0, 65, 0, 0, 115, 58, 0, 0, 0, 127, 0, 0, 102, -69, 0, 59, 0, 0, 0, 86, -4, 127, -4, -3, -3, -3, -3, -3, -3, 80, -4, -5, -7, 66, -3, -5, -3, 62, -3, -3, 55, 115, 39, -5, -2, -3, -5, 60, -7, 80, -10, -10, -10, -10, -10, -10, -10, 89, -10, -10, -7, 93, -10, -10, -10, 127, -10, -10, 30, 1, 43, 21, -10, -10, -10, 76, -6, 89, -11, -11, -11, -10, -11, -11, -11, 34, -11, -10, 11, 127, -11, -11, -11, -29, -11, -11, 123, 102, -116, 43, -10, -10, -11, 42, 1, -44, 0, 0, -1, 0, 0, 0, 0, 71, 0, 1, 40, 127, 0, 0, 0, 66, 0, 0, -73, 56, -73, 81, 0, 0, 1, -19, -3, 66, -1, -1, -1, -1, -1, -1, -1, 84, -2, -4, 56, 80, -1, -3, -1, 31, -1, -1, 21, 36, 43, -18, -1, -1, -4, 127, 3, 29, 2, 2, 1, 2, 2, 2, 2, 12, 2, 3, -7, -66, 2, 2, 2, 127, 2, 2, 32, 22, -60, 61, 2, 2, 3, 93, -5, 52, -2, -1, -1, -1, -1, -1, -1, 1, -4, -6, 97, 127, -2, -4, -2, 84, -1, -1, 4, 32, 31, 65, -1, -1, -6, 77, 0, 91, 7, 8, 8, 9, 9, 9, 8, 127, 5, -1, 37, 99, 7, 3, 7, 121, 8, 8, 57, -79, 63, -24, 9, 8, 0, 109, 0, 35, 4, 5, 5, 5, 5, 5, 5, 80, 3, -1, 114, 98, 4, 2, 4, 16, 5, 5, 75, 55, -2, -127, 5, 5, 0, 59, -4, 77, -3, -3, -3, -2, -2, -2, -3, 59, -4, -5, 21, 85, -3, -5, -3, 23, -3, -3, 127, -10, -79, 3, -2, -2, -5, 56, 1, 67, 2, 2, 2, 2, 2, 2, 2, 69, 1, 1, 105, 52, 2, 1, 2, 30, 2, 2, 45, 2, -23, 19, 2, 2, 1, 127, 1, -60, 0, 0, 0, 0, 0, 0, 0, 31, 0, 1, -7, 51, 0, 0, 0, 52, 0, 0, 71, 38, 62, 127, 1, 1, 1, 74, -1, 10, -1, 0, -1, 0, 0, 0, 0, 49, -1, -1, -56, 28, -1, -1, -1, -10, -1, 0, 50, 72, 27, -2, 0, 0, -1, 127, -3, 78, -5, -6, -6, -6, -6, -6, -6, 127, -5, -4, 39, -24, -6, -5, -6, 9, -6, -6, -15, 73, 11, 32, -6, -6, -4, 66, -13, 8, -25, -27, -26, -27, -27, -27, -27, 109, -23, -18, 127, 78, -26, -22, -26, 70, -27, -27, 58, -36, -68, 21, -27, -27, -20, 4, -1, 26, -1, 0, -1, 0, 0, 0, 0, 59, -1, -2, 69, 27, -1, -2, -1, 12, 0, 0, 14, 56, 50, -127, 0, 0, -2, 56, -2, 23, -6, -6, -6, -6, -6, -6, -6, -72, -6, -4, 20, 78, -6, -6, -6, 66, -6, -6, 77, 127, -21, -90, -5, -5, -5, 93, -5, 25, -11, -12, -12, -12, -12, -12, -12, 47, -10, -8, -9, 33, -11, -10, -11, 70, -12, -12, -127, -20, -7, 46, -12, -12, -9, 79, -9, -30, -10, -10, -10, -10, -10, -10, -10, 57, -11, -12, -9, 127, -10, -11, -10, 71, -10, -10, 90, 117, 74, -37, -10, -10, -12, 108, 3, 64, 6, 7, 7, 7, 7, 7, 7, 20, 5, 3, 127, 43, 7, 5, 6, 90, 7, 7, 17, 28, 2, 63, 7, 7, 3, 122, -9, 127, -9, -9, -9, -9, -9, -9, -9, 118, -9, -11, 24, 91, -9, -10, -9, 99, -9, -9, 51, -7, 42, -34, -9, -9, -11, -32, 0, 6, -2, -1, -1, 0, -1, -1, -1, 71, -2, -3, 102, 36, -1, -3, -1, 11, -1, -1, 37, 85, 30, -127, 0, 0, -3, 57, -6, 94, -10, -10, -10, -10, -10, -10, -10, 103, -10, -9, 127, 63, -10, -10, -10, 94, -10, -10, 19, -36, -70, 1, -10, -10, -10, 55, -1, 61, 2, 3, 3, 3, 3, 3, 3, 49, 1, -1, 95, 113, 2, 1, 2, 127, 3, 3, 87, -85, 30, 108, 3, 3, 0, 56, -2, 12, -4, -4, -4, -4, -4, -4, -4, 7, -4, -2, 2, 83, -4, -4, -4, -48, -4, -4, -127, -47, -24, -18, -3, -3, -3, 41, -3, 80, -5, -7, -5, -7, -7, -6, -7, 6, -5, -5, 57, -21, -6, -5, -6, -115, -6, -6, -1, 73, 26, 39, -7, -7, -6, 127, 0, 127, 2, 2, 2, 3, 2, 2, 2, 79, 1, 0, 29, 74, 2, 1, 2, 50, 2, 2, 81, -76, 15, 94, 3, 2, 0, 33, -2, 122, -2, -3, -2, -3, -3, -2, -3, 91, -2, -2, 106, 65, -2, -2, -2, 30, -2, -2, 127, -33, 41, 53, -3, -3, -2, 35, -4, 104, -4, -4, -4, -4, -4, -4, -4, 127, -4, -5, -44, 72, -4, -5, -4, 51, -4, -4, 90, -45, 5, 37, -4, -4, -5, 21, -4, 16, -11, -12, -11, -12, -12, -12, -12, -27, -10, -7, 8, 59, -11, -9, -11, 7, -11, -12, 127, 6, -21, 21, -12, -12, -8, 39, -2, 33, -3, -4, -3, -4, -4, -4, -4, 56, -3, -3, 127, 61, -3, -3, -3, 44, -3, -4, 34, -48, 53, 46, -4, -4, -3, 60, 0, -82, -4, -4, -4, -4, -4, -4, -4, 99, -4, -2, -1, 38, -4, -4, -4, 127, -4, -4, 55, 5, -10, 26, -4, -4, -3, 85, -9, 1, -10, -10, -9, -10, -10, -10, -10, 127, -11, -13, 32, 97, -10, -11, -10, 108, -10, -10, 73, 101, -1, 126, -11, -11, -13, 20, -1, 127, 1, 3, 2, 3, 3, 3, 3, 52, 0, -2, 17, 72, 2, -1, 2, -3, 2, 2, 46, 70, 67, 60, 3, 3, -2, 82, -3, 28, -5, -4, -5, -4, -4, -5, -4, 69, -5, -4, 104, -7, -5, -5, -5, 91, -5, -5, 35, 54, 45, -38, -4, -4, -5, 127, -1, 64, -6, -7, -7, -7, -7, -7, -7, 86, -6, -3, 76, 127, -7, -5, -6, 71, -7, -7, 73, -33, 57, -73, -7, -7, -4, 87, -2, 44, -1, -1, -1, -1, -1, -1, -1, 127, -1, -1, 31, 123, -1, -1, -1, 39, -1, -1, -20, -23, 25, 28, -1, -1, -1, -8, -7, 92, -5, -5, -5, -4, -5, -4, -5, 17, -6, -9, 5, 127, -5, -7, -5, 71, -5, -5, 114, 46, -5, -42, -5, -5, -9, -14, -6, 122, -10, -10, -10, -9, -10, -10, -10, 27, -10, -9, -8, 29, -10, -9, -10, 50, -10, -10, 127, 80, -16, 64, -9, -9, -9, 30, -6, 50, -9, -10, -9, -10, -10, -9, -10, 65, -9, -9, 59, 119, -9, -9, -9, 127, -9, -9, 5, 52, 64, -25, -10, -10, -10, 109, -3, 42, -2, -3, -2, -3, -3, -3, -3, 9, -3, -4, 86, 127, -3, -3, -3, -3, -3, -3, 53, 94, -109, 83, -4, -3, -4, -48, -4, 37, -5, -5, -5, -5, -5, -5, -5, 125, -6, -7, 30, 127, -5, -6, -5, 89, -5, -5, 70, 85, -14, 49, -5, -5, -7, 17, 4, 101, 11, 12, 12, 12, 12, 12, 12, 84, 10, 6, 65, 5, 11, 9, 11, 127, 12, 12, -11, -60, -50, 42, 12, 12, 7, 105, -4, 87, -6, -6, -6, -6, -6, -6, -6, 70, -6, -6, 63, 21, -6, -6, -6, 23, -6, -6, 47, 83, 33, -2, -6, -6, -6, 127, -3, 47, -2, -2, -1, -1, -1, -1, -2, 62, -2, -4, 97, 127, -2, -3, -2, -15, -1, -1, 48, 19, -12, -53, -2, -2, -4, 74, -7, 126, -5, -5, -5, -5, -5, -5, -5, 49, -6, -9, 98, 127, -5, -7, -5, 16, -5, -5, 51, -60, 50, 9, -5, -5, -8, -15, -4, 77, -6, -6, -6, -5, -5, -5, -6, 63, -6, -6, 62, 127, -6, -6, -6, 48, -5, -5, 79, 43, 31, 3, -5, -5, -6, 63, 0, -2, -3, -3, -3, -3, -3, -3, -3, 2, -2, -1, 4, -52, -3, -2, -3, 127, -3, -3, -96, 8, -52, 1, -3, -3, -2, 11, -6, 48, -9, -8, -9, -8, -8, -8, -8, 97, -10, -9, -28, 26, -9, -10, -9, 126, -9, -9, -30, 49, -27, 127, -8, -8, -10, 102, -5, 55, -10, -10, -10, -10, -10, -10, -10, 68, -10, -8, 127, -14, -10, -10, -10, 50, -10, -10, 22, 58, 50, 8, -10, -10, -9, 79, -3, 29, -5, -5, -5, -5, -5, -5, -5, 65, -5, -5, 127, 107, -5, -5, -5, 51, -5, -5, 110, -54, -16, -94, -5, -5, -5, 127, -4, 64, -2, -2, -2, -2, -2, -2, -2, 65, -3, -6, 31, 49, -2, -3, -2, 100, -2, -2, 59, 60, 127, -28, -2, -2, -5, 102, -2, 5, 1, 2, 1, 2, 2, 2, 2, 22, 0, -2, 122, 25, 1, -1, 1, 32, 2, 2, -29, 30, -63, 66, 2, 2, -1, 127, 0, 18, -3, -5, -4, -5, -5, -5, -5, 8, -3, -2, 1, -15, -4, -2, -4, 49, -4, -4, 55, 33, -127, 36, -5, -5, -2, 35, 0, -24, 1, 1, 1, 1, 1, 1, 1, 27, 1, 0, 127, 12, 1, 0, 1, 33, 1, 1, 5, 40, -4, -78, 1, 1, 0, 62, -6, 70, -10, -10, -10, -10, -10, -10, -10, 115, -10, -9, 100, 127, -10, -10, -10, 55, -10, -10, 53, 74, 14, 11, -10, -10, -10, 86, 2, 12, 7, 8, 7, 9, 8, 8, 8, 45, 6, 3, 109, 24, 7, 5, 7, 78, 8, 8, 45, -47, 51, 41, 9, 8, 4, 127, -9, 75, -13, -13, -13, -13, -13, -13, -13, 44, -13, -13, 127, 28, -13, -14, -13, -62, -13, -13, 78, -61, -19, 103, -13, -13, -14, 109, -3, 53, -3, -3, -2, -3, -3, -3, -3, 127, -3, -5, 83, 118, -3, -4, -3, 81, -3, -3, 95, -7, -16, 48, -3, -3, -5, -49, -1, 83, -1, 0, -1, 0, 0, 0, -1, 62, -1, -3, -39, -1, -1, -2, -1, -4, -1, -1, 45, -19, 9, 4, 0, 0, -2, 127, -1, -28, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 127, -12, -1, -2, -1, -16, -1, -1, -10, 10, 33, -13, -1, -1, -1, 5, 1, -20, 1, 1, 1, 1, 1, 1, 1, -36, 1, 0, 42, 127, 1, 1, 1, -19, 1, 1, 37, 92, 42, -1, 1, 1, 0, -36, 1, -19, 2, 1, 2, 1, 1, 2, 1, 51, 2, 1, -35, -19, 2, 2, 2, 103, 2, 2, 127, 18, -112, 15, 1, 1, 1, -41, -6, 111, -7, -7, -7, -7, -7, -7, -7, 104, -8, -8, 127, 91, -7, -8, -7, -9, -7, -7, 37, -14, 41, -77, -7, -7, -8, 100, 1, 11, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 36, 68, 0, 0, 0, -8, 0, 0, 52, 127, -117, 14, 0, 0, 0, -9, -7, 114, -14, -14, -14, -14, -14, -14, -14, -32, -14, -13, 127, 107, -14, -14, -14, -36, -14, -14, 80, 22, 123, 81, -14, -14, -14, 105, 1, -5, -1, 0, -1, 0, 0, 0, 0, 81, -1, 0, -16, 93, -1, -1, 0, 127, 0, 0, 16, 93, -14, 16, 0, 0, -1, 114, 0, 111, 0, 0, 0, 0, 0, 0, 0, 56, 0, -1, 25, 103, 0, 0, 0, 36, 0, 0, 127, 60, 38, -27, 0, 0, -1, 44, -4, 23, -5, -5, -5, -5, -5, -5, -5, 20, -6, -6, 87, 48, -5, -6, -5, 40, -5, -5, 85, 94, 5, -62, -5, -5, -7, 127, -3, 16, -9, -9, -9, -9, -9, -9, -9, 49, -8, -6, -36, 127, -9, -8, -9, 39, -9, -9, -23, 10, -118, 29, -9, -9, -7, 23, -3, 127, -6, -6, -6, -6, -6, -6, -6, 94, -6, -6, 39, 26, -6, -6, -6, 74, -6, -6, 5, 49, 55, -25, -6, -6, -6, 71, -3, 127, -1, 0, 0, 0, 0, 0, 0, -2, -2, -4, 5, 106, -1, -2, -1, 48, 0, 0, 1, 45, 30, 20, 0, 0, -3, 34, -1, 46, -1, -1, -1, -1, -1, -1, -1, 1, -2, -2, 127, 44, -1, -2, -1, 27, -1, -1, -7, 14, 25, -52, -1, -1, -2, 84, -6, 127, -11, -10, -10, -10, -10, -10, -10, 96, -11, -10, 7, 14, -10, -11, -10, 123, -10, -10, -30, -17, 23, -29, -10, -10, -10, 42, 2, -27, 3, 3, 3, 3, 3, 3, 3, -3, 3, 3, 127, 59, 3, 3, 3, 24, 3, 3, 61, 4, -28, 7, 3, 3, 3, 5, 0, 113, 4, 4, 4, 4, 4, 4, 4, 127, 3, 1, 89, 63, 4, 2, 4, 88, 4, 4, 47, -32, 60, 14, 4, 4, 1, 33, 0, -3, 0, 1, 1, 1, 1, 1, 1, 54, 0, -1, 83, 57, 1, 0, 1, 53, 1, 1, 80, -24, 55, 43, 1, 1, -1, 127, -2, -1, -1, 0, -1, 0, 0, 0, 0, 60, -1, -2, 127, 87, -1, -2, -1, -5, -1, -1, 95, -76, 2, -14, 0, 0, -2, 64, -4, 20, -5, -4, -4, -4, -4, -4, -4, 127, -5, -5, 50, 103, -4, -5, -4, 15, -4, -4, 57, 24, 9, 113, -4, -4, -5, 118, 0, 68, 0, 1, 1, 1, 1, 1, 1, 127, 0, -1, -39, 20, 0, 0, 0, 49, 1, 1, 43, 98, -1, 57, 1, 1, -1, 64, -3, 52, -6, -6, -6, -6, -6, -6, -6, 66, -5, -4, 33, -28, -6, -5, -6, 61, -6, -6, 52, 100, 11, 4, -6, -6, -4, 127, 2, 4, 2, 2, 2, 2, 2, 2, 2, 97, 2, 3, 122, -41, 2, 2, 2, 99, 2, 2, 98, 127, 44, 99, 2, 2, 3, 25, -2, 81, -4, -4, -4, -4, -4, -4, -4, 22, -4, -4, 72, 59, -4, -4, -4, 127, -4, -4, 35, 21, 71, -19, -4, -4, -5, 77, -1, -127, -2, -2, -2, -2, -2, -2, -2, 1, -2, -2, 45, 53, -2, -2, -2, 26, -2, -2, 28, 9, 4, 22, -2, -2, -2, 36, 1, 12, -1, 0, -1, 0, 0, -1, 0, 118, 0, 1, -3, 48, -1, 0, 0, 89, -1, -1, 127, -31, 76, 11, 0, 0, 0, 123, 6, 47, 17, 18, 18, 18, 18, 18, 18, 33, 15, 10, 98, 29, 17, 14, 17, 61, 18, 18, 127, -1, 17, 66, 18, 18, 12, 100, -4, 42, -3, -3, -3, -3, -3, -3, -3, 64, -4, -6, 127, 105, -3, -5, -3, 75, -3, -3, 56, -46, -39, 42, -3, -3, -6, 5, -4, -9, -5, -5, -5, -6, -5, -5, -5, -13, -5, -5, -20, 66, -5, -5, -5, -51, -5, -5, 21, 39, -127, 13, -6, -6, -5, -2, -2, 54, -1, -1, -1, -1, -1, -1, -1, -4, -1, -2, 47, 127, -1, -1, -1, -13, -1, -1, 38, 6, -63, -54, -2, -2, -2, 3, -3, 59, -7, -7, -7, -7, -7, -7, -7, 61, -6, -5, 35, 113, -7, -6, -7, 94, -7, -7, 127, -12, 58, -3, -7, -7, -6, 33, 0, 4, -1, -1, -1, -1, -1, -1, -1, 41, -2, -1, -108, 72, -1, -2, -1, 98, -1, -1, 28, 62, 66, 75, -1, -1, -2, 127, -1, 20, 0, 1, 0, 1, 1, 1, 1, 112, -1, -1, -3, 127, 0, -1, 0, 88, 0, 0, 34, 64, -4, 11, 1, 1, -1, 42, -2, 38, -9, -8, -9, -8, -9, -9, -8, 42, -7, -3, 127, 59, -9, -7, -8, 61, -9, -9, 32, 24, 31, 69, -8, -8, -5, -83, -8, 127, -14, -14, -14, -14, -14, -14, -14, 49, -13, -12, -21, 118, -14, -13, -14, 51, -14, -14, 57, -15, 8, -44, -14, -14, -13, 43, -1, 1, 2, 2, 2, 2, 2, 2, 2, 34, 1, -1, 5, 19, 2, 1, 2, 12, 2, 2, 34, 16, -127, 28, 2, 2, -1, 4, -6, 127, -7, -7, -7, -6, -7, -7, -7, 60, -8, -9, 0, 97, -7, -8, -7, 125, -7, -7, 109, -57, -21, -15, -6, -7, -9, 28, -1, 77, 3, 4, 4, 4, 4, 4, 4, 95, 2, 0, 127, 17, 4, 2, 3, 55, 4, 4, 52, 27, 101, 10, 4, 4, 0, 52, 2, -22, 8, 9, 9, 9, 9, 9, 9, 30, 6, 3, 118, 105, 8, 5, 8, 127, 9, 9, 30, 18, 10, 40, 9, 9, 3, 67, -4, 20, -6, -6, -6, -6, -6, -6, -6, 117, -5, -5, 119, 65, -6, -5, -6, 127, -6, -6, 36, 99, 68, 41, -5, -5, -5, 40, -7, 40, -13, -14, -13, -14, -14, -14, -14, 46, -12, -11, 31, -27, -13, -12, -13, 127, -13, -13, -28, 75, -70, -8, -14, -14, -12, 38, -6, 127, -9, -9, -9, -8, -8, -9, -9, 89, -9, -9, 54, 85, -9, -9, -9, 19, -9, -9, 64, 85, 32, -93, -8, -8, -10, 123, -4, 82, -3, -3, -3, -3, -3, -3, -3, 106, -4, -5, 127, 37, -3, -4, -3, 100, -3, -3, 60, 93, -80, 103, -3, -3, -5, 14, -3, 127, -6, -7, -6, -7, -7, -7, -7, 101, -6, -5, 34, 94, -6, -6, -6, 74, -6, -6, 58, -116, 26, -79, -7, -7, -6, 87, -3, 18, -6, -7, -7, -7, -7, -7, -7, 127, -6, -6, -78, 0, -7, -6, -7, 65, -7, -7, 70, -58, -44, 13, -7, -7, -6, -5, -5, 30, -8, -8, -8, -8, -8, -8, -8, 96, -8, -8, 44, 75, -8, -8, -8, 66, -8, -8, 88, 20, -12, 12, -8, -8, -8, 127, -7, 30, -11, -11, -11, -11, -11, -11, -11, 19, -11, -10, 34, 82, -11, -11, -11, 114, -11, -11, 127, 20, 30, 85, -11, -11, -11, 90, 1, 55, 4, 4, 4, 4, 4, 4, 4, 68, 3, 1, 59, 127, 4, 3, 4, 48, 4, 4, 74, -65, 73, 22, 3, 3, 2, 79, 0, 10, -3, -2, -3, -2, -2, -3, -2, 50, -3, -1, 120, 1, -3, -3, -3, -89, -3, -3, 101, 24, 118, -15, -2, -2, -2, 127, 3, -46, 5, 5, 5, 6, 6, 5, 5, 8, 4, 3, 17, -13, 5, 4, 5, 22, 5, 5, 70, 6, 25, 52, 6, 6, 3, 127, 0, 82, -1, -1, -1, -1, -1, -1, -1, 20, -1, 0, -12, 68, -1, -1, -1, 20, -1, -1, 127, -6, 7, -11, -1, -1, 0, 100, -5, 127, -7, -7, -7, -7, -7, -7, -7, 59, -7, -7, -23, 25, -7, -7, -7, 8, -7, -7, 13, 68, 18, 28, -7, -7, -7, 11, 5, -12, 8, 9, 8, 9, 9, 9, 9, 45, 8, 7, 127, -5, 8, 7, 8, 35, 8, 8, 49, 11, 14, -8, 9, 9, 7, -11, -4, 43, -8, -7, -8, -7, -7, -7, -7, 46, -7, -6, -19, 127, -8, -7, -7, 94, -8, -8, 34, -87, -2, 75, -7, -7, -7, 10, -4, 77, -5, -4, -4, -4, -4, -4, -4, 77, -5, -6, 2, 58, -4, -5, -5, 42, -4, -4, 32, 63, 37, 14, -4, -4, -6, 127, -1, 84, -1, -1, -1, -1, -1, -1, -1, 59, -2, -2, -39, -9, -1, -2, -1, 37, -1, -1, 30, -24, 16, 93, -1, -1, -2, 127, -2, -80, -8, -9, -9, -9, -9, -9, -9, -36, -8, -5, -2, 53, -9, -7, -8, 125, -9, -9, 65, 127, 2, 47, -9, -9, -6, 111, -4, 46, -9, -9, -9, -9, -9, -9, -9, 107, -9, -6, 110, 105, -9, -8, -9, 23, -9, -9, 127, -97, -68, -35, -9, -9, -7, 110, 0, -3, 3, 3, 3, 3, 3, 3, 3, 103, 2, 0, 27, 24, 3, 2, 3, -37, 3, 3, 127, 48, 33, 54, 3, 3, 1, 86, 0, 28, 3, 4, 4, 4, 4, 4, 4, 56, 3, 1, 100, 90, 3, 2, 3, 120, 4, 4, 57, -12, 32, -55, 4, 3, 1, 127, -5, 42, -4, -3, -3, -3, -3, -3, -3, 115, -5, -6, 64, 127, -4, -5, -4, 77, -3, -3, 76, 88, 18, 19, -3, -3, -6, -86, -5, 63, -7, -7, -7, -7, -7, -7, -7, 50, -7, -7, 97, 127, -7, -7, -7, 27, -7, -7, 41, 81, 53, 30, -7, -7, -7, 32, 5, 93, 11, 11, 11, 11, 11, 11, 11, 106, 10, 8, 99, 114, 11, 10, 11, 72, 11, 11, 63, -69, 5, 74, 11, 11, 9, 127, -4, 52, -9, -9, -9, -9, -9, -9, -9, 127, -9, -8, 24, 60, -9, -9, -9, 50, -9, -9, 41, 99, -58, 42, -9, -9, -8, -25, -1, 86, -5, -6, -6, -6, -6, -6, -6, 81, -4, -1, 84, 26, -5, -3, -5, 90, -6, -6, 80, 38, 49, -37, -6, -6, -2, 127, -1, 92, 3, 4, 4, 4, 4, 4, 4, 109, 2, -1, -100, 124, 3, 1, 3, 127, 4, 4, 68, -98, -81, 40, 4, 4, 0, 97, -5, -9, -5, -5, -5, -4, -4, -4, -5, 127, -6, -8, 28, 75, -5, -7, -5, 97, -5, -5, 37, 125, 25, -1, -4, -4, -8, 73, -3, 110, -5, -6, -5, -6, -6, -6, -6, 40, -5, -3, 127, 123, -5, -4, -5, 22, -5, -6, 77, 4, 41, -33, -6, -6, -4, 42, -1, 30, -1, -1, -1, -1, -1, -1, -1, 116, -1, -2, 127, -8, -1, -1, -1, 29, -1, -1, 12, 8, -45, 61, -1, -1, -2, 102, -3, -7, -3, -4, -3, -4, -4, -3, -4, 40, -3, -4, 5, 36, -3, -3, -3, 21, -3, -3, -61, 31, -127, 28, -4, -4, -4, 31, -5, 28, -6, -6, -6, -6, -6, -6, -6, 14, -6, -6, -7, 8, -6, -6, -6, -28, -6, -6, 44, 126, 12, 53, -6, -6, -6, 127, 1, -6, 3, 3, 3, 3, 3, 3, 3, 90, 3, 2, 35, 30, 3, 3, 3, -22, 3, 3, 21, 24, -109, 127, 3, 3, 2, 34, -1, 127, 0, 0, 0, 0, 0, 0, 0, 58, 0, -1, 61, 100, 0, 0, 0, 39, 0, 0, 64, -63, -28, 100, 0, 0, -1, 23, -5, -31, -10, -10, -11, -10, -10, -10, -10, 55, -10, -9, 98, 55, -10, -10, -10, 127, -10, -10, 32, 13, 92, 69, -10, -10, -10, 76, -8, 41, -12, -13, -12, -13, -13, -13, -13, 97, -12, -11, 81, 125, -12, -12, -13, 121, -13, -13, 76, 127, 32, 16, -13, -13, -12, 81, -3, -125, -4, -3, -3, -2, -3, -3, -3, 102, -4, -5, 10, 78, -3, -5, -3, 127, -3, -3, 85, 78, -37, 3, -2, -2, -5, -39, -1, 12, -2, -2, -1, -2, -2, -2, -2, -19, -2, -2, 3, 34, -2, -2, -2, 19, -2, -2, 3, 50, -127, 22, -2, -2, -2, 0, -6, 45, -10, -10, -10, -10, -10, -10, -10, 72, -10, -9, 25, 48, -10, -10, -10, 127, -10, -10, 87, 43, -49, 55, -10, -10, -10, 97, -4, 57, -4, -4, -4, -5, -4, -4, -4, 46, -5, -6, 127, 31, -4, -5, -4, -33, -4, -4, 4, -28, 61, 49, -5, -5, -6, 60, -8, -3, -12, -13, -12, -13, -13, -13, -13, 113, -11, -11, 127, 116, -12, -11, -12, 81, -12, -12, 83, 60, -27, -38, -13, -13, -11, 16, -6, 106, -11, -12, -12, -12, -12, -12, -12, 29, -11, -9, 36, 20, -11, -10, -11, 39, -12, -12, -29, 127, 87, 15, -12, -12, -10, -41, -7, 76, -12, -12, -12, -12, -12, -12, -12, 24, -11, -10, 127, 95, -12, -11, -12, 30, -12, -12, 61, 63, 9, 64, -12, -12, -11, 121, -2, -127, -7, -7, -7, -7, -7, -7, -7, 85, -6, -4, 87, 37, -7, -6, -7, 54, -7, -7, 31, 21, 65, -57, -7, -7, -5, 109, -3, 82, 2, 2, 3, 2, 2, 2, 2, 15, 0, -3, 5, 14, 2, 0, 2, 50, 2, 2, -2, 28, -127, 59, 2, 1, -2, -17, -4, 103, 0, 1, 2, 1, 1, 1, 1, 31, -1, -5, 127, 63, 1, -2, 0, 63, 1, 1, 113, 68, -13, -39, 0, 0, -4, 74, 0, 98, 0, 0, 0, 0, 0, 0, 0, 71, 0, 1, 45, 58, 0, 0, 0, 73, 0, 0, 127, -65, 19, 33, 0, 0, 1, 99, 5, 127, 11, 13, 13, 14, 14, 14, 13, 78, 9, 5, 65, 127, 12, 8, 12, -6, 13, 13, 28, 19, 43, -49, 14, 14, 6, 109, 4, 65, 9, 9, 9, 9, 9, 9, 9, 31, 8, 7, 96, -27, 9, 8, 9, 57, 9, 9, 38, 28, -23, 15, 9, 9, 7, 127, -2, -26, -5, -5, -5, -5, -5, -5, -5, 54, -5, -4, -26, 107, -5, -5, -5, 37, -5, -5, 7, 34, 20, 1, -5, -5, -5, 127, -5, 43, -9, -9, -10, -9, -9, -9, -9, 48, -9, -8, 116, 127, -9, -9, -9, 93, -9, -9, 112, 101, 76, 34, -9, -9, -9, 68, -5, 7, -6, -6, -6, -6, -6, -6, -6, 57, -7, -8, 22, 86, -6, -7, -6, 73, -6, -6, 54, 65, -4, 11, -6, -6, -8, 127, -6, -4, -8, -8, -8, -8, -8, -8, -8, 127, -9, -9, 63, 68, -8, -9, -8, 66, -8, -8, -31, -19, -38, -42, -8, -8, -9, 25, -3, 42, -4, -4, -4, -4, -4, -4, -4, 23, -4, -4, 127, -1, -4, -4, -4, -3, -4, -4, 4, 54, 51, -49, -4, -4, -4, 38, -5, 33, -5, -5, -5, -5, -5, -5, -5, 26, -6, -7, 127, 37, -5, -6, -5, 28, -5, -5, 2, -18, -14, -17, -5, -5, -7, 41, -6, 84, -10, -9, -10, -9, -9, -10, -9, 77, -10, -9, 45, 119, -10, -10, -10, 73, -10, -10, 48, 127, -20, 37, -9, -9, -10, 10, -3, -7, -4, -3, -3, -3, -3, -3, -3, 16, -4, -4, 127, 29, -3, -4, -3, 31, -3, -3, 10, -15, 17, 43, -3, -3, -4, 30, -1, 46, 1, 2, 2, 2, 2, 2, 2, 1, 1, -2, 127, -28, 2, 0, 1, 14, 2, 2, -13, 63, -3, 4, 2, 2, -1, 57, 6, 117, 6, 5, 5, 5, 5, 5, 5, 30, 6, 8, 81, 103, 5, 7, 5, 32, 5, 5, 127, -3, 61, -19, 5, 5, 7, 71, -1, -46, -6, -5, -6, -5, -6, -6, -5, 50, -5, -3, -43, 127, -6, -5, -6, -25, -6, -6, -15, 3, -97, 22, -5, -5, -4, 57, -4, 127, -7, -6, -7, -6, -6, -6, -6, 54, -7, -6, 65, 23, -7, -7, -6, -18, -6, -6, 15, 1, -3, -42, -6, -6, -6, 24, -2, 45, -8, -8, -8, -8, -8, -8, -8, 29, -7, -4, 17, 14, -8, -6, -8, 117, -8, -8, 127, 64, 57, 36, -8, -8, -5, 110, -5, 100, -6, -6, -6, -6, -6, -6, -6, 73, -7, -7, 127, 88, -6, -7, -6, 27, -6, -6, 6, 56, 13, -31, -6, -6, -7, 93, -3, 36, -6, -6, -6, -6, -6, -6, -6, 41, -6, -5, 127, 17, -6, -6, -6, 13, -6, -6, -40, 19, 34, -16, -6, -6, -6, 2, 0, -25, -2, -2, -2, -2, -2, -2, -2, 2, -2, -1, 13, 56, -2, -2, -2, 36, -2, -2, -19, 25, -127, 3, -2, -2, -1, 39, -1, -3, -4, -4, -4, -4, -4, -4, -4, 119, -4, -2, 52, 73, -4, -4, -4, 45, -4, -4, 127, -52, 13, 7, -4, -4, -3, 49, -2, 68, -3, -2, -2, -2, -2, -2, -2, 103, -3, -3, 41, 28, -2, -3, -2, 49, -2, -2, 28, -33, 127, 18, -2, -2, -3, 14, -8, 31, -15, -15, -15, -15, -15, -15, -15, -3, -15, -12, 105, 94, -15, -14, -15, 30, -15, -15, 105, 10, 53, -29, -15, -15, -14, 127, -1, 95, 0, 0, 0, 0, 0, 0, 0, 49, 0, -1, 68, 88, 0, 0, 0, 7, 0, 0, -34, 52, -118, -4, 0, 0, -1, 127, -2, 68, -3, -3, -3, -3, -3, -3, -3, 8, -3, -2, 127, -3, -3, -2, -3, -6, -3, -3, 3, -48, -6, 43, -3, -3, -3, 63, 0, -25, -2, -2, -2, -1, -1, -2, -1, 24, -2, -1, -17, 29, -2, -2, -2, 7, -2, -2, 24, 45, -127, 13, -1, -1, -2, 16, -2, 10, -4, -3, -4, -3, -3, -3, -3, 10, -4, -3, 41, 82, -4, -4, -4, 88, -4, -4, 74, 47, 30, 18, -3, -3, -3, 127, 1, -56, -6, -6, -7, -6, -6, -6, -6, 36, -5, 0, 22, 51, -6, -4, -6, 127, -6, -6, 58, 32, -39, 91, -5, -5, -2, 114, -2, 96, 0, 0, 0, 0, 0, 0, 0, 18, -1, -3, -3, 71, 0, -1, 0, 127, 0, 0, 61, -8, 77, 30, 0, 0, -2, 19, -6, 46, -9, -8, -9, -8, -8, -8, -8, 9, -9, -9, 39, 0, -9, -9, -9, 72, -9, -9, -3, -2, 36, 15, -8, -8, -9, 127, 4, 96, 3, 4, 4, 5, 4, 4, 4, 82, 3, 2, 127, 57, 4, 2, 4, 33, 4, 4, 43, 37, 55, -53, 5, 5, 2, 120, -7, 127, -7, -7, -6, -7, -7, -6, -7, 65, -7, -8, 99, 52, -7, -8, -7, -42, -6, -6, 81, -56, 42, 2, -7, -7, -8, 12, 0, 5, -2, -2, -2, -2, -2, -2, -2, 99, -2, -2, 56, 127, -2, -2, -2, 120, -2, -2, 12, 1, -42, 18, -2, -2, -2, 83, -6, 16, -9, -10, -9, -10, -10, -9, -10, 23, -9, -8, 13, 35, -9, -9, -9, 108, -9, -9, -127, 51, -10, 56, -10, -10, -8, 43, -5, 54, -10, -10, -11, -10, -10, -10, -10, 53, -10, -8, 127, 47, -10, -10, -10, 37, -10, -10, 6, 32, 5, 53, -10, -10, -9, 28, -1, 13, -7, -7, -7, -7, -7, -7, -7, 127, -6, -3, -21, -63, -7, -5, -7, 30, -7, -7, 116, 80, -37, 57, -7, -7, -4, 79, 0, 61, -2, -1, -2, -1, -1, -1, -1, 66, -2, -1, 99, 127, -2, -2, -2, -83, -2, -2, 55, -19, -35, 62, -1, -1, -1, 120, -2, -9, -4, -4, -4, -4, -4, -4, -4, 42, -4, -4, 25, 127, -4, -4, -4, 78, -4, -4, 61, -31, 42, 8, -4, -4, -4, 54, -7, 127, -13, -13, -13, -14, -14, -14, -13, 12, -12, -10, 85, 125, -13, -12, -13, 40, -13, -13, 64, 112, 63, -26, -14, -13, -11, -12, 1, 89, -2, -1, -2, -1, -1, -1, -1, 127, -2, 0, 89, 62, -2, -2, -2, 53, -2, -2, 4, 53, 28, 35, -1, -1, -1, -8, -2, 60, -1, -1, -1, 0, 0, -1, -1, 88, -2, -3, 106, 127, -1, -2, -1, 39, -1, -1, 36, -59, -5, -2, 0, 0, -3, 108, 4, -54, 1, 1, 1, 0, 1, 1, 1, 79, 2, 3, 54, 127, 1, 2, 1, -28, 1, 1, 47, 49, 1, 26, 0, 0, 3, 50, -8, 34, -14, -14, -14, -14, -14, -14, -14, 110, -15, -14, 11, -127, -14, -15, -14, 91, -14, -14, -2, 99, -60, 66, -14, -14, -15, 110, 1, 31, 2, 3, 2, 3, 3, 3, 3, -18, 2, 1, 127, 7, 2, 2, 2, -34, 3, 3, -26, 12, 11, 64, 3, 3, 1, 3, -3, 126, -4, -3, -3, -3, -3, -3, -3, 66, -4, -5, -127, 79, -4, -5, -4, 56, -3, -3, 73, -51, -18, 24, -3, -3, -5, 81, 0, -46, -3, -3, -3, -3, -3, -3, -3, 114, -3, -2, 54, 127, -3, -3, -3, 45, -3, -3, 56, 59, 76, 0, -3, -3, -2, -27, -6, -66, -12, -12, -12, -12, -12, -12, -12, 118, -11, -9, 79, 127, -12, -11, -12, 47, -12, -12, 48, -8, 4, -54, -12, -12, -10, 106, 0, 127, 0, 0, 0, 1, 1, 1, 0, 24, -1, -1, -17, 77, 0, -1, 0, 52, 0, 0, 68, 11, 79, 56, 1, 1, -1, 58, -1, 43, -6, -7, -7, -7, -7, -7, -7, 47, -5, -3, 107, 127, -7, -5, -7, 26, -7, -7, 52, -1, -12, -14, -7, -7, -4, 91, 0, 23, -1, 0, -1, 0, 0, 0, 0, -5, -1, -1, 27, -9, -1, -1, -1, 48, -1, -1, 20, 6, 11, 51, 0, 0, -1, 127, -3, 28, -7, -6, -7, -6, -7, -7, -6, 118, -6, -4, -12, 23, -7, -6, -6, 112, -7, -7, 127, 89, 37, -5, -6, -6, -5, 67, -3, -10, -8, -8, -9, -8, -8, -8, -8, 22, -8, -7, 25, 75, -8, -8, -8, 127, -8, -8, 65, 49, -9, -18, -7, -8, -8, -37, -7, 82, -11, -11, -11, -11, -11, -11, -11, 127, -11, -11, 60, 109, -11, -11, -11, 90, -11, -11, 102, 24, -18, -20, -11, -11, -12, 101, -4, 21, -7, -7, -7, -8, -7, -7, -7, 127, -6, -5, 71, 67, -7, -6, -7, 80, -7, -7, 37, -86, 25, 6, -8, -7, -5, 32, -2, -8, -3, -2, -3, -2, -2, -2, -2, 127, -3, -4, -40, -18, -3, -3, -3, 88, -2, -2, 35, 15, 2, 51, -2, -2, -4, 62, -1, 113, 1, 2, 1, 2, 2, 2, 2, 19, 0, -1, 127, 68, 1, -1, 1, 1, 1, 2, 51, -15, 81, -16, 2, 2, -1, 61, 3, 13, 7, 6, 7, 6, 6, 6, 6, -24, 6, 4, -15, -5, 7, 6, 6, -27, 7, 7, -42, -14, 127, -20, 6, 6, 5, -24, -6, 31, -10, -10, -10, -10, -10, -10, -10, 94, -10, -9, 91, 127, -10, -10, -10, 106, -10, -10, 25, 47, 65, 3, -10, -10, -10, -29, -10, 21, -19, -19, -19, -19, -19, -19, -19, 23, -18, -16, 16, 127, -19, -18, -19, -33, -19, -19, -25, 54, -61, 35, -19, -19, -17, 53, -10, 70, -17, -18, -18, -19, -18, -18, -18, 72, -17, -15, -127, 98, -18, -16, -18, 116, -18, -18, 25, 84, 65, -67, -19, -19, -16, 126, -3, 24, -6, -6, -6, -6, -6, -6, -6, 36, -6, -5, 28, 20, -6, -6, -6, 74, -6, -6, -30, 34, 36, 47, -6, -6, -6, 127, -6, 117, -4, -4, -4, -4, -4, -4, -4, 69, -5, -8, 86, 106, -4, -6, -5, 43, -4, -4, 57, 127, 76, -19, -5, -5, -7, 89, -4, -11, -7, -8, -8, -8, -8, -8, -8, 12, -7, -6, 70, 102, -8, -7, -7, 79, -8, -8, 127, 79, 18, -8, -8, -8, -7, 1, -3, -35, -7, -7, -7, -7, -7, -7, -7, 127, -7, -6, 67, 59, -7, -7, -7, 73, -7, -7, 8, 74, 15, 72, -7, -7, -6, 46, -5, 62, -7, -7, -7, -7, -7, -7, -7, 51, -7, -7, 46, 7, -7, -7, -7, 35, -7, -7, -27, -18, -127, 36, -7, -7, -7, 81, -4, -21, -7, -7, -7, -7, -7, -7, -7, -21, -7, -6, 127, 46, -7, -7, -7, 24, -7, -7, 44, 80, 51, 64, -7, -7, -7, 73, 1, -35, 3, 3, 4, 3, 3, 3, 3, 64, 3, 1, -28, 101, 3, 3, 3, -67, 3, 3, 37, 127, -53, 78, 3, 3, 2, 50, -3, -36, -1, 0, 0, 1, 1, 1, 0, 0, -2, -5, 3, 106, 0, -3, 0, 127, 0, 0, 31, 68, 43, 92, 1, 1, -5, 105, -5, 65, -8, -8, -8, -8, -8, -8, -8, 127, -8, -7, 36, 100, -8, -8, -8, 20, -8, -8, 43, 20, 71, 20, -8, -8, -7, 88, -4, 81, -7, -7, -7, -7, -7, -7, -7, 11, -7, -6, -41, 95, -7, -7, -7, 127, -7, -7, 123, 24, 28, -70, -7, -7, -7, 112, 5, -27, 6, 6, 5, 6, 6, 6, 6, 40, 6, 6, 127, 81, 6, 6, 6, -4, 6, 6, 44, 10, 59, -2, 6, 6, 6, 101, -5, 123, -11, -11, -12, -11, -11, -12, -11, 14, -10, -8, 71, 127, -11, -10, -11, 64, -11, -11, 71, 35, 50, 9, -11, -11, -9, 92, -3, 127, -4, -4, -4, -3, -3, -3, -4, 18, -4, -5, 4, 47, -4, -4, -4, 17, -4, -4, 94, 26, -23, 27, -3, -4, -5, 51, -5, 57, -8, -8, -8, -8, -8, -8, -8, 89, -8, -7, 75, 59, -8, -8, -8, 31, -8, -8, 76, 127, 6, -53, -8, -8, -8, 89, 8, -11, 10, 9, 9, 9, 9, 9, 9, 127, 10, 11, -1, -65, 9, 11, 10, -44, 9, 9, 9, -1, 1, 5, 9, 9, 12, -5, -7, 113, -1, 0, 0, 1, 0, 0, 0, 70, -3, -9, 120, 123, -1, -5, -1, 62, 0, 0, 82, -11, 16, -83, 0, 0, -8, 127, -4, -24, -8, -8, -8, -8, -8, -8, -8, 76, -8, -6, 127, 66, -8, -8, -8, 34, -8, -8, 2, 6, -8, 31, -8, -8, -7, 86};

float bias_raw[672]={0.012049785815179348, 0.027005406096577644, 0.062983438372612, 0.024800989776849747, -0.010794240981340408, -0.01148539874702692, 0.031078077852725983, 0.017923720180988312, 0.03314812108874321, 0.03124469704926014, 0.06653860211372375, 0.025512466207146645, 0.011631003580987453, -0.013022743165493011, 0.04734082520008087, -0.009548421949148178, 0.020356345921754837, 0.03543561324477196, 0.026295509189367294, -0.0018671142170205712, 0.026539253070950508, 0.028427574783563614, 0.02131652645766735, 0.040667083114385605, 0.007991667836904526, -0.033785637468099594, 0.007615434471517801, 0.04167773574590683, 0.02647852525115013, 0.03456591069698334, 0.01542858686298132, 0.007561669684946537, 0.030156515538692474, 0.010631331242620945, 0.014244653284549713, 0.007577734999358654, 0.0435137115418911, -0.015330897644162178, 0.002900482155382633, 0.03517995774745941, 0.023669332265853882, 0.024779727682471275, 0.021233603358268738, 0.009978999383747578, 0.01919625699520111, 0.034683044999837875, 0.038500696420669556, 0.03857981786131859, 0.04175742715597153, 0.048231516033411026, 0.0401487722992897, 0.028328146785497665, 0.04330455884337425, 0.053784679621458054, 0.008259575814008713, -0.02641270123422146, 0.02082553319633007, 0.025511646643280983, 0.02603604644536972, -0.008850213140249252, -0.00452943192794919, 0.09100906550884247, 0.03603746369481087, 0.02565683238208294, 0.046121738851070404, 0.0001760658051352948, 0.03677509352564812, 0.07089607417583466, 0.024882599711418152, 0.011865634471178055, 0.014454635791480541, -0.045718200504779816, 0.017878076061606407, 0.055537790060043335, 0.02181997336447239, -0.009681601077318192, 0.04114643856883049, 0.03376312926411629, 0.00644625024870038, 0.038292936980724335, 0.028807001188397408, 0.02136189304292202, -0.00944889523088932, 0.0037050179671496153, 0.016569243744015694, 0.00993999745696783, 0.05999714881181717, -0.003742730710655451, 0.05362385883927345, 0.004171096719801426, 0.031450144946575165, 0.044766489416360855, -0.005903751123696566, 0.05268606171011925, 0.03247947245836258, 0.024615157395601273, 0.04112941026687622, 0.004786510486155748, 0.03829847276210785, 0.08258466422557831, -0.013962657190859318, 0.011081896722316742, 0.0039015638176351786, 0.030368374660611153, 0.020677773281931877, 0.03238055855035782, 0.049980469048023224, 0.043958816677331924, 0.07414077967405319, 0.02039751037955284, 0.020480545237660408, -0.03496735170483589, 0.0031012985855340958, 0.03656213730573654, 0.024830736219882965, 0.04773709177970886, 0.04493187740445137, 0.05770452320575714, 0.00409339414909482, 0.053490448743104935, 0.03021237440407276, 0.013625429943203926, 0.0014231835957616568, 0.007467704825103283, 0.026205144822597504, 0.02437206357717514, 0.0013074795715510845, 0.03796934336423874, 0.02701355516910553, 0.0029957094229757786, 0.005417718086391687, 0.041722629219293594, 0.018059030175209045, 0.06287948787212372, 0.020830295979976654, 0.029571665450930595, 0.004545279778540134, 0.040084660053253174, -0.09760230034589767, 0.05957940220832825, 0.023845596238970757, 0.07599616050720215, 0.037798210978507996, 0.057199690490961075, 0.02771379053592682, 0.012722552753984928, 0.04516635462641716, 0.02096780389547348, 0.014108609408140182, 0.029424862936139107, 0.026035919785499573, 0.04150734469294548, 0.06842596083879471, 0.009305806830525398, -0.004314854741096497, 0.04273223876953125, 0.013283832930028439, -0.003656798042356968, 0.06152385473251343, 0.026433270424604416, -0.028798744082450867, 0.0323808528482914, 0.035652995109558105, 0.020416490733623505, 0.014030681923031807, -0.0017691070679575205, 0.05422661826014519, 0.02078809216618538, 0.01386233139783144, 0.016626648604869843, 0.04932328686118126, 0.013860869221389294, 0.024814419448375702, 0.04141366854310036, 0.047506462782621384, 0.02476656436920166, 0.01775001734495163, -0.006365619599819183, 0.01532432809472084, 0.0014772594440728426, 0.048286352306604385, -0.004668078850954771, 0.01644243486225605, 0.023004258051514626, 0.01364287082105875, 0.04892328381538391, 0.03783850371837616, -0.022315286099910736, 0.04689282923936844, 0.012189083732664585, 0.037674885243177414, 0.002398766577243805, 0.0542195625603199, 0.062396399676799774, 0.04738515615463257, -0.07296061515808105, 0.05109164118766785, -0.006522763520479202, 0.04592101648449898, 0.030042318627238274, 0.0331287756562233, 0.08768780529499054, 0.004279759246855974, 0.011402812786400318, -0.02540784329175949, 0.05517498403787613, 0.012920900247991085, 0.16430549323558807, 0.0029981823172420263, 0.025075361132621765, -0.004412306472659111, 0.053500138223171234, 0.03798423334956169, 0.020186927169561386, -0.014837929978966713, -0.006075452547520399, 0.0024323416873812675, 0.020999101921916008, 0.0007382530602626503, 0.015211204066872597, 0.04137487709522247, 0.019416021183133125, 0.02698764204978943, 0.01712542586028576, -0.015357280150055885, 0.05387625843286514, 0.038647592067718506, 0.033293768763542175, 0.009084485471248627, 0.0196328517049551, 0.009318353608250618, 0.0515347458422184, 0.10396822541952133, 0.04098822921514511, 0.023155299946665764, -0.004098140634596348, -0.01862822100520134, 0.03675241768360138, 0.0132826529443264, 0.047044262290000916, 0.017458094283938408, 0.024188730865716934, 0.021142994984984398, 0.041434455662965775, -0.005770896561443806, 0.02127034030854702, 0.06363023817539215, 0.0301353819668293, 0.008735865354537964, 0.021472971886396408, 0.03830328211188316, 0.04074224829673767, 0.012901428155601025, 0.03977120667695999, -0.04254350811243057, 0.03946002572774887, 0.02579915151000023, 0.04792758449912071, -0.010426046326756477, 0.03753667324781418, 0.028992706909775734, 0.0026744743809103966, 0.00887941475957632, 0.024480147287249565, 0.01564447395503521, 0.050674062222242355, 0.046732328832149506, 0.010299147106707096, 0.054516106843948364, 0.00011827884736703709, 0.03411246836185455, 0.07318437099456787, -0.012233362533152103, 0.01886298879981041, 0.07848845422267914, -0.027586907148361206, -0.0013277982361614704, -0.037933796644210815, 0.026458075270056725, 0.0041657560504972935, 0.02726120688021183, 0.012567950412631035, 0.014616809785366058, 0.023530034348368645, 0.011978458613157272, -0.01647474430501461, -0.023976780474185944, 0.0583801232278347, 0.007869387045502663, 0.041859131306409836, 0.011117983609437943, 0.025777047500014305, -0.011064024642109871, -0.01651489920914173, -0.024559207260608673, 0.019918838515877724, 0.024740904569625854, 0.023954719305038452, 0.025151409208774567, 0.01563441753387451, 0.02380468137562275, 0.03334104269742966, 0.063216932117939, 0.013790163211524487, 0.031221510842442513, 0.07448139786720276, 0.020849211141467094, 0.03178613632917404, 0.03754233196377754, 0.06501004844903946, -0.023108555004000664, 0.024866851046681404, 0.012314969673752785, -0.009952212683856487, 0.02649473398923874, -0.0033392179757356644, 0.007420277688652277, 0.04679833725094795, 0.019517796114087105, 0.033837899565696716, 0.01833653450012207, 0.03750276938080788, 0.012336046434938908, -0.025226648896932602, 0.043357133865356445, 0.05376771464943886, 0.041117940098047256, 0.0016024846117943525, -0.009145085699856281, 0.013355513103306293, 0.019687533378601074, 0.04880714789032936, -0.012915302999317646, 0.030380403622984886, 0.03260737285017967, 0.03513234853744507, -0.006279226392507553, 0.03898325189948082, 0.016748905181884766, 0.04148263856768608, 0.01111648976802826, 0.007910224609076977, -0.004299852531403303, 0.04009982943534851, 0.05913223698735237, 0.009015500545501709, 0.058747999370098114, 0.011993160471320152, 0.031315162777900696, -0.008856614120304585, 0.07634320110082626, -0.020038001239299774, 0.04352039843797684, 0.0035101755056530237, 0.015840670093894005, 0.017667997628450394, 0.048270829021930695, 0.019103318452835083, 0.06089944392442703, 0.029289482161402702, -0.015805641189217567, 0.023268895223736763, 0.02875819243490696, 0.05967992544174194, 0.02920997329056263, -0.0281077791005373, 0.0533020906150341, -0.017320740967988968, -0.004834187217056751, 0.007833762094378471, 0.03990832716226578, 0.037845149636268616, -0.007017360534518957, 0.031653497368097305, 0.03597494587302208, 0.029276439920067787, 0.0298067107796669, 0.043712690472602844, 0.02533312328159809, 0.03719610720872879, -0.019246695563197136, 0.01592778041958809, 0.039452116936445236, 0.05352700874209404, 0.06440051645040512, 0.03074643947184086, 0.02652512490749359, 0.015519976615905762, 0.01528922375291586, 0.030923321843147278, 0.002459344221279025, 0.02436739020049572, 0.027643730863928795, 0.025025570765137672, 0.03481748327612877, 0.051645148545503616, 0.016017025336623192, 0.02352852188050747, 0.021243106573820114, 0.031347088515758514, 0.050975531339645386, 0.027522709220647812, 0.017037678509950638, -0.00903314258903265, -0.008392455987632275, 0.017657019197940826, 0.05466975271701813, 0.012542317621409893, 0.0035457562189549208, 0.021386638283729553, 0.01963341236114502, 0.0030475356616079807, 0.053028497844934464, 0.035997357219457626, 0.010369718074798584, 0.04045620560646057, 0.03760341554880142, 0.011671354062855244, 0.01220453530550003, 0.0009508616640232503, 0.03034159168601036, 0.028977539390325546, 0.029145201668143272, 0.0054212152026593685, -0.0033983075991272926, 0.04109960049390793, 0.15501849353313446, 0.03492499142885208, 0.04111888259649277, 0.019216012209653854, 0.03462855890393257, 0.023129994049668312, 0.03492389991879463, 0.004582569003105164, 0.02915332466363907, 0.04035858437418938, 0.06317947059869766, -0.0335065983235836, 0.04503258317708969, 0.07182763516902924, 0.03454870730638504, -0.010720114223659039, 0.03604815527796745, -0.05876819044351578, -0.009504027664661407, 0.02897913195192814, 0.0038368147797882557, 0.04323354735970497, 0.026577644050121307, 0.0288336630910635, -0.006139146164059639, -0.0009179345215670764, 0.0249287411570549, 0.053729165345430374, 0.05398812144994736, -0.006488403305411339, 0.018614934757351875, -0.019346708431839943, 0.02389882132411003, 0.006346120499074459, 0.003991102334111929, 0.028246987611055374, -0.002956448821350932, -0.006706759799271822, 0.012373086996376514, 0.02452976070344448, 0.0909164771437645, 0.018214724957942963, 0.02336377650499344, 0.06244131550192833, 0.05705161765217781, -0.009763729758560658, 0.05370919033885002, 0.019407276064157486, 0.04697216674685478, 0.0023725684732198715, 0.02738535962998867, 0.03295263275504112, -0.00018911872757598758, 0.010939543135464191, 0.02614232338964939, 0.0532204806804657, 0.01674177497625351, 0.01391606219112873, 0.055993206799030304, 0.009625941514968872, 0.014478052966296673, 0.014864650554955006, 0.010259829461574554, 0.04862181469798088, 0.044353459030389786, 0.04614667966961861, 0.0219075009226799, 0.03961749002337456, -0.02582671120762825, 0.03171638771891594, 0.019750535488128662, 0.038908701390028, 0.034780025482177734, 0.015174472704529762, 0.04010529816150665, 0.044569093734025955, 0.03008640930056572, 0.020945901051163673, 0.010755687952041626, 0.025506557896733284, 0.004596983082592487, 0.05586949735879898, -0.013295444659888744, 0.06813123822212219, 0.024649903178215027, 0.018424218520522118, 0.016915244981646538, 4.847930176765658e-05, -0.005808347836136818, 0.04881768301129341, 0.0025521907955408096, 0.04947967827320099, 0.0037512138951569796, 0.004965685307979584, 0.04393855109810829, 0.0408841148018837, 0.029407929629087448, 0.024667682126164436, 0.014505447819828987, 0.04938046261668205, -0.027015460655093193, -0.002179380739107728, 0.0056921509094536304, 0.012873496860265732, 0.025762557983398438, 0.004634558688849211, 0.02293144166469574, -0.010110446251928806, 0.024537667632102966, 0.016150787472724915, -0.0017924737185239792, -0.03967084363102913, 0.031030001118779182, 0.10034538060426712, 0.01636703684926033, 0.024890420958399773, 0.006996662821620703, 0.00559381814673543, 0.018918853253126144, 0.06559120118618011, 0.017936989665031433, 0.040713921189308167, 0.00570810679346323, -0.009863835759460926, 0.0200760867446661, 0.08427093178033829, 0.0407552570104599, 0.02077614516019821, 0.02805357426404953, 0.038734450936317444, 0.049346182495355606, 0.06217947602272034, -0.005795221775770187, 0.004223570693284273, -0.018819043412804604, 0.0017884820699691772, 0.04165608063340187, -0.04702151194214821, 0.03988650441169739, 0.03453153371810913, 0.00777252996340394, 0.024615030735731125, 0.0268916804343462, 0.0007636200753040612, -0.002170638646930456, 0.02892838791012764, 0.03878233954310417, -0.02420775778591633, 0.06104991212487221, 0.005585581995546818, 0.0032943179830908775, 0.03979594260454178, 0.019414419308304787, 0.009620374999940395, 0.06221649423241615, 0.04138824716210365, -0.012896782718598843, 0.003781395498663187, 0.03845527395606041, 0.062181588262319565, 0.028056735172867775, 0.04055903106927872, 0.059892959892749786, 0.03463500738143921, 0.053506795316934586, 0.05479981377720833, 0.05799265205860138, 0.01980787143111229, 0.0321253165602684, 0.02895996905863285, -0.002093592192977667, -0.017937757074832916, -0.037251994013786316, 0.022480569779872894, 0.036250654608011246, 0.053525254130363464, 0.06713104993104935, 0.034607384353876114, 0.04970864579081535, 0.051271259784698486, 0.032060448080301285, 0.012784854508936405, -0.027334999293088913, 0.022467393428087234, 0.04345027357339859, 0.011436915956437588, 0.03416375443339348, 0.05041855201125145, 0.014640622772276402, 0.015730878338217735, 0.019419698044657707, 0.0628657191991806, 0.003821553196758032, 0.0122246528044343, 0.02521628700196743, 0.01673685386776924, 0.0038736213464289904, 0.01655569113790989, 0.04631667956709862, -0.004209143575280905, 0.04639636352658272, 0.007995746098458767, 0.06476400047540665, 0.0638328492641449, 0.012145704589784145, 0.004773313645273447, 0.022176576778292656, 0.04389209672808647, 0.0032962465193122625, 0.013311370275914669, -0.01362929493188858, 0.07782687246799469, -0.009452203288674355, 0.02557063102722168, 0.009178422391414642, 0.05521203950047493, 0.009117319248616695, 0.015272253192961216, 0.008242064155638218, 0.02190636657178402, 0.05229815095663071, 0.05833772197365761, 0.029375925660133362, 0.024967560544610023, 0.0036403608974069357, -0.09940408170223236, 0.052619569003582, 0.11534518748521805, 0.07287263125181198, 0.027996765449643135, 0.03398555517196655, 0.035127706825733185, 0.03454119339585304, 0.053511280566453934, 0.023386992514133453, -0.006671329960227013, 0.02629168890416622, 0.030682314187288284, 0.03444124385714531, -0.028963550925254822, 0.036683373153209686, 0.0325089693069458, 0.046044159680604935, -0.15510879456996918, 0.032966431230306625, 0.03586837276816368};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActNone;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={672,1,1,28};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={672};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteInt8;
const TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;
// const float scales_filter=;
// const int32_t zero_points_filter=;
// const float scales_bias=;
// const int32_t zero_points_bias=;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void ExtractConvParams(TfLitePadding padding, int stride_width, int stride_height, 
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteConvParams* data_params) {
  // TfLiteConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params,
                       char* tensor_data, TfLiteAffineQuantization* quant_struct,
                       size_t bytes_size, TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  const float* input_data = GetTensorData<float>(input);
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  const bool need_non_dilated_im2col =
      params->stride_width != 1 || params->stride_height != 1 ||
      filter->dims->data[2] != 1 || filter->dims->data[1] != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // std::cout << "codes runs here #-1" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #-2" << std::endl;
  bool has_bias = false;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));
  // TfLiteTensor* filter;
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  auto filter_input_channel = filter->dims->data[3];
  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;
  // std::cout << "codes runs here #-3" << std::endl;
  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }
  // std::cout << "codes runs here #-4" << std::endl;
  const TfLiteTensor* bias = nullptr;

  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.
  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }
  // std::cout << "codes runs here #-5" << std::endl;
  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8));

  if (is_hybrid && filter->type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  // std::cout << "codes runs here #-6" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  data->supports_multithreaded_kernel =
      (kernel_type == kMultithreadOptimized) &&
      (context->recommended_num_threads != 1) && !is_hybrid &&
      (params->dilation_width_factor == 1) &&
      (params->dilation_height_factor == 1) &&
      (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);

  int channels_in = filter->dims->data[3];
  int channels_out = filter->dims->data[0];
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  // std::cout << "codes runs here #-7" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #-8" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    // std::cout << "affine_quantization->scale->size: " << affine_quantization->scale->size << std::endl;
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #-9" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter->type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }

  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }

  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #-10" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }

    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }

    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #-11" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  // std::cout << "codes runs here #4" << std::endl;
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }
  // std::cout << "codes runs here #5" << std::endl;
  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }
  // std::cout << "codes runs here #6" << std::endl;
  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // std::cout << "codes runs here #7" << std::endl;
      const float* filter_data;
      if (data->need_hwcn_weights) {
        filter_data = GetTensorData<float>(hwcn_weights);
      } else {
        filter_data = GetTensorData<float>(filter);
      }
      // int index;
      // for (index = 0; index < 432; index++){
      //   // std::cout << "filter_data[" << index << "] = " << filter_data[index] << std::endl;
      //   std::cout << filter_data[index] << ", ";
      // }
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(filter), filter_data, GetTensorShape(bias),
          GetTensorData<float>(bias), GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = params->stride_width;
      op_params.stride_height = params->stride_height;
      op_params.dilation_width_factor = params->dilation_width_factor;
      op_params.dilation_height_factor = params->dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #0" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #1" << std::endl;
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;

  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data), 
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;
  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  // std::cout << "codes runs here #10" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_bciiuh_REF() {
  static TfLiteRegistration r = {bciiuh::Init, bciiuh::Free,
                                 bciiuh::Prepare<bciiuh::kReference>,
                                 bciiuh::Eval<bciiuh::kReference>};
  return &r;
}

TfLiteRegistration* Register_bciiuh_GENERIC_OPT() {
  static TfLiteRegistration r = {bciiuh::Init, bciiuh::Free,
                                 bciiuh::Prepare<bciiuh::kGenericOptimized>,
                                 bciiuh::Eval<bciiuh::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_bciiuh_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {bciiuh::Init, bciiuh::Free,
                                 bciiuh::Prepare<bciiuh::kMultithreadOptimized>,
                                 bciiuh::Eval<bciiuh::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_bciiuh_CBLAS_OPT() {
//   static TfLiteRegistration r = {bciiuh::Init, bciiuh::Free,
//                                  bciiuh::Prepare<bciiuh::kCblasOptimized>,
//                                  bciiuh::Eval<bciiuh::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_bciiuh() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_bciiuh_MULTITHREADED_OPT();
#else
  return Register_bciiuh_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
