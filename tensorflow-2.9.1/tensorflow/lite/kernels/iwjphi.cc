/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h"

#include <stddef.h>
#include <stdint.h>
#include <vector>

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/optimized/cpu_check.h"
#include "tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h"
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h"
#include "tensorflow/lite/kernels/internal/optimized/neon_check.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/internal/types.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"

namespace tflite {
namespace ops {
namespace custom {
namespace iwjphi {

constexpr int kInputTensor = 0;
constexpr int kFilterTensor = 1;
constexpr int kBiasTensor = 2;
constexpr int kOutputTensor = 0;

// This file has three implementation of DepthwiseConv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  kNeonOptimized,
};

const int kTensorNotAllocated = -1;

int8_t filter_r   aw[4320]={4, 8, -15, 22, 21, 1, -51, -7, -2, -9, -3, -5, -15, -35, 78, 14, 5, -10, -8, 20, 6, -123, 6, 23, 35, -12, 34, -21, -6, 2, 9, 38, -3, -15, 15, 12, 8, 6, -1, 16, 9, 10, -14, 20, -5, 0, 39, 4, 22, 15, 4, -26, -1, 16, -8, 10, -40, 8, -3, -11, -35, 52, 49, -6, 40, -5, 3, -2, 15, 1, 4, -20, -11, -6, -10, -69, 15, -21, -10, 19, 12, 2, 30, 1, -2, -4, -20, 11, 11, 5, 5, 18, 21, -20, 7, -20, 74, -20, -8, 20, -5, 12, 5, -2, 12, 1, 8, -14, -6, 0, -83, 23, -28, 10, -61, -22, 33, 2, -38, 30, 9, 4, 14, -23, -5, 15, 4, 3, 1, 14, 1, -10, 8, 23, -35, 55, -30, -21, 18, -71, 25, 2, -1, 10, 7, -9, 9, -9, 0, -4, 8, 11, 40, -1, 57, -80, -25, 0, -7, 16, 0, 20, -1, -3, -24, 6, 41, 10, 0, 17, -8, -14, 18, 16, -64, -15, -12, 15, 35, 7, -18, -22, 44, -7, -12, -3, 1, 6, -3, -2, 10, -19, 11, 0, 14, 12, -4, 1, -20, 2, 75, 106, 5, 65, -58, 9, 10, -12, 9, -8, -89, 17, 9, 60, -97, 5, 1, -35, -3, -6, -2, 16, -11, -5, 2, -56, -3, -10, -11, -6, -1, 35, -33, 13, -102, 4, 7, 23, 18, 25, -4, -20, -4, -28, 38, -14, 10, 2, 1, 13, -62, -97, 14, 39, -9, 0, -28, 12, 6, 6, -20, -6, 28, 17, 18, 1, -40, 0, -42, 22, -100, -8, -18, -4, -27, 11, -44, 16, -2, -4, 6, -1, 1, 12, -126, 4, 78, 5, -5, 2, 15, 8, 8, 3, -25, -99, -9, -25, -6, -1, 69, 18, 11, 1, 5, -5, 8, 42, -36, 103, 3, 29, 13, 3, 6, 4, 2, 3, 12, -4, 5, -16, -16, -2, 31, 4, 10, -10, 2, -31, -1, 5, -13, -8, -16, -1, 15, 127, 5, 27, 2, 10, 3, 5, -11, 15, -20, 4, -17, 8, 2, 8, -24, -12, 1, 15, -59, 0, 0, 70, 11, 6, -2, 31, -5, 8, -11, 9, 5, -23, -7, 59, 6, 25, 46, 25, 0, 1, -1, -37, 37, 9, -30, 7, 7, -117, -39, -1, 20, -8, 4, 13, 26, 9, 1, 5, 19, -5, 35, -13, -20, 3, 98, -78, -22, -16, 9, -1, -13, 2, 1, 7, 10, -16, -28, 2, -4, -13, 57, 47, 9, -2, -59, -8, -46, 7, 2, 7, -17, 12, 12, -12, 8, -2, -29, -8, -9, 6, 25, 7, 5, 1, -3, -15, 5, -42, 0, 14, 6, -18, -1, 18, 12, -33, 13, 19, 23, -10, -36, -4, -14, -4, 11, 9, 10, 2, 0, 0, -44, 93, 25, -7, -10, -2, -2, 5, 6, 11, -9, -30, 40, 127, 127, 59, -88, 16, -127, -100, -58, -11, -4, 21, -39, 127, -24, 11, 44, -4, -3, -41, -97, -116, -6, 25, 61, -76, 127, -24, -21, 22, 14, 61, 22, 39, 12, -2, 8, 33, -35, 32, 24, 127, 3, 47, 2, 0, 97, 18, 70, 20, 12, -98, -9, 17, -127, -6, -80, -127, 45, 17, -127, 4, 81, 23, 89, -62, 0, -9, 37, 9, 12, 5, -100, -8, -3, -106, 27, -70, -44, 18, 2, -69, 37, 14, 19, -5, 127, 21, -61, 10, 33, 127, 127, 6, 39, -22, 39, -101, -72, -49, -32, -98, 18, 17, 16, 11, 21, 14, -27, 0, -67, -6, -41, 89, -114, -35, -101, 7, -91, -9, 20, -92, 127, -35, -6, 77, 9, 22, -14, 49, 124, 39, 62, 11, -69, 87, -68, 28, 44, -127, 84, 35, 7, 12, 10, -9, 43, 14, 7, -20, -14, 15, 93, 0, 62, 127, -30, 19, 38, -61, -4, 6, 18, 26, -72, 11, 12, 35, -1, 127, 62, -29, 43, 105, -113, -119, -126, 24, -117, 51, 94, 0, -69, 32, 106, 16, 125, -45, 26, -30, 24, -24, 13, -33, 20, 34, 29, 35, -60, 9, 94, 81, 93, 89, -32, -2, 55, 8, 23, -5, 11, 56, 25, -63, 80, 30, 11, -119, -17, -8, 12, 56, -62, -13, -59, 127, 16, 24, 0, 44, 70, 69, 18, 48, -127, 13, -20, 110, 23, -7, 3, 25, 4, -85, -37, -72, 2, -115, 8, 22, -127, -83, -118, 35, -127, 19, -55, 94, 4, -65, -53, 4, 24, 33, 12, 15, -124, 21, -20, 10, -70, -2, -127, 4, 106, -51, -35, 42, -38, 3, 12, -53, -2, 3, -120, 106, 76, 127, 90, 7, 0, 21, 16, -3, -24, -103, 15, -114, 11, 19, -12, 60, 38, -72, 19, -1, 10, 16, -110, 98, -11, 59, 36, -6, 111, 19, -36, 31, 2, -100, 23, -56, -39, 13, 3, 47, 54, -51, 10, -72, -23, 9, -24, 127, -1, 25, 5, 16, 23, 25, 0, 13, 27, 3, 32, -9, -39, 13, -91, 37, 28, 38, 10, 30, 65, 28, 27, 89, -38, 29, 83, -9, -7, 123, 68, 43, -35, 11, -7, 0, -48, 107, 35, -27, -115, 106, 15, 101, 117, -29, -43, 11, 6, -20, -68, -71, -8, 38, 27, -28, 16, 19, 84, 116, 4, -4, 47, -10, 127, -1, -5, -16, -10, -116, -112, 120, -65, 12, -127, 49, 2, 16, 6, -31, -91, 19, -3, -37, -2, 63, 42, 94, 33, 24, -99, 18, 20, 13, -99, 37, 21, 15, 27, 16, -127, 16, 20, -6, 62, 32, 21, -3, 87, 12, -14, -116, -20, -127, 7, -9, -12, -28, 32, -50, 24, 86, 1, -16, 18, 85, -26, -8, 3, 36, 66, 87, 9, 8, 67, 49, -24, -39, -14, 19, 26, 48, -3, 8, 35, 9, 13, 17, -10, 9, -105, -91, -54, -38, 1, -9, -3, -3, -10, -47, -70, 127, 2, -13, -13, 11, -108, -109, 6, 25, 41, 16, 30, -27, -5, 4, 9, 39, -2, -20, 11, 11, 6, 4, 0, 12, 9, 9, -14, 15, -1, -9, 0, 2, 58, 14, 3, 8, -6, 18, -13, 7, -37, 3, -6, -11, -50, -71, 42, 4, -4, 6, 3, -1, 14, 4, 3, -17, -20, -9, 7, -66, 13, 22, -9, 15, 10, 9, 37, -4, 1, -4, 26, 9, 7, 7, 8, 16, 12, -22, 0, -23, 62, -7, -18, -1, -4, 9, 7, -2, 10, -2, 10, -11, -5, -3, -66, 18, -28, 36, -54, -14, -35, 1, -23, -36, -14, 2, 6, -23, -8, -1, 3, 2, 2, 11, 8, -13, -1, 0, -36, -1, -30, -18, -14, -58, -22, -3, 1, 8, 8, -8, 4, 2, -2, -3, 3, 3, 3, 14, 26, 13, -24, -1, -8, 4, 1, -58, 4, 11, -29, 6, 18, 19, -1, 11, 127, -7, 17, 76, 0, -12, -35, 12, 44, 8, -13, -25, 54, -3, 74, 1, 0, 6, 17, -5, 11, -19, 9, 1, 17, -9, -3, 1, -16, -1, 82, -2, 20, 2, 34, 5, 12, -9, 5, -9, 1, 15, -53, 39, -14, 3, -2, 14, 0, 19, -2, 15, -12, -6, -7, -60, 3, -25, -8, 4, -4, 0, 26, 6, -107, 6, 12, 44, 10, 0, -2, -19, -6, -24, -32, -2, 7, -28, 0, 13, -59, -89, 23, 44, -15, -1, -42, 18, 3, -4, -9, -2, -19, 2, 18, -3, -55, 0, -52, 33, -3, -11, -3, -1, -31, -41, 9, 21, 3, -2, 6, 2, 1, 8, -114, 23, 19, 8, 57, -1, 10, 4, 7, 4, 2, -98, -4, -4, -23, 3, -17, 18, 13, -3, 12, -6, 6, -24, -45, -1, 2, 17, 15, 3, 20, -2, 1, 6, 11, -34, 11, -17, -12, -4, -8, 1, 23, -6, 6, -30, -2, 2, -8, 99, -6, -1, 20, -3, 0, 32, -1, 29, 3, 9, -9, 12, -18, 2, -18, -6, 8, 1, -26, -3, 3, 26, 18, 38, 5, -11, -4, 5, 1, 18, -3, 10, -6, 9, 6, 33, -9, 46, 12, 25, 32, 12, -2, 3, 12, -36, -41, 9, 16, 5, 1, -75, -32, 0, 17, -3, 7, 10, 26, 13, 1, 5, 20, -8, 81, 4, -17, -3, -124, -89, -11, -32, 2, -3, -43, -46, 2, 4, 10, -15, -25, 0, -3, -13, -14, 40, 10, -2, 21, 50, -37, 12, 4, 9, -12, -2, 12, -3, 8, 0, 73, -7, -8, 9, 26, 7, 14, 0, 56, -18, -27, 3, -4, 11, 6, -19, -6, -28, 12, -32, 10, 25, -24, -11, -5, 30, -10, -8, 10, 11, 9, 2, -3, -3, 40, -58, 2, 1, -8, -2, -8, 3, 2, 11, -6, -46, 21, -17, -68, 111, 22, 37, -97, 117, -23, 3, 25, 20, -11, 98, 6, -78, 24, -13, -6, -40, 57, -102, 16, 35, 68, 82, 121, -7, -13, 18, 23, 24, 26, 40, 8, 7, 5, 38, -28, 33, 19, -1, 0, 32, -47, 11, 106, 10, -25, 18, 9, -38, -8, 21, 115, 44, -94, 54, 42, 11, -112, -6, -11, 11, 98, 127, 56, -5, 39, 14, 8, -5, -127, -33, -22, -127, 36, -127, -44, 24, 9, -48, 51, 23, 16, -7, -57, 17, -49, 18, 22, -29, -14, 15, 39, -16, 54, -114, -46, 127, -19, -127, 16, 2, 13, 20, 19, -24, -24, -3, -121, 0, -36, -121, -123, -34, 102, 14, -92, 122, 56, 69, 3, -28, -5, 105, 23, 17, -11, 39, -47, 33, 86, 53, -49, 112, 53, 27, 0, 24, 127, 17, 26, 11, 16, -14, 45, 30, -1, -28, 1, 5, 97, 82, 29, 13, -27, 25, 14, -107, -4, 16, 14, 90, -83, 12, 7, 29, -3, 11, -27, -37, 30, -127, -127, -127, 78, 19, -98, -51, -9, 3, 127, 30, -127, 127, -16, 5, 20, -28, 30, 9, 5, -38, 22, 23, 25, 34, -110, 9, 81, 82, -127, 64, 127, -16, 62, 19, 25, -8, -67, 37, 20, 116, 70, 36, 13, -127, -14, 63, 11, 36, -52, -21, 12, -20, 13, 45, -11, -2, 16, 121, 11, 5, -110, 8, -13, 62, 31, 98, 5, 24, 0, -72, 127, -45, 14, 78, 10, 17, -93, -127, 127, -41, 28, 20, -61, -13, 74, -86, -61, 2, -26, 34, 26, 16, -122, 26, -32, 13, 3, -1, -101, 5, -45, 1, 2, 54, 121, -6, 9, -34, -9, -3, -101, 127, 20, -46, -127, -3, 20, 30, 21, 5, -6, -111, 36, -127, 6, 4, 2, 31, 37, -73, 1, -6, 9, 127, 27, 115, -11, 62, 32, 0, -120, -24, -37, 27, 21, 122, 19, -41, -41, 27, -127, 15, 69, -40, 23, -91, -28, 5, -26, -123, 85, 58, 12, 93, -74, -32, 12, -13, 28, 8, 24, -4, -9, -1, 114, 36, 40, 34, 20, 20, 127, 30, 2, -127, 13, -2, 76, -9, -3, 10, -4, 44, 109, 19, -4, -127, -68, 89, 26, 38, 115, 120, 9, -39, 110, -22, -74, 12, -110, -27, -75, -82, 87, 46, -86, 5, 7, 20, 127, 83, 9, 1, 42, -28, -79, -10, -7, -21, -27, -127, 109, -65, 65, 15, 35, 5, -2, 23, 13, -26, 7, 10, -5, -34, -100, 90, 37, 20, 32, -37, -33, 23, 21, 18, -98, 19, 7, -1, 18, 45, -17, 72, -4, 10, 26, 27, 43, 1, -127, -100, -35, -80, 39, 109, 11, -15, -5, 108, 35, 8, 19, 16, 108, -22, -127, -127, 91, -5, 7, 37, 67, -14, 10, 6, -80, 5, 64, -42, -12, 19, 32, 58, -1, 27, 11, 31, 127, -21, -16, 0, 127, 127, -126, -10, -124, -127, 127, 127, -127, 97, 127, 52, -127, 127, 127, -127, 127, -109, -127, 127, 127, -127, 125, 127, -127, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, -118, 127, 127, 127, -127, 14, 127, -127, -127, -127, -127, 127, -127, -108, 127, -127, 75, 127, 127, -4, 127, -127, 127, -18, -119, -127, 127, 127, 127, -127, 127, -60, -127, -94, -79, 127, -11, 127, 127, -127, -127, 127, 127, 127, -127, 111, 127, -127, 127, -127, -101, -102, 127, 127, 127, 127, -83, -127, -108, -127, -51, 127, 127, 127, 127, 127, 127, -127, 127, -78, 127, -127, 37, -75, 127, 113, 127, -127, -127, 86, -127, -127, 127, 127, 32, 127, 127, -127, 127, -127, 127, 127, 115, -127, -33, -99, 127, -127, 89, 119, 127, -127, -127, 127, 127, 127, 107, 127, -127, -127, 127, -33, -127, 127, -35, 127, 127, 127, -127, -127, -127, 127, -8, 127, 127, 127, 127, -127, 8, -113, -127, 127, -122, -10, -82, 20, -127, -76, -127, 127, 127, -123, 127, -27, 121, -127, -126, 127, -127, 127, 127, 127, -127, 127, 126, 127, 127, 109, 127, 127, -127, 9, -127, 64, -127, 127, 127, -127, -127, 127, 127, 77, -127, 109, -127, 127, -4, -127, -127, 127, -127, 127, -91, 127, 37, 127, 111, -86, 127, -127, 54, -127, 127, 5, 127, 127, 110, 127, -127, 127, 127, 127, -127, -47, -99, 127, -92, -127, 127, -49, -76, -19, -127, 84, -127, -127, -127, -127, -127, -127, 127, 127, 127, 127, 127, -114, 127, -127, 127, 127, 127, -52, 127, 127, 127, 127, -127, -109, 127, -127, -127, -127, -127, -73, -18, -127, -113, -45, 127, 127, 127, -127, 127, -89, -91, 127, -41, 127, 127, 127, -127, 127, 127, 127, 127, -127, -73, 127, -116, -127, 127, 127, -127, 55, 127, -37, -127, -127, -44, -127, 127, 127, -127, 41, 127, 127, -127, 127, -127, -127, 127, 127, -101, 127, -127, 127, 20, 127, -127, 127, 127, 127, -127, 127, -127, -59, 127, 12, 127, 127, 127, -127, 127, 118, 127, -127, -1, -127, 127, 127, 127, 127, -127, -127, -127, -127, 127, -127, 74, -127, 127, -127, -127, -105, 32, 127, -127, 127, -127, 24, 127, -38, -127, -127, 127, 127, 127, -127, -118, 127, 127, 56, 19, -127, 127, -127, -107, -85, -103, 127, 26, 38, 20, -35, 13, -127, 127, 109, 127, 127, -127, 127, -127, -127, 127, 127, -127, 92, -127, 127, -7, 127, -127, -127, 127, 127, 127, -127, -127, -127, -124, 127, 127, 1, -127, 127, 127, -127, 127, 127, -127, 12, 127, 127, -26, 127, 91, 127, 127, 127, -17, -127, -121, -127, 127, -127, 127, 112, 12, -127, 127, 127, 127, 127, -127, 127, 127, 127, -127, -127, -127, -127, 127, 127, 127, 127, 127, 127, 127, 24, -19, -80, 127, 66, -57, -95, -127, -25, 1, 31, 22, 7, 112, -12, 121, 26, -12, -2, -37, -15, -98, 5, 25, 72, -99, 116, -16, -11, 17, 21, 15, 21, 37, 11, 9, 9, 42, -30, 34, 15, -5, -1, 38, -40, 12, -127, 12, 38, 18, 10, 70, -4, 18, -84, 47, -88, 59, 57, 8, 75, 6, -5, 10, -127, -30, 52, -2, 31, 10, 11, -3, 100, -20, 127, -108, 35, 102, -41, -32, 9, -44, 49, 27, 15, -8, -6, 15, -51, 16, 28, -25, -18, 13, 46, -16, 50, 127, -56, 10, -16, -123, 10, -2, 13, 19, 22, -8, -22, -7, -127, 7, -41, 127, -124, -31, 113, 15, -88, -41, -127, -100, 4, -32, -4, -127, 31, 11, -12, 35, -54, 32, 71, -127, -59, -127, 28, 19, -14, 26, 109, 22, 37, 13, 21, -9, 43, 26, 2, -24, 94, 11, -127, 3, 36, -1, -27, 21, 14, -119, -5, 22, 8, -127, -94, 11, 12, 26, 0, 5, 30, -41, 27, 53, 116, 97, -127, 19, -97, -53, -13, 0, 123, 35, 102, 126, -12, 127, 31, -26, 25, -1, 4, -37, 26, 51, 32, 39, -127, 7, 70, -86, 94, -36, -61, -14, 70, 10, 23, -10, 34, 43, -127, 98, -127, 44, 6, 103, -17, 59, 12, 41, -55, -20, 20, -11, 11, -127, 127, -6, 10, -114, 85, 9, -115, 8, -9, 98, 31, -9, 4, -1, 1, -81, -58, 62, 19, -118, 7, 16, -94, -107, 112, -43, 33, 18, -71, -11, 79, 59, -83, 1, 45, 33, 15, 15, -127, 21, -24, 6, 38, -2, -103, 8, -81, -14, -17, 57, 127, 0, 10, -36, -10, -11, -124, 114, -86, -50, 117, -5, 31, 32, 18, 5, 127, -127, 31, 105, -2, 7, 27, 32, 35, -71, 0, -4, 8, -74, 32, -127, -11, 69, 28, 4, 124, -11, -31, 38, 24, -127, 24, -40, -39, 20, 83, 39, 66, -50, 17, -96, -24, 7, -30, 111, 79, 52, 10, -92, -37, -42, 9, 26, 30, 7, 25, -5, -1, 1, 111, 40, 35, 31, 19, 21, 116, 29, 47, 115, 13, 113, 95, -9, -5, 16, 4, 44, 113, 5, -3, 68, -68, 55, 21, 39, 113, -103, 11, -39, 80, -33, 127, 8, 127, -25, -79, 52, 84, 54, -88, 6, 11, 12, 112, -92, 6, 1, 37, -40, 19, 127, -4, -20, 20, -112, -127, -68, 56, 12, -17, -7, -2, 26, 13, -18, -4, 8, -7, -30, 127, 87, 34, 25, 31, 89, -37, 14, 26, 16, -109, 24, 7, -8, 20, 44, 118, 55, -5, 8, 31, 33, 44, 0, 107, -100, -59, 73, 40, 115, 13, -12, -8, -127, 38, 3, 19, 19, -59, -25, 45, 120, -61, -11, 12, 40, 63, -14, 6, 7, 84, 2, 68, -43, -16, 19, 38, 56, 1, 28, 17, -18, 2, -4, 18, 47, 4, 46, -76, 54, 34, -5, -3, 0, 20, -34, -76, -120, 8, -15, -10, 11, 127, -95, 13, 18, 29, 111, 7, -7, -8, 3, 8, -42, 4, -5, 15, 19, -11, 12, -10, 15, 16, -22, -15, 7, -8, 15, -8, 3, -22, 12, 6, 18, 2, 14, 57, -8, -34, -4, 36, -5, 13, -74, 51, -5, 1, 29, 11, 2, 17, -1, 5, -22, -8, 14, -5, -65, 3, -12, -14, -1, 9, 16, 42, -3, 1, -4, -16, 12, 14, 13, 8, -7, -9, -25, 8, -22, 43, -7, -4, 38, -9, 14, 8, 4, 12, 6, 12, -18, -7, -4, -99, 14, -26, -89, -45, 17, -21, 8, -28, 41, 29, 106, 3, -20, 6, 11, 13, 6, -2, 16, 1, -3, -23, -7, -41, 6, 36, -21, -20, 13, -11, -4, 1, 7, 5, -6, 23, 6, -8, -2, -6, 5, -9, 6, 26, 85, -28, -5, -1, 6, -3, -68, 2, -52, -32, 4, 27, 17, 1, 22, -21, -13, 11, 29, 3, -9, 34, 11, 16, 4, 12, -17, 35, -3, -56, 17, 1, -27, 22, -7, 5, 27, 9, -2, 7, 12, 8, 4, -23, -3, 77, -4, -39, 0, -50, 3, 12, -7, 0, -14, 14, 17, 15, -2, -94, -1, 0, 0, 4, 13, -2, 22, -15, 13, -8, 24, 3, 5, -2, 9, -7, 14, -21, 21, -81, 8, 10, 38, 11, 22, -5, -32, -2, -14, 40, -4, 11, 105, -2, 8, -58, -66, -1, 38, 17, 12, -56, -9, 11, 1, -23, -1, -17, -20, 7, 3, -57, -3, -40, 28, -6, -7, -19, 2, -19, -39, 10, 21, -4, -6, 7, -1, -4, 9, -127, 28, -26, 5, -27, -3, 9, 9, 8, 5, -1, -86, -3, 2, -19, -6, -5, 14, 20, -4, 10, -9, 6, 52, 7, -25, -2, 23, 10, 4, -111, -12, 29, 13, 15, 58, 10, -15, -19, 30, 54, 25, 14, -3, 11, -41, 0, -8, -8, -18, 5, -1, 16, -19, -40, 26, 1, -117, 6, 24, -5, 12, 15, 6, -9, -1, -3, 11, 24, 3, -10, 19, 14, -49, -4, 2, 13, 2, -3, -11, 1, 12, -4, 4, 8, -26, -16, 73, 7, 27, 31, 2, -3, 1, 14, -39, 17, 5, -26, 4, 10, -45, -49, 7, 2, 9, 6, 8, -5, -9, 4, 1, 20, 16, -28, -22, -17, -16, -127, -99, 31, -15, 7, -7, 12, 41, -4, 10, 5, -12, 34, -5, -5, -11, -12, 49, 5, -19, 21, -4, -25, 8, 7, 5, 7, -3, 11, 7, 16, -5, -11, -4, -4, 5, 18, 9, 1, -3, -72, -1, -20, 6, 11, -43, 9, -22, -9, 50, 11, 12, 10, 27, 16, -12, -9, -48, 47, -5, 10, 14, 9, 2, -4, -2, -68, -27, 1, -4, -9, 1, -11, 3, 3, 3, -39, 8, 16, -46, -49, 34, 16, 19, -80, 92, 127, 124, -3, 26, 121, 104, -9, -45, 37, -9, -5, -35, 72, -127, -10, 30, 81, 69, 120, 32, -13, 23, 17, -88, 23, 40, 25, 3, -9, 37, -24, 32, 18, 6, -4, 42, 12, 126, -112, 14, -33, 15, 15, 66, -1, 20, 63, 45, -103, -100, 80, 23, 127, -9, 86, 11, -82, -31, -4, -1, 33, 10, 8, 7, 95, 115, -36, -78, 38, 75, -44, 25, 2, -106, 69, 13, 16, -8, -25, 19, -53, 18, 28, -33, -28, 1, 53, -15, 52, 124, -61, -26, -19, -98, 11, 11, 19, 18, 15, 25, -24, 3, -106, 17, -32, -105, -127, 34, -127, 13, -79, -6, 10, 78, -72, -40, -4, -87, 21, 2, -11, 55, 125, 24, 79, 25, -39, -97, 127, 47, 18, 0, 65, 33, 7, 16, 19, -17, 69, -127, 5, -21, -2, 16, -107, -7, 85, -97, -13, 25, 20, -81, -1, 14, 7, -6, -96, 17, 29, 14, -1, 111, -70, -44, 29, -42, 111, 104, 109, 25, -127, 59, 78, -1, -10, 42, -113, 40, -19, -22, 44, -33, 31, 38, 9, -32, 33, -127, 21, 31, -57, 5, 90, -63, -86, -31, 34, -4, 68, 7, 34, -3, 16, 33, 17, 11, 81, 35, 13, 82, -15, -5, 13, 33, -59, 127, -33, -36, 20, 29, -55, 69, 65, -103, 15, 49, -88, 3, -9, 127, 17, -23, 7, 5, 3, -58, -19, 46, 23, 127, 10, 31, -97, -117, -98, 37, 42, 21, -56, 4, 1, 79, -60, 0, -87, 37, 21, 12, -111, 23, -28, 6, 75, -12, -107, 3, 85, -18, -61, 45, -31, -2, 14, -33, 2, -2, -120, 80, -21, 104, -104, 1, 15, 22, 21, 3, -38, -99, 29, 115, -1, 10, 8, 50, 38, -66, 13, -4, 5, -4, 46, -111, -4, 57, 47, -5, -127, 27, 127, 25, 18, 94, 23, -29, -38, 64, -18, 19, 41, -30, 26, -75, -23, 29, -24, -110, 18, 14, 7, -41, 14, 51, -7, 31, 25, 26, 32, -11, 127, 18, -127, 41, 13, 40, 110, 20, 70, 41, 7, -112, -51, 15, 78, -2, -2, -83, 73, 35, -27, 5, -12, 1, -28, 100, 20, -29, -127, -127, 20, 108, 91, -29, 29, 9, 1, -20, -89, 94, -33, 37, -11, 127, 16, 17, 73, -127, 7, -5, 28, 127, -51, -21, -8, 127, 16, -98, 113, 127, -86, 15, 46, -12, -5, 12, 16, -32, 118, 10, -9, -36, -32, 77, 44, -127, 50, -32, -97, 20, 27, 24, -88, 38, 16, 127, 24, 26, -1, 10, 2, -12, 27, 27, 25, 2, -101, -1, -7, 127, 18, -123, 11, -2, 4, 33, 43, 127, 20, 71, -3, -23, 43, -107, 101, -15, 2, 37, 71, 83, 24, 7, -89, -15, -13, -31, -19, 16, 17, 41, 2, 26, -5, 30, 6, -10, 13, 34, -4, 24, -69, 2, 32, -7, -4, -4, 21, -45, 86, -12, 4, -13, -7, 11, -3, -98, 14, 17, 38, -3, 15, -10, -5, 0, 8, -39, -7, 5, 12, 17, -6, 11, -1, 17, 19, -25, -16, 11, -13, 10, -46, 10, -2, 13, 8, 67, -1, 16, -60, -7, -34, 2, 29, -9, 43, 58, 50, -1, -22, -1, 5, -2, 20, 2, 7, -23, 76, 18, 3, -77, 9, 16, -16, -109, 11, 15, 45, 2, 3, 2, -8, 14, 16, 9, 4, -5, -11, -26, 10, -25, 25, 65, 0, -30, -8, 10, 6, 6, 11, 6, 11, -20, -7, -2, -72, 13, -26, 7, -45, 15, 12, 11, -44, -27, -2, 9, -5, -25, 3, -56, 6, 6, -1, 9, -2, -6, -23, -42, -39, -24, 35, -25, 54, 12, 32, -5, 5, 8, 7, -7, 26, 6, -5, -3, 5, 7, -4, -2, 42, 0, -25, -3, -2, 8, -2, 22, 5, 63, -29, 6, 41, 13, -3, 31, 1, -15, 20, -34, 63, 98, 3, 14, 22, 8, -19, -20, 37, -3, -1, 27, -7, 44, -11, -4, 5, -14, 11, 2, 5, -87, 6, 3, -26, -5, 70, -22, 2, -18, 19, 12, 5, -7, 8, -12, 21, 19, -45, 5, -13, 2, -8, 45, 11, -2, -1, 22, -4, 7, -3, 16, 2, -8, -12, 8, -6, -127, 35, 19, -82, 6, 11, 9, 4, -1, -9, 17, -9, -11, -29, 127, 10, 24, 3, 9, -66, -77, -3, 29, 14, 9, -56, -9, 9, 104, -20, -10, -1, -12, 9, 3, -52, -2, -35, 38, 87, -9, -24, -1, -19, 10, -25, 17, -4, -6, 9, -6, -2, 12, -120, 28, 28, 1, 9, -9, 6, 1, 6, 0, 28, -79, -2, 61, -10, -4, 83, 16, 17, -4, 9, -7, 6, -45, 12, -7, 0, 17, 7, 4, 4, 8, 29, 15, 17, -7, 10, -17, -17, 31, -26, 17, 8, -7, 12, -31, -1, -6, -15, -10, -13, -2, 14, -85, 12, 27, 3, 3, 5, 21, -7, 11, 18, 3, -18, 0, -6, 7, 24, -2, -25, 22, -64, -1, 3, 2, 17, 3, -2, -21, -4, 8, -5, 7, 5, -7, -15, 51, 8, 27, 43, -95, -3, -6, 31, -28, -49, 8, 18, 3, 6, 105, -34, 1, 10, 3, 5, 8, 19, -25, 3, 6, 24, 18, -11, 8, -18, -19, 77, -105, -10, -2, 1, -6, 29, -14, 1, 14, 11, -13, 36, 3, -3, -7, -99, 46, 11, -16, -32, 25, -9, 14, 10, -1, -1, 10, 9, 5, 15, -6, 22, -5, -1, 3, 20, 11, -1, -3, 7, -5, 16, 37, 16, -32, 7, -20, -7, -16, 12, 11, 13, 28, -41, -12, 1, 2, -2, 1, 11, 11, 27, 5, -7, 0, -49, 32, 19, -8, -10, -2, -9, -11, 3, 2, -44, -32};

float bias_raw[480]={-4.681673526763916, 0.155743807554245, 0.42355242371559143, 0.3177405297756195, -0.09573709964752197, -0.12264591455459595, 2.698241710662842, -0.9229760766029358, -1.065106987953186, 0.47252416610717773, -1.1291974782943726, -2.676919937133789, -1.128328561782837, -2.06193208694458, -4.108729362487793, 0.15694954991340637, 0.8587119579315186, -1.641684651374817, -0.10636013746261597, 0.5805909633636475, 0.06108393520116806, -0.7632166743278503, 0.4025022089481354, 0.9138486385345459, -1.6398658752441406, -0.8164005279541016, 1.1131248474121094, -1.7389633655548096, 3.983290910720825, -1.7879319190979004, 0.5332023501396179, 0.42337650060653687, -4.038797378540039, -1.813978672027588, -3.329697608947754, -0.8061094880104065, 0.009342044591903687, -0.5641916990280151, -0.5484683513641357, 0.6077709197998047, -1.8874226808547974, -0.9020753502845764, 0.05944538116455078, -2.88584041595459, -0.9056156873703003, -1.8755556344985962, -1.0247411727905273, 1.8437438011169434, -1.1807968616485596, -1.0765808820724487, -0.29869168996810913, -1.396447777748108, -0.9463791847229004, -0.636090099811554, -0.14350199699401855, 0.5256249904632568, 1.2880136966705322, 1.0255705118179321, -0.4148196876049042, -0.6238424777984619, -0.783958911895752, -3.0234179496765137, 0.6293473839759827, -1.4155771732330322, -1.1715946197509766, 0.9637565016746521, 0.4541603624820709, -0.7549603581428528, -2.7280783653259277, 0.5791088342666626, 1.982872724533081, -0.37210023403167725, -0.8893604278564453, 0.0044367313385009766, -1.2521697282791138, -0.00864727795124054, -0.36845290660858154, -0.9396104216575623, 0.854457437992096, -0.015513598918914795, 0.047856688499450684, 0.9371742606163025, -4.094842433929443, -0.7302548885345459, -1.4095067977905273, 1.5909004211425781, -3.142090082168579, -2.283898115158081, 1.1636220216751099, 0.3529112637042999, -0.5981217622756958, -0.9088491201400757, -0.5882773399353027, -0.9052978157997131, 1.5492583513259888, 0.6304776072502136, 1.4777007102966309, -0.8080909252166748, 2.239285469055176, -1.0238085985183716, 2.186107635498047, 1.5160787105560303, -0.9920927882194519, 0.20856738090515137, -1.1007875204086304, 1.256713628768921, -3.8637638092041016, -1.6049671173095703, 1.0374722480773926, -0.6021837592124939, 1.7317218780517578, 0.5113061666488647, -0.2884562015533447, -0.8794034719467163, 1.2684544324874878, -1.9336867332458496, -2.262784004211426, 0.07010793685913086, 0.21125656366348267, -1.1863473653793335, -1.845696210861206, -0.7689026594161987, -0.7987087965011597, -0.33641111850738525, -0.46575331687927246, -0.8436806797981262, 0.7723917961120605, -4.244075775146484, 2.925367832183838, -0.9563844799995422, 0.8644701838493347, -1.871848225593567, 0.5462245941162109, 0.3178308308124542, -0.26382943987846375, -0.892757773399353, -1.5398316383361816, 0.12212853133678436, -1.3451611995697021, -0.16862303018569946, 0.4628325402736664, -3.023449659347534, -1.0492656230926514, 1.2044607400894165, -3.1160073280334473, 0.13826313614845276, -1.048843264579773, 0.5122714042663574, -0.4022565484046936, 2.421151876449585, 0.16812121868133545, -0.5135593414306641, -0.881062924861908, 0.450305700302124, -3.15065336227417, -1.5827511548995972, -0.8649827241897583, 2.262392282485962, -0.47140538692474365, 1.436868667602539, 3.3637595176696777, 1.18290114402771, -1.215856671333313, 0.6785425543785095, 0.9141963720321655, -1.6355438232421875, -1.9555613994598389, 0.5353068709373474, 1.6088309288024902, -0.3677009344100952, -1.2059838771820068, 3.603437900543213, 1.2719645500183105, -0.9415306448936462, -0.8780157566070557, -0.8740187883377075, -0.7226091623306274, 0.5719699859619141, 0.6491708159446716, 0.36998316645622253, -2.853332281112671, -0.49393969774246216, 0.6062474250793457, -3.9894919395446777, -1.001329779624939, 1.3020347356796265, 0.5227633714675903, -0.5735512971878052, -3.8887240886688232, -0.5015939474105835, -3.5809192657470703, -0.2470095008611679, -0.7808097004890442, 1.1672637462615967, -2.540442943572998, -0.3026535212993622, 2.016288995742798, -3.4444971084594727, -0.29147428274154663, -0.18757909536361694, -2.405452251434326, -1.0719454288482666, -0.8835780024528503, -1.5480916500091553, -1.0062748193740845, 1.9124748706817627, 0.8080739974975586, -0.9357738494873047, -0.7201049327850342, 0.8423873782157898, -0.008391141891479492, -4.254336357116699, -0.3947238028049469, -0.7721646428108215, -1.3077232837677002, 0.2656484544277191, -1.051986813545227, -0.8658103942871094, 1.9640772342681885, -0.7488630414009094, -2.9654135704040527, -0.1856052279472351, 0.7395586967468262, -1.4039630889892578, -1.1304887533187866, 0.5152387022972107, -1.2321099042892456, 0.6425618529319763, -0.3084348440170288, -2.959108829498291, 0.3477726876735687, -1.0313366651535034, -1.2994928359985352, -2.1405768394470215, 1.0087292194366455, 0.3042784333229065, 0.48969754576683044, -0.9543178677558899, 1.82431960105896, -0.42109692096710205, -0.37507331371307373, 1.1942912340164185, -0.8862282037734985, -0.6285358667373657, -0.995258092880249, -1.2558923959732056, -1.6909306049346924, -0.9165710806846619, 2.409849166870117, -2.6020994186401367, 0.07574748992919922, 1.4796257019042969, 0.7508274912834167, -0.562255322933197, -3.4323408603668213, 0.7133997678756714, 1.0978999137878418, 0.1346050500869751, 0.6948272585868835, -0.856279730796814, -0.7552308440208435, -0.7177616357803345, -1.6170156002044678, 0.5243499279022217, -1.696675419807434, -2.0719077587127686, 0.6413407921791077, -6.5159077644348145, 0.11888101696968079, -2.8358569145202637, -0.15072551369667053, 0.6683512926101685, 1.1005645990371704, -0.6629462242126465, -5.178581237792969, -1.8588454723358154, -1.2894474267959595, 1.0438146591186523, 0.635892927646637, -0.3395143151283264, 0.42074131965637207, 0.1762007772922516, 3.262479305267334, 0.523818850517273, 0.6205027103424072, 0.48310160636901855, -0.528527557849884, 0.7776390314102173, -0.8616200089454651, 0.3312712609767914, 1.4399266242980957, -1.0398359298706055, 0.622978925704956, -1.5153741836547852, -0.8928930163383484, 0.25867366790771484, -0.7991605997085571, -0.817817747592926, -0.8184685707092285, 0.167703777551651, -0.2067316323518753, 0.6880570650100708, -2.174342155456543, 1.2487232685089111, -0.31887102127075195, -1.040595531463623, 0.3354337811470032, 0.7144469618797302, -2.2631964683532715, -0.6357932090759277, 1.929964303970337, 1.1192171573638916, -2.590221881866455, 1.3724517822265625, -0.8596450686454773, -0.17219388484954834, -0.0959048718214035, 0.47633326053619385, -0.2845146059989929, -1.0140044689178467, -1.0660916566848755, 0.974211573600769, 1.031653881072998, -1.4301342964172363, -1.381251335144043, -0.37392133474349976, -3.0166544914245605, 1.192983627319336, -1.9009040594100952, 1.2486319541931152, 1.94148588180542, -0.2716665267944336, 0.8319941759109497, -1.0156886577606201, -2.5141124725341797, -0.9476592540740967, -0.08840453624725342, 0.5473000407218933, -2.2947585582733154, -0.013585925102233887, -0.660183310508728, 0.061315715312957764, -3.3416006565093994, -0.14260202646255493, 0.5176255702972412, 1.184119462966919, -1.278398871421814, 0.08699938654899597, 0.1447984278202057, 1.0973360538482666, 1.0833138227462769, -0.753863513469696, -1.98817777633667, 1.3726203441619873, -1.2789599895477295, -1.124814748764038, 0.5772402286529541, -0.9774401783943176, -2.3553197383880615, 0.6226720809936523, -3.9577128887176514, -0.6108365058898926, 0.19188781082630157, -2.638977289199829, -0.8425580263137817, 0.7441891431808472, 1.033552646636963, -5.574705600738525, 1.2536354064941406, 0.5911336541175842, -2.258369207382202, 1.1921099424362183, -0.7656255960464478, -0.34398341178894043, -0.11514125764369965, -0.7301336526870728, -0.09671127796173096, 0.6614382266998291, -3.2509655952453613, 1.690146565437317, -0.10600578784942627, -2.989485502243042, -0.6756069660186768, 3.071211814880371, 2.704556465148926, -1.9637730121612549, -2.2879765033721924, -5.252303123474121, 1.3193906545639038, -1.2767070531845093, -0.15902866423130035, -0.5817676782608032, -3.3694064617156982, -1.245147705078125, 2.6990277767181396, -0.8229618668556213, -0.4384143352508545, -0.7846412658691406, -0.8422362208366394, -1.0217868089675903, 0.12154662609100342, 0.46379905939102173, 0.7158571481704712, 1.1727197170257568, -0.5244184732437134, -0.3658398985862732, -0.4288948178291321, -0.7180628776550293, -1.1506708860397339, -0.7488783597946167, -0.1945773959159851, 0.41892948746681213, -1.3085131645202637, 0.8475521802902222, -1.5689547061920166, -0.006003379821777344, 0.4901646673679352, 1.6958056688308716, -1.897442102432251, 1.257288932800293, 0.9660669565200806, -0.8980696797370911, 0.8007168769836426, -1.044358253479004, 2.277209520339966, -1.0565533638000488, -0.4664299488067627, -1.2568066120147705, 0.7830413579940796, -0.7715645432472229, 0.9324720501899719, -0.9549340605735779, -0.5626808404922485, -4.918803691864014, -2.0154294967651367, 0.41174837946891785, 0.3311450481414795, -0.5640780925750732, 0.06967215985059738, -4.187865734100342, -0.8278869390487671, 1.9393585920333862, -0.8602114319801331, -0.01529105007648468, -1.4045515060424805, -1.0472261905670166, -1.183565378189087, -2.545426607131958, -1.392102599143982, 0.550696611404419, -0.10640734434127808, -1.009121060371399, -0.1468537449836731, -0.8109447956085205, 0.02817496657371521, -0.2973673343658447, -1.6747472286224365, -0.6375072002410889, 0.26727747917175293, -1.038055419921875, -0.9790055751800537, -0.06259091198444366, -1.524171233177185, 1.114579677581787, 0.4374285042285919, 0.42404380440711975, 0.5902460813522339, -0.2170535922050476, -1.6201684474945068, 0.24227845668792725, -0.81181800365448, 1.9233442544937134, 1.5573947429656982, -0.7732111215591431, -0.24307072162628174, 0.535834550857544, -1.0332274436950684, -3.505392551422119, -0.3025486469268799, -0.37545865774154663};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
int stride_width=1;
int stride_height=1;
TfLiteFusedActivation activation=kTfLiteActNone;
int dilation_width_factor=1;
int dilation_height_factor=1;
const int filter_dims_size=4;
const int filter_dims_raw[4]={1,3,3,480};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={480};
TfLitePadding paddings=kTfLitePaddingSame;
TfLiteType filter_type=kTfLiteInt8;
TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;

struct OpData {
  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;
  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // Hybrid per channel temporary tensors.
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t input_offset_index;
};

void ExtractDepthConvParams(TfLitePadding padding, int stride_width, int stride_height,
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteDepthwiseConvParams* data_params) {
  // TfLiteDepthwiseConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetDepthConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params, char* tensor_data,
                       TfLiteAffineQuantization* quant_struct, size_t bytes_size,
                       TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}
void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to carry information from Prepare() to
  // Eval().
  return new OpData;
}

void Free(TfLiteContext* context, void* buffer) {
  delete reinterpret_cast<OpData*>(buffer);
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;

  // TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;
  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  const TfLiteTensor* bias = nullptr;

  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));

  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);
  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);
  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);
  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);

  const TfLiteType data_type = input->type;

  const TfLiteType filter_type = filter->type;
  const bool is_hybrid =
      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;
  TF_LITE_ENSURE(context,
                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||
                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);
  if (!is_hybrid) {
    TF_LITE_ENSURE(context,
                   filter->type == data_type || data_type == kTfLiteInt16);
  }

  if (data_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }

  // Filter in DepthwiseConv is expected to be [1, H, W, O].
  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));
    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (data_type == kTfLiteInt16) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);
    }
    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);
    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),
                      SizeOfDimension(bias, 0));
  }

  int channels_out = SizeOfDimension(filter, 3);
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int batches = SizeOfDimension(input, 0);

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (data_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }

  if (is_hybrid) {
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE_EQ(
        context, affine_quantization->scale->size,
        filter->dims->data[affine_quantization->quantized_dimension]);

    int temporaries_count = 0;
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;
    data->input_offset_index = temporaries_count;
    if (data->input_offset_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_offset_id));
    }
    ++temporaries_count;

    TfLiteIntArrayFree(node->temporaries);
    node->temporaries = TfLiteIntArrayCreate(temporaries_count);

    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    const int batch_size = SizeOfDimension(input, 0);
    int scaling_dims[1] = {batch_size};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    node->temporaries->data[data->input_offset_index] = data->input_offset_id;
    TfLiteTensor* input_offsets;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->input_offset_index,
                                       &input_offsets));
    input_offsets->type = kTfLiteInt32;
    input_offsets->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {
      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
      input_offsets_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                       input_offsets_size));
    }
  }

  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);
  outputSize->data[0] = batches;
  outputSize->data[1] = out_height;
  outputSize->data[2] = out_width;
  outputSize->data[3] = channels_out;
  return context->ResizeTensor(context, output, outputSize);
}

TfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    const TfLiteTensor* filter,
                                    int16* depth_multiplier) {
  int num_filter_channels = SizeOfDimension(filter, 3);
  int num_input_channels = SizeOfDimension(input, 3);
  TF_LITE_ENSURE(context, num_input_channels != 0);
  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);
  *depth_multiplier = num_filter_channels / num_input_channels;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
                       TfLiteDepthwiseConvParams* params, OpData* data,
                       const TfLiteTensor* input, const TfLiteTensor* filter,
                       const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output));
  } else {
    optimized_ops::DepthwiseConv<float, float>(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                           TfLiteDepthwiseConvParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  } else {
    optimized_ops::DepthwiseConv<uint8, int32>(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                                     TfLiteDepthwiseConvParams* params,
                                     OpData* data, const TfLiteTensor* input,
                                     const TfLiteTensor* filter,
                                     const TfLiteTensor* bias,
                                     TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));

  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output));
  } else {
    optimized_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

TfLiteStatus EvalQuantizedPerChannel16x8(
    const TfLiteDepthwiseConvParams* params, const OpData* data,
    const TfLiteTensor* input, const TfLiteTensor* filter,
    const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.weights_offset = 0;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier.data(),
      data->per_channel_output_shift.data(), GetTensorShape(input),
      GetTensorData<int16>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<std::int64_t>(bias), GetTensorShape(output),
      GetTensorData<int16>(output));

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteDepthwiseConvParams* params,
                                  OpData* data, const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* input_quantized;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &input_quantized));
  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;

  op_params.weights_offset = 0;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr);
  } else {
    optimized_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr, CpuBackendContext::GetFromContext(context));
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // const TfLiteTensor* bias =
  //     (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetDepthConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TFLITE_DCHECK_EQ(input_type, input->type);

  switch (input_type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteFloat32) {
        return EvalFloat<kernel_type>(context, node, params, data, input,
                                      filter, bias, output);
      } else if (filter->type == kTfLiteInt8) {
        return EvalHybridPerChannel<kernel_type>(context, node, params, data,
                                                 input, filter, bias, output);
      } else {
        TF_LITE_KERNEL_LOG(
            context, "Type %s with filter type %s not currently supported.",
            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));
        return kTfLiteError;
      }
      break;
    case kTfLiteUInt8:
      return EvalQuantized<kernel_type>(context, node, params, data, input,
                                        filter, bias, output);
      break;
    case kTfLiteInt8:
      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,
                                                  input, filter, bias, output);
      break;
    case kTfLiteInt16:
      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,
                                         output);
      break;
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));

  switch (input->type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

}  // namespace iwjphi

TfLiteRegistration* Register_iwjphi_REF() {
  static TfLiteRegistration r = {
      iwjphi::Init, iwjphi::Free, iwjphi::Prepare,
      iwjphi::Eval<iwjphi::kReference>};
  return &r;
}

TfLiteRegistration* Register_iwjphi_GENERIC_OPT() {
  static TfLiteRegistration r = {
      iwjphi::Init, iwjphi::Free, iwjphi::Prepare,
      iwjphi::Eval<iwjphi::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_iwjphi_NEON_OPT() {
  static TfLiteRegistration r = {
      iwjphi::Init, iwjphi::Free, iwjphi::Prepare,
      iwjphi::Eval<iwjphi::kNeonOptimized>};
  return &r;
}

TfLiteRegistration* Register_iwjphi_NEON_OPT_UINT8() {
  static TfLiteRegistration r = {
      iwjphi::Init, iwjphi::Free, iwjphi::Prepare,
      iwjphi::EvalImpl<iwjphi::kNeonOptimized, kTfLiteUInt8>};
  return &r;
}

TfLiteRegistration* Register_iwjphi() {
#ifdef USE_NEON
  return Register_iwjphi_NEON_OPT();
#else
  return Register_iwjphi_GENERIC_OPT();
#endif
}

// Warning: Clients using this variant are responsible for ensuring that their
// models only need the UINT8 type. TFLite's op registration mechanism doesn't
// yet allow for more nuanced registration mechanisms.
TfLiteRegistration* Register_iwjphi_UINT8() {
#ifdef USE_NEON
  return Register_iwjphi_NEON_OPT_UINT8();
#else
  return Register_iwjphi();
#endif
}

}  // namespace builtin
}  // namespace ops
}  // namespace tflite
