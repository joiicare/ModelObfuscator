/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h"

#include <stddef.h>
#include <stdint.h>
#include <vector>

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/optimized/cpu_check.h"
#include "tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h"
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h"
#include "tensorflow/lite/kernels/internal/optimized/neon_check.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/internal/types.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"

namespace tflite {
namespace ops {
namespace custom {
namespace lnnnsn {

constexpr int kInputTensor = 0;
constexpr int kFilterTensor = 1;
constexpr int kBiasTensor = 2;
constexpr int kOutputTensor = 0;

// This file has three implementation of DepthwiseConv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  kNeonOptimized,
};

const int kTensorNotAllocated = -1;

int8_t filter_r   aw[4320]={24, 4, -20, 6, 7, 24, -127, 105, 32, 14, -25, 26, 45, 0, -20, -4, 10, 17, 35, 34, -1, 10, -21, -19, -6, -26, 75, -5, -29, -3, 34, 35, -12, 69, 24, 1, 46, -28, 127, -116, -3, 20, 19, -8, 2, -8, -17, 2, 15, 14, 4, 9, -4, -19, 33, -9, 8, -3, 16, -66, 3, 31, -14, -18, 1, 15, 2, 5, 39, -44, 7, 4, 5, -5, 22, 37, -63, 45, 67, 12, 40, -2, -2, -17, 14, 50, 4, -6, 9, -12, -40, 36, -2, 10, 16, -11, 6, 0, -7, -1, -1, 8, -70, 25, 3, 54, 40, -9, 21, 34, -28, -15, 5, -7, 40, 7, 5, 6, 19, -1, 20, -74, 14, 28, -7, -20, 17, -6, 9, 28, 36, -4, 0, -26, -15, 6, -22, 0, 32, -18, -40, 0, 1, 6, 0, 86, 93, 55, 19, -87, 12, 14, -2, 11, 2, -28, -6, 6, 15, 17, -24, -10, 5, -30, -18, 26, 9, -1, 20, -4, 16, 14, -58, 16, 5, 7, -16, 7, -19, 2, 4, 29, -14, 10, -1, -27, 1, -12, -65, -16, 4, 3, 19, 7, 5, 14, 37, -12, 10, -7, 36, -1, 17, 8, 11, 72, -38, -11, 13, 2, -2, 25, 6, -24, 17, 6, 37, -3, 87, -2, -14, -27, -26, 18, -7, 62, 5, -12, 4, -53, 46, -16, 4, 16, 9, 28, 4, -3, 9, -19, 67, -15, 28, 24, -9, -10, -4, 13, 12, -56, 6, -14, -2, 16, -6, 11, -23, -9, -36, 2, -13, 10, -34, 96, 4, -2, -1, 18, 8, 17, 21, 14, 4, -6, -29, 17, -19, -4, -14, -2, 65, -8, -10, -9, 20, 11, -102, 23, 39, 14, 9, 12, -7, -14, -1, -1, 17, -30, 11, 3, -5, 8, 18, -1, -9, 12, 22, 6, 35, 23, 4, -33, 79, 45, 53, 7, -1, -6, -21, 17, -5, 5, -11, -22, 2, -1, 7, 0, -10, 15, 16, -63, 58, -38, 22, 21, 1, -3, 9, 46, 10, 79, -35, -18, -3, 21, -16, 125, 0, 0, 4, 27, -10, -17, 3, 0, 1, -2, 11, 11, -28, -1, -101, 14, 7, 56, 14, 8, 20, 12, 20, 14, -16, -15, 18, -37, 16, 0, -15, -10, 17, 24, 3, 20, 8, 10, 12, -9, -75, 13, -52, 17, 8, 12, -11, 15, -9, 16, -44, 30, -9, 4, -33, 1, -39, -1, -25, 5, -17, 4, 0, 44, -30, -13, -6, -30, -1, 24, -2, 1, 18, -74, 118, -23, -2, 48, 0, -4, 4, 6, 38, 42, 13, 3, 4, -73, 0, -42, 14, -4, -58, 5, 12, 0, 11, -1, 1, -4, -63, -16, 9, -38, 53, -5, 16, 14, 9, 21, 5, 50, 3, 10, -1, 5, 2, 7, -33, -3, 1, 2, -87, -2, -57, 13, -12, -51, -4, 10, 1, -35, 107, 31, -23, 2, 5, 36, -43, 61, 18, -127, -114, 48, 93, -3, -73, -2, 81, 17, 33, -71, -82, -2, 18, 33, 96, -42, 91, 6, -27, -2, 87, 59, -63, -12, 20, -35, 111, 45, 93, -95, 18, 56, 47, -17, 2, -18, -117, 10, 28, 71, 63, 4, -15, 15, 46, 50, -18, -49, 23, 19, -9, 99, 24, 28, -14, 83, 48, -18, -92, 54, -18, 19, 23, 3, -64, 106, -97, -101, 82, 26, 127, -6, 21, -40, 30, 1, 2, 14, 48, -9, -105, 66, -4, -13, 62, -5, 8, 0, -88, -9, 4, 16, -83, -29, 41, 83, 80, 47, 83, -72, -111, 2, -5, -13, -127, 8, 67, -110, 62, 5, 64, -58, 35, 34, -15, 118, 32, -36, 15, 77, 55, 47, 7, 42, -84, -29, -2, 9, 111, -51, -74, 29, 6, -25, -48, 20, 61, 22, 30, 26, 21, 38, 18, 38, 6, -23, 17, 18, -7, 34, -60, -9, -16, -45, -31, 75, 38, -111, 51, -111, 43, 14, 50, -32, -28, 13, -50, 79, 47, 30, 16, 127, -37, -3, 4, 56, -1, -45, -72, -32, 9, -18, 95, 15, -52, 127, 23, 32, -21, -2, 30, 9, 127, -2, 19, 8, -127, -113, 29, 29, 41, -46, -3, -29, 38, -127, 36, 1, 108, 57, 45, -31, 6, -42, -8, 33, -5, 13, 8, 29, 67, -31, 12, 4, 25, 123, 6, -28, -86, 60, 95, -29, -42, 33, -120, -41, 1, 18, -10, -127, 12, -3, 15, 5, -127, -19, -99, -122, 65, -67, -127, -13, -30, -101, 12, 1, 2, -14, 127, 33, 33, 30, -112, -14, 31, 17, -34, 40, 12, 0, 42, 38, -7, 108, 45, 15, -26, 56, 101, 8, 73, 42, 23, 115, 30, 5, -47, -15, 59, 63, -4, -8, 127, 5, 8, 5, -67, -4, 28, 45, 26, -5, 4, 95, 6, 15, -54, 18, -31, 42, -10, 20, 24, 127, -68, 38, 1, 5, -8, 40, 44, -41, 5, 64, 127, 70, -1, 8, 16, 0, -13, 93, -77, 1, 4, 10, 7, 89, -1, -8, 8, -10, 44, -57, 113, 7, -27, -95, 11, 23, -108, 6, -38, 14, 26, 98, 3, 22, -127, 101, 75, 7, -31, -15, 22, -93, -4, 5, -26, 39, 7, 65, 14, 61, 24, 21, 15, 61, -92, 33, -115, 19, 33, 11, 17, 22, 99, 66, -39, 19, -104, -6, -42, 32, 11, 5, -25, 9, -27, 42, 27, 20, -64, -127, 29, 21, 11, 38, -104, 3, 31, -109, 16, -42, 10, 65, 27, 23, 15, -3, 98, 77, 18, 27, 1, 8, 11, -108, 40, -8, 49, 23, 34, -43, 23, 11, 13, 12, -92, 7, 70, -56, 106, 35, 44, 21, -6, 110, 5, 112, 8, 23, 109, 49, 9, 21, -66, 1, 47, 48, -17, 25, -45, 45, 12, -98, -8, 31, 9, -62, 12, 3, -24, 5, 2, 22, 13, -24, -31, -53, -12, 29, -127, -3, -25, -2, 16, 20, -1, 32, 1, 8, 2, 75, 127, -23, -15, -3, -36, -3, 38, 29, -26, -16, 25, 1, 63, -27, 126, -119, 0, 20, -17, 75, 2, -8, -2, -3, 11, 14, 4, 11, -2, 60, 26, -17, -16, -10, 20, 36, 2, 31, -5, -6, 0, 11, 7, 6, -34, 27, 6, 3, 6, -4, 25, 41, -75, -20, 68, 12, 39, -1, -1, -17, 14, -84, 7, -5, 11, -14, -33, 36, -4, 4, 18, -3, 1, 0, -8, 0, 1, 9, -48, 25, 18, -7, 7, 102, 25, -102, 49, -19, -2, 4, 31, 8, 9, -64, 25, 0, 5, -56, 18, 25, -3, 95, 21, -9, 11, 43, 33, -6, 0, -12, -21, -1, -17, 1, 40, -23, -49, 4, -71, -20, -11, -42, 90, 36, 20, -7, 10, 11, -3, 13, -1, 13, -10, 4, -25, 19, -14, -13, 5, -27, -26, -2, 12, -31, 15, -28, 22, 12, -71, -40, 20, 6, -15, 2, 32, 7, 10, 28, -16, 5, -2, -21, 2, -13, 18, -16, 2, 7, 21, 5, 9, 47, 37, -17, 7, -10, 32, -1, 12, 8, 13, -31, -9, -14, 10, 2, 3, -24, 4, -29, 13, 7, -14, 1, -55, -28, -9, -26, 7, -4, -5, 59, 5, -14, 7, -36, 37, -18, 3, 17, 7, 41, 2, -1, 14, -61, 57, -21, -2, 25, -1, -11, -3, 7, 12, -3, 3, -16, 5, 13, -14, -39, 36, -6, -45, -64, -34, -12, 3, -77, 4, -5, -1, -127, 7, 14, 18, 13, -92, -4, 85, 14, -17, -6, -9, -1, -33, 8, 8, 25, 25, 13, 127, 29, 31, 12, 11, -65, -13, 13, -1, -1, -62, -35, 15, 4, -2, -17, 14, 1, -9, 10, 23, 4, 16, -9, -9, 60, -53, 50, 7, 7, 8, -5, -18, 9, -4, 5, -34, -50, 40, -3, -1, 3, 57, 17, 17, 11, -66, -52, 22, -51, 2, 0, 5, -74, -10, -4, -36, -24, -3, 26, 36, -8, 0, -2, -2, -30, -9, -13, -15, 3, 0, -119, 11, 17, 7, 2, 3, 17, 10, 1, 13, 8, -98, 10, -25, 15, -9, -21, 18, -38, 8, 2, -14, -8, 18, 28, -1, 16, 13, 8, 10, 38, 11, 12, -47, 11, 6, 1, 35, 12, -47, 19, -34, -50, 1, 7, -27, 3, 16, 4, -17, 2, -24, -2, -6, 45, -27, -45, 11, 40, 4, -34, 45, -1, 26, -63, 127, -27, -4, 38, -1, -1, 11, 2, -22, -39, 9, 11, 2, 99, 1, -27, 16, -3, -21, 2, 13, -93, 3, -4, 1, -1, 11, -9, 18, -29, 53, -5, 12, 14, 14, 20, 6, 53, 5, 7, 42, 3, 2, 10, -35, -5, 4, 17, 58, 2, -47, 12, -14, -52, -4, 12, 5, 43, 119, 34, -24, 5, 7, 60, 32, 120, -3, 95, -124, 63, 68, 2, -74, 6, 42, 37, -32, 57, 127, 1, -127, -19, -15, -42, 115, -3, -32, -7, 71, 78, -71, 0, 68, -39, 127, -10, 91, -88, 21, 106, 53, 3, 1, -18, -127, 20, 27, 22, -11, 18, -14, 127, 127, 46, 108, 106, 21, 4, -3, 90, 41, -72, -10, -103, -36, 61, -127, -126, 106, 14, 21, -4, -123, 51, -121, 35, -95, 27, 79, -7, 17, -44, 23, 14, -4, 6, 47, -4, -107, 49, -6, 15, -43, -71, 8, -4, 13, -11, 5, 23, -106, 51, 47, 73, 90, -112, 5, 65, 115, 25, 51, 8, -100, 9, 69, 109, 39, 0, 53, -64, 27, 45, -14, -103, 21, -8, 13, 85, 55, 6, -2, 33, -82, -57, 125, 3, 127, -51, -70, 29, 79, 91, -24, -94, 75, 127, 13, -121, 4, 23, 17, 34, 0, -127, 59, 19, 112, 39, -71, -19, -19, 34, -27, 45, 33, 92, 37, 100, 45, 21, 29, 56, -82, 26, -55, -50, 23, 23, 9, -34, -5, 46, 2, -28, 1, -56, 127, -32, -45, 72, -1, 9, 49, 118, 18, -87, 61, -8, 47, 5, 105, 2, 25, 41, -13, 109, 27, 21, 21, 127, 0, 4, -11, 93, 40, -1, 102, 65, 40, -38, -39, -15, -17, 123, -8, -33, 18, 44, 79, -42, 13, 1, 20, -16, 0, -29, -120, 64, -66, -28, 127, 28, -127, -48, 5, 12, 53, -124, 10, 14, 3, 32, -86, -28, 127, -111, 54, 44, 113, 127, -73, 80, 9, 1, -8, 2, -13, 24, 15, 16, 110, -8, 127, 17, -33, 48, 5, 4, -73, 29, -8, -127, 52, 3, 20, -57, 104, 21, 82, 52, 28, -127, 23, 10, -3, -28, 65, 22, -5, 127, -13, 6, -1, 3, -52, 1, 46, 14, 36, 2, -1, 30, 48, 12, 56, -19, -22, 23, -6, 24, 28, 92, -15, 13, 0, -10, 60, -25, -50, -23, 1, 40, -25, 80, 1, 11, 28, -1, 127, 127, -106, -37, 2, 23, 44, 30, 6, -10, 6, 26, 2, -53, -57, 8, -21, -77, 15, -8, -110, 6, -78, 7, 21, 106, 1, 23, 112, 109, 127, 6, -12, -14, 18, -100, 43, -58, -30, -21, 19, 53, 11, 8, 47, 20, 8, 23, -127, 32, -111, 19, 22, 76, -36, 26, 127, 61, -19, -12, -127, 20, 79, 35, -5, 9, -17, 3, -28, 11, 27, 36, -74, 1, 25, -74, 9, 45, -47, 9, 24, -100, -5, -46, 8, 56, 31, 20, 8, 15, 126, -72, 25, 26, -1, 4, 9, -85, 33, -12, 23, 20, 21, 78, 14, 2, 12, 8, 91, -2, -2, 24, 76, 37, 42, 9, 21, 8, -7, 80, 35, 26, -117, 44, 10, 21, -105, 0, 35, 62, -24, 25, -107, 43, 17, -105, -8, 36, 10, -5, 37, 127, 127, -127, 127, 127, 104, 91, 127, 54, 46, 127, 108, 127, -127, 127, 127, 127, -127, -127, -24, -127, 114, -127, 88, 127, -42, 127, 127, 127, 127, 127, -127, -127, 127, -127, -67, 127, 82, -98, 127, 127, 127, -127, 127, 127, -119, 127, 127, 127, -127, 127, 127, 90, 8, 127, -127, -91, 127, 127, -127, -127, 127, -127, -127, 78, -127, -127, -41, 116, -127, 127, -127, 127, -102, 53, -115, -127, -127, 127, 98, -127, 127, -127, 127, -127, -127, 127, 127, 127, -127, -127, 127, -127, -127, 127, 127, 127, 127, -127, 127, 127, -127, -127, 127, 15, 26, 124, 127, 127, -81, 127, -127, 127, -46, 127, 127, 18, -127, 127, -127, -127, 127, 127, -127, -127, 127, -127, 127, 127, 127, -127, 127, 127, -127, 127, -127, 127, -28, -127, 127, 127, 127, -127, -117, 127, 127, 47, -127, 127, 127, 127, 127, -127, 127, -119, 127, 127, -127, 127, -127, -127, -127, -64, 127, 127, -127, 33, -127, -41, 127, -127, 127, -127, -127, 127, -127, -127, -80, 127, 127, -55, -119, 127, 127, 127, 127, -127, -59, 127, 127, -127, -127, 127, -127, -9, 127, 127, -127, 127, 127, -127, -4, 127, 127, -31, 81, -56, 127, 127, -127, -97, -127, -127, -127, 56, 127, 127, -127, 127, 127, -127, -127, 127, -127, -17, 127, -80, 127, 127, 127, -127, 127, -127, 127, -127, 127, -127, -30, 127, -127, 127, -93, 127, -24, -127, -127, 127, -127, 11, 127, 127, 127, -127, -67, 127, 92, -95, 127, 127, 2, -44, 127, 127, 127, 127, -127, 112, -92, -127, -127, 127, 75, -127, 106, -127, 127, -127, 127, 127, -127, 119, -127, -14, 127, -127, -37, -127, 90, 127, -127, 127, 127, 26, 127, -127, -65, -127, -127, 127, 127, -104, -89, 127, 127, 127, 127, 127, 127, 127, 127, -127, -127, -127, 127, 127, 50, -127, -127, 67, 127, 127, 127, 98, -115, 127, 127, 127, 127, -127, -127, 127, -127, 127, -114, -127, 127, 127, 127, 127, -80, -50, -127, -108, 127, -127, 127, 108, -127, -127, 127, 127, 127, -127, -82, 127, -127, -127, 127, 127, 127, 127, 127, 127, -127, -26, -127, 127, 95, 33, -72, 127, 127, 127, 127, 110, -127, 127, -127, 127, -127, 127, 127, -127, 127, 127, -127, 127, 62, 127, 127, 127, -127, 127, -127, -127, 65, 127, -127, 127, 39, 127, 127, -127, -127, 127, 127, 127, -127, 127, 127, 127, -127, 104, 127, -68, 127, 127, 116, 127, 127, 92, 69, -127, 127, 127, 127, 127, 127, 127, -15, 127, -127, 127, 127, -127, 127, -127, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, 80, 127, -127, -60, -127, 127, 127, -127, 127, -127, 127, 127, 127, 127, -30, 127, -127, 127, 127, 127, 127, 127, 127, 127, -127, -127, 127, 18, -127, 127, 127, -97, -118, 31, -30, 10, 6, 51, 21, -127, 10, -125, -123, 60, 92, -1, -72, 5, 51, 42, -10, 68, -15, 1, 76, -82, 32, -44, -127, -4, -29, -11, 74, 90, -57, 11, 52, -50, 124, 40, 114, -102, 19, 99, 54, 4, 1, -18, 86, 20, 33, 22, -5, 14, -14, -100, 118, 48, 14, 127, 27, -10, -4, 93, 35, -67, -10, 98, -36, 53, 100, 31, 105, 16, 20, -3, -121, 61, -127, 33, -94, 25, 87, -9, 16, -50, 27, 17, 2, 15, 46, -1, -103, 40, -6, 18, -48, -65, 4, -5, 6, -11, 17, 19, -124, 55, 57, -127, -127, 127, -18, -79, -127, 18, 51, 19, -113, 12, 63, -127, 25, 3, 8, -49, 29, 46, -16, 109, 29, 0, 14, 77, 52, 8, 0, 35, -76, -58, -4, 5, 127, -52, -64, 28, -101, -38, -10, 24, 83, 125, 14, 111, 5, 30, 19, 41, 3, 29, 51, 12, -10, 40, -61, -14, -15, 22, -34, 71, 37, -127, 37, -127, 44, 17, 60, -119, 108, 23, -50, -40, 127, 15, 13, -35, -5, 48, -4, -38, 3, -58, 3, -28, -68, 75, 7, 14, 64, 120, 12, -83, 67, -9, 43, 4, 99, 5, 33, 127, 35, -127, 34, 17, 25, -11, -4, 4, -19, 94, 45, -2, -104, -50, 36, -36, 113, -26, -17, 127, -11, -28, 16, -29, 71, -37, 17, 4, 19, -72, 3, -26, -119, 69, -76, -22, -28, 30, 110, -38, -1, 19, 55, 118, 10, 11, -4, 35, -60, -5, 102, -115, 52, 2, -115, -88, -22, 98, 9, 2, -9, 6, -11, 24, 7, 18, -127, -6, 7, 23, -31, 48, 1, 6, -68, 30, 99, 98, 50, 4, 13, -64, 127, 26, 84, -24, 29, -125, 22, 7, 127, -30, 53, 16, -1, -3, -8, 6, -1, -3, -50, 5, 99, 10, 26, -8, 5, 33, 39, 13, 62, -16, -18, 11, -10, 24, 33, 111, 127, 14, -3, -10, 61, -33, -49, 2, 16, 48, -26, -23, 1, 15, 24, -9, 3, -126, -110, -18, 2, 26, 25, -7, 5, -10, 6, 40, 1, -55, -51, 3, -24, -94, 17, -10, 43, 3, 75, 15, 21, -127, 2, 26, -100, -127, -55, 3, -2, -17, 21, -105, 37, -57, -36, -22, 17, 57, 15, 8, 56, 16, 8, 21, 95, 32, -98, 34, 23, 68, 101, 25, 123, 62, -29, 3, 97, 17, 69, 32, 87, 4, -11, 8, -23, 9, 27, 45, -68, 62, 8, 127, 8, -70, -13, 7, 26, -127, -5, -43, 11, 40, 31, 27, 6, 15, -127, -80, 23, 25, 1, -1, 9, -85, 37, -17, 50, 10, 35, -66, 21, 1, 18, 5, -127, 3, -2, 33, 77, 34, 39, 2, 18, -4, -7, 106, 24, 28, 109, 42, 9, 20, -97, -1, 43, 56, 23, 19, -108, 42, 24, -105, -4, 31, 7, 127, 7, -20, -23, 6, 1, 22, 1, -3, -8, 36, -9, 15, 22, -6, -21, -3, -1, 34, 3, 29, 30, 4, -24, 13, -103, -23, -5, 0, -33, 0, 40, 35, -16, -10, 27, -10, 77, 32, 97, -127, -2, 11, -39, 91, 0, -14, 3, 0, 19, 19, 12, 13, -1, -101, 40, 1, -10, -18, 15, 38, 5, 43, -10, 2, -2, -73, 11, 7, -15, 3, 6, 4, 5, 7, 60, 13, -78, -29, 5, 12, 49, -1, 0, -21, 24, -87, 6, 4, 9, -16, -39, 31, -2, 44, 18, -4, 7, 0, 3, -4, -11, 8, -83, 17, 17, -12, -1, -115, 23, -42, 44, -19, -1, 3, 20, 9, 11, 39, 18, 2, -3, -54, 14, 23, -7, -18, 30, 18, 12, 42, 17, 3, 1, -16, -12, 20, 13, 1, 40, -29, -52, 6, 49, 16, 27, -21, 102, -18, 17, -19, -19, 14, 0, 14, 2, -8, 3, 4, 20, 16, -16, -17, 2, 24, -19, 13, 15, 27, 24, 59, 28, 10, 41, 35, -1, 5, -21, 16, -22, -3, 14, -8, 0, 7, -1, -29, 2, -22, -10, -18, 15, 5, -13, 20, 35, 13, 31, -19, 12, -6, 30, -2, -31, 5, 11, 35, 8, 83, 32, 3, 13, 25, 3, 36, 10, -8, -4, -5, -64, -60, -1, -32, -32, -86, -2, 30, 7, 15, 2, -13, 68, -17, 0, 19, 6, -37, 2, -7, -15, -64, -9, -15, 16, 11, 1, -20, -2, 28, 23, -1, -1, -4, 4, 12, -22, -44, 9, -20, -32, 36, 42, 18, 14, -27, 6, 2, 1, 15, 3, 16, 14, 14, 14, -3, 13, 14, -18, -19, -14, 1, -46, -127, -7, -24, 29, 11, 118, 30, 39, 14, 14, -58, -21, -8, 12, 0, -11, -27, 20, -9, -3, 20, -18, -1, 3, 13, 10, 2, 25, 5, 0, 70, -69, 9, -30, 6, -38, 21, -19, -24, -17, 3, -27, 17, -13, 0, 1, 1, 49, 14, 33, 9, -63, -49, -3, -64, -3, -7, 13, -79, -62, -2, -9, 12, -6, 31, 21, 12, -2, -2, -4, -15, 2, -15, -5, -4, 1, -89, 10, 11, -30, -3, -3, 23, 10, -7, 6, 6, -12, 3, 5, 19, -8, -18, 17, -37, 13, 5, -18, -22, 19, 16, 4, 18, 19, 6, 7, -71, 13, 22, -42, 6, 6, 18, -18, 14, -80, 3, -16, -58, -2, 4, -57, 0, 6, 5, -11, 0, -16, -1, 10, 47, -27, 5, 16, -9, 2, -42, -127, -1, 29, -75, -47, -24, 1, 36, 4, 4, 10, 4, -3, -13, 11, 1, 2, 87, 2, -64, 19, -1, 29, -6, 20, 116, 0, -2, 0, -2, -11, -9, -3, 26, 60, 0, 7, 9, 7, -6, 8, 62, 38, 13, -48, -3, 1, 4, -24, -7, 2, 9, 57, 4, -52, 19, -9, -63, -14, 8, 9, 9, -127, 47, -24, 7, 1, 67, 30, -11, -17, 94, -127, 63, 53, 13, -71, 13, 60, 27, 59, -74, 68, 2, 5, 57, -73, -41, -103, 2, -30, 2, 80, 9, -70, -4, 12, -27, 113, 16, 90, -94, 15, 45, 67, -15, 4, -10, 118, 5, 21, 82, 63, 21, -2, -30, 79, 37, -6, -45, 25, 9, 1, 90, 28, 21, -10, -127, 38, -1, 100, -127, -22, 14, 18, -7, 127, 127, -112, -81, 12, 27, 120, -10, 6, -51, 39, 0, -6, 4, 53, -7, -88, -5, -6, 49, 55, -19, 10, -4, 34, -12, 19, 15, -113, -24, 35, -93, -106, -4, 65, 62, 108, 2, 12, -4, -79, 11, 58, 99, -3, 7, -17, -85, 40, 48, -11, -97, 10, 126, 10, 87, 46, 63, -1, 56, -88, -38, -50, 1, 114, -67, -66, 31, 38, -13, 127, -63, 57, 103, 26, 26, -7, 29, 20, 34, 11, 21, 23, 23, -19, 40, -71, -12, -18, 127, -35, 62, 30, 89, 41, 92, 67, 16, 57, 36, -12, 36, -44, 86, -64, 18, 5, 9, 127, 58, 2, 65, -6, -61, -17, -27, 2, -13, -18, 8, 91, 119, 34, 44, -37, -5, 45, 9, -115, 7, 32, 70, 48, 92, 31, 21, 44, -51, 2, 58, 66, -102, 36, 0, -103, 11, 29, -36, -3, -2, -29, 89, -11, 127, 19, -10, 91, -27, 19, -6, 13, 8, 2, -24, -127, 62, -14, -25, -50, 28, 101, -24, 0, -2, -25, 94, 7, 8, 3, 2, -43, -37, -103, -127, 53, 57, 92, -6, 0, -62, 8, 8, -9, 12, -36, 32, 28, 25, 96, -8, 18, 25, -36, 58, 0, 0, 14, -96, -20, -119, 47, -1, 16, 77, 88, 20, 67, -14, 42, 121, 32, 6, -15, -24, 57, -86, 5, -26, -45, 6, 11, -3, -47, -6, 21, -34, 33, 4, 10, -7, 34, 9, 127, 82, -27, -127, 8, 22, 58, 89, -43, 38, 3, 13, 10, 44, 75, 12, 17, 58, -20, -6, 1, 13, 17, -16, -68, -125, -103, 127, -2, 25, 47, -65, 4, -14, 9, 47, 41, -69, 127, 5, -25, -101, 17, 25, 57, 12, 41, 21, 33, -106, 9, 19, 92, -121, -63, 1, -34, -2, 8, -127, -13, 1, -31, -43, 17, 71, 18, 59, 41, 21, 18, -21, 77, 31, -95, 27, 27, 41, -19, 26, 77, 38, -37, 11, 84, 1, -57, 24, 49, 3, 2, 7, -24, 34, 31, 32, -61, -5, 31, -34, 3, -16, 127, 9, 30, -105, -57, -42, -3, 73, 21, 25, 16, 10, -96, 79, 12, 28, 2, 15, 12, -71, 30, -17, 17, 16, 14, 57, 26, -5, 11, 9, 39, -6, -3, 127, 76, 7, 21, 97, -9, -57, 6, 76, 21, 37, -127, 25, 7, 23, -81, -6, 35, 64, 14, 30, -54, 46, 20, -127, -5, 33, 10, -71, -93, -10, -21, 7, 5, 41, 8, -118, -52, -13, -18, 4, 44, -3, -19, 3, 3, 36, 25, 27, -47, 6, 8, -30, 11, -23, -38, 0, -27, -1, 38, 33, -24, 77, 20, -6, 64, -38, 123, -126, 3, 19, 13, -4, 1, -20, 122, -3, 17, 18, 11, 11, -5, -30, 35, -5, -4, 13, 14, -75, 5, 40, -12, -6, -1, 5, 10, 9, 26, 9, 6, 4, 5, 11, 70, 20, -61, 42, 4, 7, 37, 0, 1, -18, 18, 60, 4, -1, 15, -14, -42, 28, -1, 42, 22, -10, 3, 1, 3, 0, -6, 10, -81, 20, 6, -36, -42, -5, 26, 42, -24, -18, -1, 6, 18, 10, 12, -1, 9, 1, -23, -47, 16, 29, -12, -26, 32, 27, 10, 43, 22, 3, 1, -10, -8, 21, -4, 1, 43, -20, -54, 10, -9, -18, 29, 69, 80, -7, 20, 72, -11, 14, 3, 12, 1, 30, 1, 2, -22, 16, -14, -23, 2, 23, -15, 21, 11, -6, 21, -4, 30, 12, -40, -36, 32, 4, -13, 17, 44, -8, 12, -7, 1, 10, 0, -16, 0, -29, -17, -17, -20, 10, -13, 15, 26, -4, 38, -10, 19, -9, 31, -2, -11, 6, 9, 25, 17, -2, 21, 8, -5, 13, 5, 35, 20, -12, 35, -4, 56, 40, -3, -31, 27, 11, -4, 19, 9, 12, 3, 54, 61, -22, 3, 12, 6, 8, 3, -6, -21, -41, -11, -23, -6, 15, 48, -18, -2, 22, 22, 48, 1, -3, 1, 14, -26, 7, 2, -21, -15, -7, -5, -8, -20, 44, 3, 0, -5, 6, 2, 14, 13, 11, 2, -7, 7, 12, -23, -16, -17, 0, 41, 14, 6, -10, 28, 9, -112, 34, 37, 11, 10, 69, -19, -23, 10, -1, -1, -24, 15, -12, -4, -28, -17, -2, 0, 17, 1, 2, 7, -77, 5, -43, 87, 9, 61, 6, -52, 23, -21, -28, -10, 5, -23, -60, 28, -2, 4, -1, -26, 13, 29, -30, 53, -37, -1, 2, 0, -2, 10, 54, 7, -34, -10, 4, -6, 30, -44, -127, 2, -1, -1, -31, 5, -9, 7, -1, 5, -10, 5, 7, 39, -3, 109, 17, 5, -29, 5, 8, 7, -49, -21, 19, -25, -17, 17, -43, 12, 2, -21, -14, 18, 17, 3, 23, 22, 9, 9, -6, 14, 18, -44, -1, 8, 28, 35, 13, -22, 13, -25, 35, 6, 5, -66, -1, 28, 2, 1, 2, -17, -2, 6, 37, -31, 22, 4, 4, 3, 64, 17, -2, 34, -63, -71, -33, -1, 40, 4, -2, 6, 7, -38, 39, 10, 6, 3, -80, 0, -59, 17, -5, -29, 2, 20, -10, 7, -3, -1, 2, 26, -16, -5, 21, 71, 5, 11, 14, 4, -18, 6, 36, 39, 12, 17, 3, 1, 0, -20, -6, 11, 12, -95, 8, -29, 16, -10, -72, -5, 15, 7, 34};

float bias_raw[480]={-0.9024484157562256, 1.1209183931350708, 1.2506855726242065, 1.3498855829238892, -0.4645673334598541, -0.3564493656158447, -0.29967623949050903, 0.436200886964798, -0.6952002644538879, -1.1647865772247314, 2.0722739696502686, 1.048959732055664, 0.19406628608703613, -0.04707622528076172, 0.9918310642242432, -0.7405598759651184, 1.8514420986175537, 0.481931209564209, -0.42397260665893555, -0.8960617184638977, -2.4632670879364014, 1.0047136545181274, 0.6429134011268616, 0.35956498980522156, 0.4739122986793518, 1.5318050384521484, -0.6845850348472595, -0.6107519865036011, -0.2972002625465393, -0.9662070870399475, 1.1236696243286133, -3.430455207824707, -0.5261720418930054, -0.08096527308225632, -0.12829852104187012, 2.070230484008789, 1.1806656122207642, 1.7548985481262207, 0.18786096572875977, 1.757535457611084, 1.146216630935669, -2.4789059162139893, 2.067563056945801, -0.036096908152103424, -0.6312335729598999, 0.8509823083877563, -0.7318407297134399, -2.690904140472412, -0.8776940107345581, -0.02158522605895996, -1.0515449047088623, -1.2753386497497559, -0.4261322617530823, -2.096855401992798, 1.2963581085205078, 0.6646416783332825, 0.43309998512268066, 0.3060987889766693, -3.5246200561523438, -0.7235032916069031, 1.2875518798828125, 0.01644045114517212, -1.1158671379089355, 4.920371055603027, 2.087632179260254, -1.0134766101837158, 0.16663914918899536, -0.10268603265285492, -0.9103877544403076, -0.9903836250305176, 0.8248217701911926, 0.3426443636417389, 0.9867057800292969, -0.5553662180900574, -0.00637364387512207, 0.5491209030151367, 0.7802427411079407, 0.8949573040008545, -0.8833094835281372, -4.112155914306641, -1.0019302368164062, 3.3017730712890625, -1.4984443187713623, 1.2134709358215332, -2.4774975776672363, -1.0050561428070068, 2.384645700454712, -0.08570757508277893, 0.6893067359924316, 0.5162943005561829, 2.1316866874694824, -0.3185366988182068, -0.7272175550460815, -0.890265941619873, -0.3377087414264679, 0.3179573714733124, -0.7723865509033203, -0.7957069873809814, 0.7718769311904907, 4.860105991363525, -0.8355910778045654, -0.7206519246101379, 1.0803126096725464, -0.5545995831489563, -4.311656951904297, -0.7011821269989014, -0.9644724726676941, -0.14781706035137177, -1.906717300415039, -1.1783479452133179, -0.7714987397193909, 1.03065824508667, 0.04974953830242157, -0.10252910852432251, 0.8987287878990173, 1.1038484573364258, -3.9769787788391113, -0.7872269153594971, -1.9735968112945557, -0.741443395614624, -1.5480659008026123, 0.7024162411689758, 1.2679351568222046, -1.5822277069091797, 4.345407009124756, -0.8014687299728394, -1.972658634185791, -1.0686414241790771, -1.4390045404434204, 1.4418517351150513, -1.86928129196167, -0.37538790702819824, -0.9010009765625, -2.1248817443847656, 0.7533573508262634, -0.6077081561088562, 0.7448794841766357, -1.640349268913269, -2.5395848751068115, 1.1195690631866455, 1.4896620512008667, 0.6752720475196838, -0.3081773817539215, -0.00648343563079834, -1.0038955211639404, -3.355264902114868, 0.1099010705947876, 0.5485949516296387, -0.5551808476448059, 0.044142432510852814, -0.3156258165836334, 0.5714839696884155, -2.897876262664795, 0.6978845596313477, -1.0898706912994385, 0.40459221601486206, -0.2755405902862549, -3.305929660797119, -1.2689642906188965, -2.3963520526885986, 1.095034122467041, 1.3864059448242188, 1.95286226272583, -0.13374167680740356, -0.8741992115974426, 0.10601690411567688, 0.3860478103160858, -1.1738382577896118, 0.8253085613250732, -0.9070626497268677, 2.497100830078125, -0.011448979377746582, 0.4451628625392914, 0.3260016441345215, 0.757376492023468, 1.183976411819458, 0.8907115459442139, -0.8147510886192322, -2.9059858322143555, -1.348751425743103, 0.15566599369049072, -1.4572006464004517, -0.030143529176712036, 0.5895891189575195, -0.8260598182678223, -5.000491142272949, -0.010064661502838135, 0.8356573581695557, -0.34929436445236206, 0.8617435693740845, -1.8602632284164429, 0.28463611006736755, -1.3266472816467285, -1.113494634628296, -0.14263474941253662, 0.8813401460647583, -3.3635928630828857, 1.0164673328399658, -0.8065524697303772, 0.007596105337142944, -2.035336971282959, 1.3634850978851318, 0.5629655122756958, -0.5871492028236389, -2.0301761627197266, 0.10905711352825165, -1.5217407941818237, -0.94868403673172, -3.295724868774414, -1.1135261058807373, -1.0432053804397583, -0.6328905820846558, 1.4607778787612915, -2.489879608154297, -0.937129557132721, 0.9792169332504272, -3.6080222129821777, -0.6166785359382629, -0.697360634803772, 0.5306457281112671, 2.601933479309082, -0.6753511428833008, -1.2127609252929688, -1.15203857421875, -0.47679197788238525, 1.0453760623931885, -0.42710256576538086, -0.983879029750824, -0.726547122001648, 0.047982070595026016, 0.8965902328491211, 1.4116146564483643, 0.4724879562854767, 0.9822636842727661, 1.1725492477416992, -1.1653672456741333, -1.0672719478607178, 2.317078113555908, 0.5873076319694519, 2.3655500411987305, -0.8889583349227905, 0.06551675498485565, -1.179133415222168, 2.530211925506592, -0.9847678542137146, 1.278393268585205, 0.9562749862670898, -4.019354820251465, -0.6701054573059082, -1.0149449110031128, -1.094310998916626, -0.08696508407592773, -0.536358118057251, -0.4551399350166321, -0.49273860454559326, -1.4619481563568115, -3.2607064247131348, 1.300719141960144, 1.7385646104812622, -1.1045578718185425, -0.9278934597969055, 0.6855227947235107, -1.1188762187957764, -3.1511454582214355, -1.8273611068725586, -0.8957599401473999, 2.4555041790008545, -0.3088987469673157, -0.4546077847480774, -0.6462095975875854, -0.27171701192855835, -3.767659902572632, -0.8907773494720459, 4.059847831726074, 0.6229900121688843, -0.2014932781457901, -0.2247251272201538, -0.6962382197380066, -0.4818563461303711, -0.03200129419565201, 0.24866914749145508, -0.6672275066375732, 0.6012241244316101, -0.7037525773048401, -2.568255662918091, 0.2610016465187073, 0.654193103313446, -0.3914697766304016, -0.1991479992866516, 0.4868246912956238, 1.4163806438446045, 2.057770013809204, 2.196288824081421, -0.2128753960132599, -3.1363563537597656, 1.6680612564086914, -0.25722837448120117, 0.8560569882392883, 0.22885750234127045, 0.15667249262332916, -0.297681987285614, -0.9970860481262207, -1.1376631259918213, 0.5801178812980652, -1.0200421810150146, -1.5544209480285645, -0.10283955931663513, -0.7505346536636353, -0.06919336318969727, -1.2037372589111328, -1.9376877546310425, -0.37771350145339966, 0.44931501150131226, 0.32732000946998596, -3.8158488273620605, -1.2942314147949219, -0.8196648955345154, -0.6102513074874878, 2.466282367706299, -1.4469517469406128, -2.0474040508270264, -0.2070084810256958, 1.8685342073440552, 0.849198579788208, -0.7391254901885986, -2.194692373275757, -0.7896865606307983, 0.40165045857429504, -4.149015426635742, 0.0723276138305664, -0.8142145872116089, -1.6032240390777588, -1.3419678211212158, 0.7825645208358765, -0.6931147575378418, -1.8052051067352295, -1.1107304096221924, 0.6481927633285522, 0.006690561771392822, -2.005078077316284, -0.5501932501792908, -1.2400593757629395, 1.1954305171966553, -0.7296953201293945, 0.0947955846786499, -0.10247379541397095, 2.188971519470215, 0.04276743531227112, 1.956972599029541, 3.710404634475708, -0.8353173732757568, 0.5210545063018799, -0.30681031942367554, 1.0616281032562256, 0.7485066056251526, -2.4966912269592285, 3.4085257053375244, 0.34248173236846924, -2.2464652061462402, -1.990017294883728, -1.6864985227584839, 0.875453770160675, 0.2005854845046997, -3.7035231590270996, 0.04180293530225754, -0.7470000982284546, 0.4898717403411865, -1.2370802164077759, -0.960895299911499, -0.8608709573745728, -1.0457128286361694, 1.0449784994125366, 0.6222130656242371, -0.719616711139679, -0.44043517112731934, 0.13121485710144043, -0.6800074577331543, 0.6385152339935303, 1.6800047159194946, -1.4103069305419922, -0.8121657371520996, -0.04884982109069824, -0.20816802978515625, -0.7700849771499634, -1.7040209770202637, -0.271992564201355, 0.5690447092056274, -0.709628701210022, -0.8919637799263, -3.475052833557129, 1.6034575700759888, 1.4228912591934204, -0.9535403251647949, -3.921875238418579, -1.7849630117416382, -0.3221161961555481, 1.761441946029663, -1.5650712251663208, 0.28325706720352173, -1.1641335487365723, -1.0808223485946655, 0.782949686050415, 0.16092002391815186, -0.22082780301570892, -1.9571399688720703, -0.10860949754714966, -0.8128772377967834, 1.1956902742385864, -0.8070396184921265, -0.5445460677146912, 0.8327269554138184, 1.1883273124694824, 1.0496160984039307, -1.2729275226593018, -2.3327221870422363, -0.9373703002929688, -1.2076389789581299, 0.7704485654830933, -2.1571738719940186, -0.5701937079429626, -2.4533121585845947, 1.3036911487579346, 0.08533286303281784, -0.2880508303642273, -0.41917145252227783, 2.398155927658081, -2.579972505569458, 0.29780346155166626, -1.3287689685821533, 0.9258279800415039, -0.8386682271957397, -3.1060681343078613, 0.5578666925430298, 0.05354297161102295, -0.3604925870895386, 0.5105307698249817, -0.044684767723083496, -0.3385152518749237, 1.0163979530334473, 1.0738601684570312, 1.8132708072662354, 1.0739595890045166, -3.7514164447784424, 0.147658571600914, -0.43700718879699707, 0.7299471497535706, -1.1400694847106934, 1.8723270893096924, -0.947730541229248, -0.16848528385162354, 0.9242150187492371, -1.7499284744262695, 1.1532788276672363, -2.324587821960449, -1.9531365633010864, -1.142845869064331, -2.1966092586517334, 0.07374340295791626, -0.47387802600860596, 3.4972450733184814, 0.2671818733215332, -1.942692756652832, -0.9062531590461731, -2.90956974029541, 1.80856454372406, -1.8269457817077637, 0.4976106882095337, 0.13913969695568085, -2.393240451812744, -2.601536273956299, -1.5305882692337036, 0.20895254611968994, -0.9155042171478271, 1.0869934558868408, 0.10991740226745605, 0.6566532254219055, 2.073089122772217, 0.13308238983154297, -0.9663084745407104, -0.7557480335235596};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
int stride_width=1;
int stride_height=1;
TfLiteFusedActivation activation=kTfLiteActNone;
int dilation_width_factor=1;
int dilation_height_factor=1;
const int filter_dims_size=4;
const int filter_dims_raw[4]={1,3,3,480};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={480};
TfLitePadding paddings=kTfLitePaddingSame;
TfLiteType filter_type=kTfLiteInt8;
TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;

struct OpData {
  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;
  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // Hybrid per channel temporary tensors.
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t input_offset_index;
};

void ExtractDepthConvParams(TfLitePadding padding, int stride_width, int stride_height,
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteDepthwiseConvParams* data_params) {
  // TfLiteDepthwiseConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetDepthConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params, char* tensor_data,
                       TfLiteAffineQuantization* quant_struct, size_t bytes_size,
                       TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}
void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to carry information from Prepare() to
  // Eval().
  return new OpData;
}

void Free(TfLiteContext* context, void* buffer) {
  delete reinterpret_cast<OpData*>(buffer);
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;

  // TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;
  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  const TfLiteTensor* bias = nullptr;

  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));

  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);
  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);
  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);
  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);

  const TfLiteType data_type = input->type;

  const TfLiteType filter_type = filter->type;
  const bool is_hybrid =
      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;
  TF_LITE_ENSURE(context,
                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||
                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);
  if (!is_hybrid) {
    TF_LITE_ENSURE(context,
                   filter->type == data_type || data_type == kTfLiteInt16);
  }

  if (data_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }

  // Filter in DepthwiseConv is expected to be [1, H, W, O].
  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));
    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (data_type == kTfLiteInt16) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);
    }
    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);
    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),
                      SizeOfDimension(bias, 0));
  }

  int channels_out = SizeOfDimension(filter, 3);
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int batches = SizeOfDimension(input, 0);

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (data_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }

  if (is_hybrid) {
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE_EQ(
        context, affine_quantization->scale->size,
        filter->dims->data[affine_quantization->quantized_dimension]);

    int temporaries_count = 0;
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;
    data->input_offset_index = temporaries_count;
    if (data->input_offset_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_offset_id));
    }
    ++temporaries_count;

    TfLiteIntArrayFree(node->temporaries);
    node->temporaries = TfLiteIntArrayCreate(temporaries_count);

    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    const int batch_size = SizeOfDimension(input, 0);
    int scaling_dims[1] = {batch_size};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    node->temporaries->data[data->input_offset_index] = data->input_offset_id;
    TfLiteTensor* input_offsets;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->input_offset_index,
                                       &input_offsets));
    input_offsets->type = kTfLiteInt32;
    input_offsets->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {
      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
      input_offsets_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                       input_offsets_size));
    }
  }

  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);
  outputSize->data[0] = batches;
  outputSize->data[1] = out_height;
  outputSize->data[2] = out_width;
  outputSize->data[3] = channels_out;
  return context->ResizeTensor(context, output, outputSize);
}

TfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    const TfLiteTensor* filter,
                                    int16* depth_multiplier) {
  int num_filter_channels = SizeOfDimension(filter, 3);
  int num_input_channels = SizeOfDimension(input, 3);
  TF_LITE_ENSURE(context, num_input_channels != 0);
  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);
  *depth_multiplier = num_filter_channels / num_input_channels;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
                       TfLiteDepthwiseConvParams* params, OpData* data,
                       const TfLiteTensor* input, const TfLiteTensor* filter,
                       const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output));
  } else {
    optimized_ops::DepthwiseConv<float, float>(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                           TfLiteDepthwiseConvParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  } else {
    optimized_ops::DepthwiseConv<uint8, int32>(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                                     TfLiteDepthwiseConvParams* params,
                                     OpData* data, const TfLiteTensor* input,
                                     const TfLiteTensor* filter,
                                     const TfLiteTensor* bias,
                                     TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));

  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output));
  } else {
    optimized_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

TfLiteStatus EvalQuantizedPerChannel16x8(
    const TfLiteDepthwiseConvParams* params, const OpData* data,
    const TfLiteTensor* input, const TfLiteTensor* filter,
    const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.weights_offset = 0;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier.data(),
      data->per_channel_output_shift.data(), GetTensorShape(input),
      GetTensorData<int16>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<std::int64_t>(bias), GetTensorShape(output),
      GetTensorData<int16>(output));

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteDepthwiseConvParams* params,
                                  OpData* data, const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* input_quantized;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &input_quantized));
  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;

  op_params.weights_offset = 0;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr);
  } else {
    optimized_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr, CpuBackendContext::GetFromContext(context));
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // const TfLiteTensor* bias =
  //     (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetDepthConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TFLITE_DCHECK_EQ(input_type, input->type);

  switch (input_type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteFloat32) {
        return EvalFloat<kernel_type>(context, node, params, data, input,
                                      filter, bias, output);
      } else if (filter->type == kTfLiteInt8) {
        return EvalHybridPerChannel<kernel_type>(context, node, params, data,
                                                 input, filter, bias, output);
      } else {
        TF_LITE_KERNEL_LOG(
            context, "Type %s with filter type %s not currently supported.",
            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));
        return kTfLiteError;
      }
      break;
    case kTfLiteUInt8:
      return EvalQuantized<kernel_type>(context, node, params, data, input,
                                        filter, bias, output);
      break;
    case kTfLiteInt8:
      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,
                                                  input, filter, bias, output);
      break;
    case kTfLiteInt16:
      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,
                                         output);
      break;
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));

  switch (input->type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

}  // namespace lnnnsn

TfLiteRegistration* Register_lnnnsn_REF() {
  static TfLiteRegistration r = {
      lnnnsn::Init, lnnnsn::Free, lnnnsn::Prepare,
      lnnnsn::Eval<lnnnsn::kReference>};
  return &r;
}

TfLiteRegistration* Register_lnnnsn_GENERIC_OPT() {
  static TfLiteRegistration r = {
      lnnnsn::Init, lnnnsn::Free, lnnnsn::Prepare,
      lnnnsn::Eval<lnnnsn::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_lnnnsn_NEON_OPT() {
  static TfLiteRegistration r = {
      lnnnsn::Init, lnnnsn::Free, lnnnsn::Prepare,
      lnnnsn::Eval<lnnnsn::kNeonOptimized>};
  return &r;
}

TfLiteRegistration* Register_lnnnsn_NEON_OPT_UINT8() {
  static TfLiteRegistration r = {
      lnnnsn::Init, lnnnsn::Free, lnnnsn::Prepare,
      lnnnsn::EvalImpl<lnnnsn::kNeonOptimized, kTfLiteUInt8>};
  return &r;
}

TfLiteRegistration* Register_lnnnsn() {
#ifdef USE_NEON
  return Register_lnnnsn_NEON_OPT();
#else
  return Register_lnnnsn_GENERIC_OPT();
#endif
}

// Warning: Clients using this variant are responsible for ensuring that their
// models only need the UINT8 type. TFLite's op registration mechanism doesn't
// yet allow for more nuanced registration mechanisms.
TfLiteRegistration* Register_lnnnsn_UINT8() {
#ifdef USE_NEON
  return Register_lnnnsn_NEON_OPT_UINT8();
#else
  return Register_lnnnsn();
#endif
}

}  // namespace builtin
}  // namespace ops
}  // namespace tflite
