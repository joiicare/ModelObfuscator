/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv.h"

#include <stddef.h>
#include <stdint.h>
#include <vector>

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/optimized/cpu_check.h"
#include "tensorflow/lite/kernels/internal/optimized/depthwiseconv_multithread.h"
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/depthwise_conv_hybrid.h"
#include "tensorflow/lite/kernels/internal/optimized/neon_check.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_float.h"
#include "tensorflow/lite/kernels/internal/reference/depthwiseconv_uint8.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/depthwise_conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/internal/types.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"

namespace tflite {
namespace ops {
namespace custom {
namespace uhkmte {

constexpr int kInputTensor = 0;
constexpr int kFilterTensor = 1;
constexpr int kBiasTensor = 2;
constexpr int kOutputTensor = 0;

// This file has three implementation of DepthwiseConv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  kNeonOptimized,
};

const int kTensorNotAllocated = -1;

int8_t filter_r   aw[12000]={-6, 19, -3, 0, 12, 0, 10, -2, 0, -2, 1, 0, 9, 8, -3, 7, 3, -5, 9, -1, -2, -2, 3, 2, -3, -1, 0, -3, 2, 2, 4, -6, 6, 3, -7, -2, -3, 0, -10, 1, 7, 4, -3, -2, 67, 4, -10, -3, -2, -2, -1, 3, -2, 0, -2, 3, 3, 4, -2, 2, -5, 10, -6, 0, -10, 1, 2, -2, -39, -2, -1, 3, -6, -2, -5, -4, -6, -16, 1, 1, -8, 7, 1, -2, 9, -1, -7, -1, -1, 14, 0, -1, -7, -13, -3, -3, -1, -113, 15, 0, 0, 18, -2, -4, -92, -8, -4, -2, 5, 0, -44, -23, 3, -9, -1, -1, 7, 9, -8, -4, -6, -2, -2, -2, 0, -4, -6, 3, 15, 16, -2, -5, -5, -4, -4, -7, -2, -2, -28, -1, 10, -16, -20, 2, -2, -1, -12, -16, 5, 2, -1, -3, -1, -1, -3, -50, -18, -2, 3, 2, 0, -3, -2, 1, -4, 6, 3, -17, -1, 14, 1, 0, -2, -16, -1, 1, 1, 4, -15, -2, -26, 4, -1, 6, -5, -4, -10, -3, 1, 2, -7, -8, -8, -103, -2, 14, -1, 5, -2, -3, -5, -6, 2, -23, 1, -8, 5, -4, -78, -11, -1, 11, 0, -3, -2, -2, -13, -3, -4, 26, -9, -5, 0, -3, -4, 1, 0, -4, -5, -7, -1, 5, -1, -2, -2, 1, -2, -5, -2, -4, -1, -19, -7, -7, -16, -33, -19, -10, 7, 1, -28, 4, -1, 5, -1, 1, -12, -6, -1, 0, -3, -9, -4, 6, -1, 6, 1, -3, 0, 7, 6, 2, 17, 0, 4, -2, -1, -1, -2, -5, -6, -1, -2, -2, 7, -2, 2, -2, -1, 0, -3, -9, 7, 30, -16, 5, -7, -7, 7, -2, -2, 2, 19, 3, -7, -5, -3, -3, -3, -4, -5, -7, 4, 22, 3, -6, -13, -5, -3, 9, -11, -1, -4, -2, -1, 0, 2, -1, -4, -26, -7, 7, -6, -2, -65, 8, -7, -4, -2, -3, 0, -3, -1, 0, -2, -17, -4, -4, -1, -4, -6, 0, -4, 8, -4, -32, -21, 0, -3, -3, -15, -6, 4, -3, -3, -4, -1, -3, 17, -6, -7, -1, 3, -1, 0, 0, -6, -6, -2, -3, 5, 7, 0, -3, 7, -22, 23, -118, -3, 0, -5, 3, -4, -8, 1, -9, -8, -3, -1, 0, 13, 8, -9, 1, -3, -3, 2, -3, 37, -8, -6, -1, -4, -3, 3, 0, -6, -1, 10, 4, -3, -2, 0, 8, -1, -40, -4, -2, -9, -37, 1, -3, -1, -3, 0, -1, -5, -2, 2, 2, 0, -1, 1, -4, -83, 0, 1, 7, -2, 11, -2, 104, -12, 3, 4, -121, -2, 24, -2, -5, 3, -2, -4, 0, 5, -127, 8, -15, -9, -30, 2, 6, 7, 0, -5, -6, -5, -5, -12, -3, 20, 24, -12, 14, 32, 1, 7, 0, -1, -1, 1, -6, 21, 4, -4, 6, 1, -1, 14, -1, 0, -2, 15, -5, -1, -3, -3, 0, 19, 5, 13, -5, -6, 2, 6, -1, 1, 0, -13, -2, -8, -1, -5, -2, -6, 19, -6, -3, 0, -1, -4, -5, 3, -1, 2, 6, -7, 7, -6, -7, -4, -3, -7, 0, 4, 6, 19, 1, -38, -3, 0, -7, -51, -3, -24, -8, 1, -1, 3, 0, 0, 13, -8, -2, 14, 2, -2, 0, -2, 11, -3, 0, -6, -5, -4, 0, -8, -70, -1, 3, -3, -2, -5, -3, -47, -16, -1, -3, 4, -4, 45, -31, 19, 0, 1, -3, 4, 9, -26, 24, -3, -3, -2, 0, -3, -9, 17, 8, 2, 6, 0, 3, 0, -3, -1, -11, -1, 0, -16, -2, 8, -5, -44, -1, -2, -1, -25, -17, 21, -2, -1, -4, -1, 2, 2, -48, -33, -4, -15, -1, -2, 0, -5, -2, -2, 38, 2, -5, -1, 1, 1, -2, 0, -12, 8, 1, -5, 17, -24, -1, -43, 9, 4, 1, -13, 0, -22, -3, 0, 4, -10, -2, -6, -72, -1, -22, -1, 11, -4, -3, 5, -8, 6, -17, 0, -6, -8, -1, -59, 1, -1, 15, -3, -11, -1, -4, 8, -3, -1, 21, -3, -3, -1, 0, -3, -9, -4, 1, 16, -17, 9, 14, -6, -2, -1, 4, -1, -2, -2, -3, -1, -6, 0, -2, -37, -24, -10, 0, 5, 0, -2, 2, 0, -1, -1, -14, 1, -5, -4, 22, -2, -10, -3, -23, 1, 0, 2, -3, -2, 1, -8, -5, -1, -2, 7, 2, 1, 0, -3, -4, 2, -6, 3, 4, 14, -4, -2, -1, 2, -2, 2, -15, 10, 5, -6, -9, -2, -2, 1, -3, -1, 5, -1, 0, -2, -9, 0, -3, -21, -4, -2, -23, 21, -9, 2, 5, -17, -2, 0, 12, -9, 0, -2, -1, -4, 3, -1, 1, -3, -8, -11, 5, 0, 1, -28, 6, -6, -2, 0, 0, 3, -4, -3, 1, -3, -16, -1, -5, -4, 1, 8, -18, -1, 8, 9, -46, -20, 1, -3, -3, -19, -2, 3, -9, -1, 2, -16, 2, 17, -4, -6, -1, 0, -2, -2, 0, -2, -13, -1, -7, -2, 21, -2, -1, 10, 10, 2, 26, -5, 1, 1, -6, -5, -4, 1, -4, -5, 3, 2, 2, 24, 7, -10, 3, 1, -26, 2, 1, -22, -14, -3, 0, -2, -2, 7, 0, 0, -5, 6, 14, -2, -1, -2, 4, -3, 14, 2, 0, -8, 13, 0, 0, 0, -3, -2, -1, -8, -7, 2, 20, 2, 1, -3, -1, -16, 1, 0, 7, -13, 13, -15, -14, 0, 1, 14, -49, 2, -16, -22, 2, -4, -9, -9, 0, 1, -50, 23, -24, -59, -20, -4, -8, 0, 1, -6, -4, -21, 1, -7, -2, -11, 47, -75, 18, 25, 1, 11, -2, -6, -2, 4, -8, 66, -66, -9, -2, 3, -4, 8, -3, 2, -6, -8, 4, -7, -2, -7, 1, -14, 10, 10, -8, 0, -15, 23, -4, -3, 3, 0, -8, 0, 4, -7, -5, -42, 59, -5, -13, -3, -4, -4, -7, 2, -8, -4, -6, -9, -11, -7, -5, -9, -12, -6, -5, 1, 2, 71, -3, -43, -5, -9, -6, -120, -8, -78, -12, -6, 7, -15, -4, -12, 43, 27, -4, 43, 1, -6, -8, 1, -3, -1, -9, -4, -19, 0, -6, 4, -94, -5, -1, -3, -1, 0, -2, -127, -58, -9, -6, -17, 1, 66, -51, 65, -2, 13, 0, 3, -18, -50, -5, 5, -9, -7, -7, -2, -19, 26, 11, -22, -16, 4, 3, 0, -11, 4, -32, -5, -3, -35, -2, -1, 3, -27, -2, -9, -9, -26, -11, 61, -8, -8, -5, -9, 1, 2, -76, -28, -9, -10, -5, -1, -4, -8, 3, -9, 12, 2, -4, -6, -74, 7, -6, 7, -3, 14, 14, 2, 64, -38, -3, -97, 9, 3, -5, -36, 0, -36, 3, 5, -4, 2, 0, -18, -5, -4, -27, -5, -7, -10, -15, 1, -21, 12, 19, -4, -35, 0, 2, -75, 3, -3, 38, 1, -39, -4, -4, -14, -5, 1, 13, -20, -9, -10, -6, -8, 7, -1, 10, 5, -35, 21, 14, -8, -8, -5, -9, 0, -9, -8, -5, -3, 0, 6, 3, -78, -5, -2, -3, 0, -4, 16, 1, -7, -22, 0, -22, -20, -21, -3, 65, -9, -8, -7, -42, 1, 1, -3, -2, -3, 5, 9, -17, -16, -9, 16, -3, 5, -8, -11, -3, -9, 2, -2, 1, 15, -6, -10, 1, 25, -10, -1, -118, 19, -14, -22, -9, -6, -17, 16, -7, -4, -9, 3, 6, 4, -21, -14, -7, -80, -4, -4, -79, 33, -52, 1, 8, -10, 7, -4, 14, -1, -5, 0, 3, -10, -2, -4, -1, -7, -56, -81, -4, -4, 5, -44, -1, -5, -8, -6, -6, -13, -12, -3, -3, -16, -30, 2, -6, -5, -34, -5, -13, -2, -4, -7, 9, -35, 1, -5, -1, -20, -4, -9, 1, -11, 5, -13, -10, 15, 5, -21, -2, 7, -2, 1, -11, -11, -53, -7, 0, -3, 73, -5, -6, -5, 0, -21, 29, -13, 2, 5, 10, -10, -7, -10, -52, -6, -4, 5, -2, 10, -2, -18, -2, 3, -66, 4, -2, -99, 3, -11, -2, -8, -7, 2, 1, -1, -7, 20, 57, 10, -8, -2, -12, 4, 10, 0, -7, -4, -5, 9, -3, -5, -6, -7, -6, -17, 2, -2, 18, 5, -3, -1, 3, -64, 4, 5, 0, -7, 5, -55, -111, -3, 6, 54, -74, -14, -33, -75, 0, 1, -15, 0, -9, 0, -72, 69, -37, -84, -30, -16, -10, -4, 5, -11, 1, -70, -1, -4, -13, -14, 12, -12, 12, -3, 0, 9, 0, 3, 2, 1, -6, 22, 3, -3, -6, 3, -1, 12, -2, 0, -2, -3, -4, -2, -2, -2, -2, 19, 5, 9, -5, -5, 0, 4, 1, 1, 0, -5, -2, -3, 1, -5, -1, -28, 19, -7, -4, -2, 0, -3, -2, -7, -2, 2, 5, -8, 4, -3, -5, -5, 14, 8, -1, 1, 4, 18, 0, -31, -2, -1, -4, -40, -3, -25, -1, 0, -5, 1, 0, 0, 14, 28, -3, 21, 2, -3, 1, -4, -11, -3, 0, -6, -8, -4, 0, 6, -71, -1, -1, -2, 2, 6, -4, -38, -21, -2, -2, 3, -4, -51, -34, 18, -1, 0, 7, -9, 6, 1, -24, -4, -4, -1, 3, -3, -7, 20, 8, -9, 3, 2, -1, -3, -3, -3, -11, -2, 0, -13, -1, 10, 4, -24, -2, -2, -1, -17, -15, 22, -1, -1, -2, -1, 0, 2, -51, -34, -2, -13, 4, -2, -1, -5, -2, -2, -16, 2, 3, 0, -1, 0, -3, 0, 7, 4, 2, 7, 17, -22, -3, -31, 9, 5, 9, -14, 0, -20, -3, -1, 5, 0, 4, -4, -68, -1, -11, 1, -5, -4, -5, 6, -6, 4, 34, 0, -1, -13, -2, -41, -2, -2, 20, -11, -12, 0, -3, 4, -3, 1, 25, -19, -2, 1, 1, -5, -7, -3, -1, 16, -15, 4, 13, -4, 0, -1, 3, -3, -2, -2, -3, 1, 4, 2, -2, -33, 24, 10, 3, -6, 0, -12, 2, 0, -1, 1, -23, -1, -1, -3, 24, -4, -9, -4, -22, 2, 8, -1, -3, -1, 9, -7, -3, 3, -3, 7, 0, 2, -2, -3, 5, 4, -7, 3, 3, 16, -2, -6, -2, 4, -1, 2, -17, 11, -11, -1, -9, -3, -4, 0, -2, 0, 4, -12, -1, 1, -9, -2, 1, -18, -4, -3, -25, 19, 5, 2, 5, -13, -2, -3, 13, 11, 2, 0, -1, -3, -1, 2, 2, -2, -18, -13, -6, -1, 0, -35, -2, -4, 2, 0, 1, 3, -5, 0, 1, -2, -19, 0, -3, -2, -2, 9, -5, -2, -5, 6, 24, -21, 1, -3, -4, -7, -2, 11, -10, 0, 1, -20, 1, -1, -5, -7, 0, 0, -1, -4, -1, -1, -14, 0, 4, -2, 17, -3, 0, -7, 10, -15, 28, -4, 0, 3, 12, -5, -6, -1, -21, -6, 2, 0, 3, -48, -8, -8, 6, 0, -22, -2, -2, -45, -3, -1, 1, -1, 0, 5, 1, -1, -5, -21, 16, -2, -2, -1, 4, -1, 17, 2, 1, 14, 15, 0, -1, 0, -3, -2, 0, -9, -11, -1, 16, 1, 1, -3, 4, -24, 2, 1, -7, -12, 8, -15, -79, 1, 1, 15, -52, 7, -13, -27, 2, -6, -8, 1, 1, 2, -45, 23, -13, -40, -17, -5, -3, 2, 2, -8, -3, -21, 0, -2, 0, 4, 11, -4, 1, -2, -1, 12, -4, -1, 2, 1, -1, 9, 10, -2, -4, 2, -5, 9, -1, -1, -1, -3, 1, -2, 0, -1, -5, 2, 2, 5, -5, 6, 3, -8, -2, -4, 0, 6, 0, 0, 3, -2, -2, -27, 6, -7, -3, -1, -2, -3, 5, -25, 0, -2, 5, 3, 6, -1, 2, -3, -4, 8, -1, 7, 1, 7, -1, -42, -3, -1, 2, -14, -3, -4, -3, -5, -14, -1, -1, -8, 6, 9, -2, 20, 1, -7, -2, 0, -16, 0, -2, -9, -11, -2, -4, 11, -127, 8, 2, 0, -17, 2, -4, -89, -9, -2, -2, 2, 0, -24, -11, 4, -7, 0, 5, -19, -16, -10, -31, -7, -2, -2, -3, -3, -6, -3, 2, -20, 16, -2, -2, -3, -5, -2, -9, -2, -4, -36, -1, 5, -7, -1, 5, -2, -1, -1, -17, 6, 1, -1, -1, 0, 0, -3, -38, -24, -3, 0, -1, 0, -2, -2, 1, -5, -23, 2, -22, -2, 14, 1, 1, -2, 13, -4, 2, -4, 4, -13, -2, -30, 4, 1, 1, -7, -4, -9, -3, 1, 4, -1, 2, -8, -116, -2, -5, -2, -6, 0, 1, -6, -6, 1, 5, 0, -8, -9, -3, -91, -7, -1, 10, 8, -2, -2, -3, -13, -3, -3, -42, -16, -6, 0, -4, -4, 1, 0, -3, -4, -12, 4, 7, -1, -3, -3, 0, -2, -3, -2, -5, -5, 4, -5, -5, -21, 4, 11, -10, -4, 1, -19, 4, -1, 0, 0, 19, -11, 18, 1, -1, -1, -10, -4, 7, -1, -7, 2, -2, 0, 4, 5, 2, 19, -1, 5, -1, 1, -1, -2, 10, -9, -3, -2, 1, 6, -5, 7, -2, -2, -1, -2, -7, 5, -14, 13, 1, -5, -10, 7, -3, -2, 2, -11, 3, -8, -5, -2, -4, 2, -4, -3, -5, 5, 22, 1, -4, -5, -4, -2, 10, 3, 0, -6, -3, -1, 0, 5, -1, -5, -20, -6, -5, -7, -1, -36, -10, -5, 2, -1, -4, 2, -4, -5, 2, -4, -17, -3, -4, -2, -4, -7, 0, -4, -6, 2, 14, -17, 0, -2, -1, -4, -7, 3, -3, 0, 1, 4, -4, -8, -5, -5, -3, 4, -1, -4, 0, -6, -7, -5, 5, 0, 8, -1, -4, -8, -22, -2, 23, -3, 2, -6, 10, -6, -6, 2, -11, -8, -3, 0, 1, -25, -7, -4, -1, -2, -9, 1, -2, 25, 18, -5, -2, -5, -1, 2, -2, -5, -1, -6, 5, -4, -1, -1, 7, -3, -40, -3, -4, 6, -2, 4, -3, -1, -2, 0, 0, -6, -1, -1, 6, 1, 0, -1, -2, -76, 0, 0, -6, -1, -1, -4, 54, -11, 4, 8, -127, -9, 22, -3, -6, 3, -2, 10, 2, 4, -99, 6, 44, 2, -30, 1, 6, 7, 0, -6, -7, -4, -3, -15, -4, -26, 4, -2, -3, -7, 3, 11, 1, 4, 1, 2, -6, 9, -8, -2, 22, 3, -5, 2, -1, -2, -1, -14, -4, 1, 1, 0, 5, -6, 8, -6, -2, 6, 4, -10, -3, -1, -1, -10, -2, 31, 1, 0, -1, -2, 5, 0, -4, 0, 0, -2, -3, -14, -2, 0, 4, 4, -3, -4, 13, -7, 22, -27, -3, -21, -3, 9, 0, -39, -2, -1, 1, -31, -2, -7, -5, -1, 0, 4, 2, -6, 2, 1, -1, 4, 0, -4, 0, -1, 17, 2, 0, -5, -1, -4, 0, 24, -22, 10, -1, -3, 15, -10, -2, -24, -6, -1, -2, 3, -1, -33, -41, 1, -10, 2, -22, 21, 4, 4, -31, -9, -2, -2, 0, -1, -9, -7, -7, 21, 16, -3, -3, -1, -3, -3, -3, -3, 0, -31, -3, 12, -23, 29, -25, -1, -4, -1, -10, 8, 1, -2, -3, 0, 1, 1, -30, -34, -2, 13, -7, -4, -1, -2, 0, -2, 42, 8, 13, 0, 6, -1, -1, 2, -12, -11, 7, 20, 5, -20, -3, -27, 2, -5, 26, 0, -1, -15, -1, -1, 1, 2, -32, -1, -103, -2, -25, -3, 21, -4, -2, -2, -10, 5, -19, -4, -11, 12, -4, -52, -1, 0, -2, -2, 9, -1, 1, 10, -3, 0, -73, -21, 0, -2, -1, 3, -1, -1, 3, -22, -6, 9, -7, -3, -2, 0, 3, -4, -1, -3, 0, 5, -23, 5, 3, -37, -30, -17, 2, 22, 1, 11, 0, 1, 5, 2, -21, -8, -3, -3, 1, -1, 3, -2, 12, -5, 12, -2, -1, -2, 9, 17, -2, -5, 0, 9, 2, -5, -3, -3, -27, -5, -1, -5, 3, -1, -5, 5, -3, -3, 4, 0, -39, 4, -16, -14, 3, -2, -14, 3, -4, 0, -3, 21, 2, -9, -3, -2, -1, -2, -3, -1, -4, -7, -32, 4, -5, 8, -2, -2, -8, -17, -1, 1, -1, -4, 3, -6, -1, -3, -19, -4, 27, 0, -1, -31, 27, -1, -32, 0, -1, 2, -4, 4, 2, -4, -7, 3, -2, -3, 0, -8, 2, -2, 20, -10, -68, -19, -2, -1, -3, -4, 0, 0, -6, -1, 2, 23, 1, 17, 17, -7, -3, 1, 0, -2, -3, -7, -4, -5, -24, 10, 2, -2, -2, 27, -10, -29, 29, -4, 1, 1, 11, -4, -4, 2, -10, -6, 0, 4, 0, 23, 25, 1, 1, 4, -7, -3, -1, 33, -12, -4, -2, -1, 7, 3, 4, 1, -6, 12, 4, -4, -1, 2, -5, -7, -8, 2, -3, -18, 10, 2, -1, -1, -2, 0, -2, -6, 13, 4, -4, 1, 0, -4, -3, -70, -1, 1, 28, 6, 9, -3, 24, 0, 0, 4, -54, 6, -15, -6, -2, 9, -6, -17, -6, 0, -52, 9, -9, -11, -18, 2, 9, -3, 1, -5, -1, -4, -2, -14, -1, -9, 51, -41, 16, 74, -8, -15, 1, 2, 2, -1, -4, 38, 50, -3, 30, 2, 0, 31, -3, -2, -6, -16, -15, -4, -3, -2, -5, 4, -3, 5, 6, 7, 1, -11, 3, 1, 1, -30, -1, 49, 9, 8, -2, -8, 37, -8, 0, -4, 2, 3, -15, 1, -1, -3, -10, 1, 47, -8, 13, -3, 54, -47, -1, -15, -3, 38, 0, -29, -1, -3, -1, 5, -5, -47, 17, -2, 9, -35, 5, -2, 16, -7, -1, 18, 0, 8, 0, 1, 54, 1, -3, 5, -12, -3, 0, 43, 3, 13, -6, -1, 126, -19, 3, -21, -35, 0, -1, 3, -3, -45, -51, 33, 11, -12, -34, -21, -12, -49, 54, 13, -3, -1, 1, 2, -9, -1, 8, 24, 39, -4, 5, 2, -4, -11, -37, 10, 0, -41, -3, -7, -71, -4, -30, -3, -7, 2, 32, 44, 3, -2, -2, -1, 2, 4, -58, 16, -10, 6, 15, 8, -3, -8, -3, -1, 127, 5, -10, -1, -6, 6, -3, -2, -37, -8, 5, 17, 40, -42, -3, -38, 5, -10, 24, -34, 8, 4, -10, 4, -4, -21, -41, -14, -88, -2, 18, 0, 38, -1, -6, 23, -14, 5, -120, 4, -17, 10, -5, -37, -41, 0, 18, -13, -26, 3, -15, -24, 0, 4, 10, 13, -1, -1, 0, 2, 1, 6, 1, -9, -44, -10, 21, -3, -2, 1, -6, -2, -6, -1, -2, -8, -33, -4, 2, -53, -69, -50, -2, 37, -1, 52, -4, -7, -28, -3, 6, -9, 1, -7, 26, 0, -2, 0, -7, 5, -4, 5, 7, -2, 5, 8, -9, -20, -3, 4, 9, 0, -7, -2, -48, -4, 5, -1, -4, -3, -1, -15, 5, 1, -3, 3, -44, 24, 20, -19, -2, 3, 8, 9, 0, -6, 11, 38, 7, -1, -24, -5, -12, -37, -1, 3, -44, 34, 4, -6, 0, 2, -3, -10, -32, -46, -3, 2, 0, -1, -5, -23, -2, -3, -27, -27, 39, 1, 3, -26, 25, -1, -39, -3, 0, 3, 0, -13, 0, 19, -39, 17, 0, 1, -17, -10, -2, 0, 35, -32, -127, -22, -1, -4, 5, -36, -3, -15, -2, -2, -2, 1, -7, 30, 4, -3, 1, -3, 3, 1, -1, 2, -36, -5, -40, -38, 34, -3, 3, 38, -40, 16, 7, 2, 1, 3, 5, -3, -1, 2, -16, -15, 0, 5, -4, 95, 41, -10, -2, -3, -38, 6, 2, 16, -45, 0, 9, 2, 6, -4, -3, -4, -3, -19, 39, -5, -8, 6, 13, -2, -47, 15, 1, -47, 7, 3, -5, -2, -8, -8, 1, -17, 6, -6, 24, 3, -1, 4, 2, -53, 5, 5, 36, -2, 54, -38, 39, -9, 2, 35, -17, -21, 3, -47, 6, 6, 1, -36, 3, 7, -43, 39, 16, -68, -19, 12, 0, 12, 8, -3, -10, -39, -28, -14, -2, -22, 111, -104, 49, 127, -22, -71, 1, 11, 20, -2, -11, 117, -13, 0, -2, -1, 17, 127, -12, 33, -12, -3, -30, 5, 8, -10, 41, 57, -54, 127, 14, -13, -38, 80, 1, -7, 5, 1, 9, 35, 18, 0, 3, 18, 113, -67, 6, 4, 2, 14, -32, -38, -11, -13, -64, -53, 127, 2, -57, 6, -20, 1, -4, -1, 19, 121, -4, -43, 4, -6, 34, 66, -4, -127, -13, 3, 15, -84, 12, 23, 125, 127, 2, 110, 1, 26, 6, 12, -19, 0, 9, 21, -97, -14, -4, -3, -10, 68, 9, 7, -2, 1, 10, -116, -127, 10, -1, -47, -3, 93, -50, 127, 35, -22, -3, -24, 14, -127, -80, 34, 5, -3, 20, 11, -9, 71, 59, -32, 127, -31, 25, 0, 5, 1, -127, 17, 7, -90, -1, -60, -33, 39, -2, -1, -18, 29, 6, 119, 3, -1, -6, 6, -15, -7, -79, 53, -55, -70, 62, 9, 9, -24, -10, -4, -25, -56, -73, -3, -118, 18, -10, -4, -16, 31, -9, 15, 127, -100, -6, -106, 37, 127, -3, -127, 31, 13, -46, 17, 5, -72, -1, -19, 21, 0, 12, 1, -10, 9, 7, 127, -45, 31, -14, 44, -48, 2, -5, -90, -24, -5, 127, 39, -78, 2, -54, -107, -1, -10, 82, -36, 11, 15, -6, -11, 28, 22, -20, 127, -127, 17, 127, -3, 2, -4, -20, -13, -2, -4, 7, -44, -21, -40, 1, -60, -70, 10, -23, -4, -2, 38, -5, -22, -67, -34, 52, -34, 35, -1, 114, 2, -11, -1, -59, 127, 0, -13, 29, -2, 28, 1, -25, -39, -10, -10, 21, -22, -15, -3, -4, 10, 9, 25, -24, 127, -4, -85, 8, 63, 8, 3, -50, 127, 26, -31, -48, 9, 53, 28, 3, -34, 12, -8, -22, -30, -81, -13, -85, -115, -7, 13, -126, 127, 42, -57, 127, 35, 2, -33, -106, -3, 15, -12, 1, -7, -26, 44, -12, -1, -29, -93, 0, 2, 10, -54, -5, -3, 6, 6, -4, -29, -1, -43, -5, 17, -103, 41, 11, 8, -88, 127, 13, 17, -9, -45, 43, -53, 4, -11, 38, -35, 9, -52, 10, -5, 39, -123, 11, 38, -47, 21, 8, -25, 6, -66, -25, 12, -117, 21, -7, -107, 113, -7, 6, -3, -118, 57, 41, -3, 2, 43, 63, 10, -43, 14, -127, -15, 3, 2, -27, 41, -7, 32, -23, -22, -97, -20, 2, 20, -7, 6, 17, 8, 30, -30, -36, -4, -3, 99, 120, 34, -7, 9, 32, 9, 56, 9, 13, -12, 15, 2, -4, -2, 11, -27, -10, -46, -57, 19, 127, 8, 16, 34, -13, -66, 7, 8, 0, -82, 22, -127, 34, -25, 5, 127, -16, -86, 34, -127, 19, -48, 25, -4, 12, -9, -42, 125, 20, -113, -53, 17, -67, 12, 21, 13, 16, -120, -21, 17, -8, 20, 16, -43, 15, -10, -5, -16, 1, -1, -3, 0, -3, 38, 52, -1, -34, 3, -1, 33, -1, -3, -6, 14, -13, -2, -2, -2, 17, 2, 1, 5, 5, 5, 3, -19, 1, 0, 1, 8, -4, -19, 8, 7, -4, 9, 40, -7, -1, -4, 1, -1, -18, -68, -1, -3, -9, 6, 50, -7, 11, -3, -37, 38, -2, -8, 1, 38, 1, -16, -1, -2, -3, 1, -5, -42, 5, -1, 11, -23, -3, -4, 16, 69, 0, 51, -2, 6, -1, 0, -36, 0, -4, 5, -7, -7, -1, -43, 10, 8, -3, -1, -127, 17, 4, -20, -30, 2, 0, 5, -2, 44, -45, 32, 12, -10, 31, -33, -2, -16, -73, 13, -2, -2, 1, 2, -7, -8, 5, -20, 43, -1, 5, 7, -4, -10, -29, 7, -1, -53, -2, -7, 3, -15, 26, -1, -6, 0, 5, 41, 2, -2, -3, -2, 2, 3, -59, 10, -6, -7, 10, 6, 0, -5, -4, -3, -116, 10, -6, -3, 2, 6, -3, -1, 55, -19, 6, -37, 36, -44, -2, -31, 2, -11, -21, -28, 11, 2, -8, 5, -2, 12, 33, -13, -71, -2, -5, -1, -44, -1, -7, 25, -13, 6, 127, 4, -18, -33, -3, -34, -4, 0, 16, 43, -25, 0, -14, -15, -1, 2, 18, -36, -3, 1, -1, 6, -1, 3, 2, -2, -48, -15, 21, -5, -2, -1, -6, -2, -4, -2, -3, -7, -25, -4, 3, -47, 52, 51, -2, -38, -1, 56, -4, -7, -32, -7, 8, -10, -2, -6, 22, 2, -4, -3, -5, 3, -11, 9, 12, -3, 2, 8, -7, -19, 1, 6, 10, 1, -7, -2, 42, -6, 3, -2, -8, -7, -2, -16, 4, 0, -1, 3, -41, 22, 25, 23, -3, 2, 7, 16, 3, -8, 14, -48, 6, -28, -22, -5, -10, -34, 0, 2, -45, 29, 14, -7, -2, -3, -5, -5, -40, 36, -1, 2, 1, -2, -6, -16, -4, -3, -15, -34, -43, 3, 4, -40, -37, -2, 21, -5, -1, 1, -1, -15, 0, 16, -40, -7, -1, -1, -16, -12, -16, 1, -36, -30, 98, -26, 0, -5, 1, 13, -5, -16, 1, -3, -3, 7, -3, -5, 2, -3, 1, -2, 3, -19, 0, 1, -35, -3, 35, -35, 36, -3, 1, -33, -44, 22, 20, 0, 0, -2, 24, -2, -1, 5, -41, -14, -5, 1, -4, -127, -43, -19, -7, -3, -46, 5, 0, 25, 37, -6, 11, -1, 6, -4, -3, -5, -2, -28, 40, -5, -8, 3, 13, -3, -49, 13, 0, 44, -3, 4, -4, -2, -9, -7, 3, -18, 4, 2, 28, 4, -3, 4, -3, -40, 5, 5, -40, -4, 55, -40, 66, -11, 3, 30, -7, -19, -2, -49, 2, 12, 2, 32, 2, 6, -60, 35, 2, -70, -27, 11, 1, 14, 7, -1, -10, -35, -28, -8, -2, 25, 5, -3, -1, 3, 2, 6, 2, 1, 5, 0, -8, 6, -8, -4, -25, 1, -3, -1, 1, -2, -1, 20, -1, -1, 1, 0, 4, -5, 6, -5, -3, 7, 6, -13, -2, 0, 1, 8, -1, 4, 4, 1, 0, 25, 5, 1, -3, 1, 0, -1, 2, -22, -2, 0, 3, 1, 1, -4, 13, -6, -21, 22, -1, 11, -2, 10, 0, -41, -1, -1, 3, -33, -2, -7, -11, -3, -1, -1, -2, -5, 3, -8, -1, 2, -1, -4, -2, 1, -11, 2, -1, -5, -10, -3, 0, -30, -16, 9, -5, -3, -19, 10, -1, -31, -4, -2, -3, 3, -2, -13, -27, 0, -12, 2, 22, -18, 3, -10, -26, -3, -2, -2, -1, -1, -5, -12, -5, -21, 19, -4, 0, 1, 0, -4, 3, -5, 1, -16, -1, 13, -9, -25, 25, -2, -5, -4, -10, 5, 1, -2, -3, 2, -1, 2, -33, -26, -5, 4, -6, -3, -3, -2, 1, -2, -18, 3, 11, 1, 8, 0, -2, -1, 18, -15, 6, -10, 6, -22, -2, -12, 3, -7, -21, 1, 1, -8, -2, 0, 2, -5, 28, -2, -97, -2, -11, -2, -31, -3, -1, 0, -7, 7, 49, -4, -11, -14, -2, -40, -11, 0, 0, 35, 9, -1, 0, 6, -3, 1, -15, -6, 0, -4, 0, 1, -3, 1, 3, -20, -3, 10, -8, -3, 0, 1, 4, -4, -2, -2, -1, 5, 17, 3, 3, -31, -23, 19, -1, -21, 0, 10, 2, 0, 7, 4, -17, -10, -15, -4, 3, -3, 3, -1, 4, -4, -22, 0, -4, -1, 7, 17, -2, -11, 0, 7, 1, -4, -3, -4, 21, 0, 1, -5, 2, 1, -5, 6, -2, -4, 2, 1, -33, 5, -14, 17, 5, -1, -10, -2, -3, -2, -4, -11, 3, -4, -2, -2, -2, -5, -2, -1, -4, -6, 19, 3, -5, -1, -5, -2, -7, 9, 2, 0, -1, -4, 1, -4, -1, -3, -28, -7, -22, -2, 0, -36, -26, -1, 27, 1, -1, 3, -2, 2, 0, -4, -11, 4, -3, -1, 1, -9, 4, -2, -25, -9, 26, -19, -1, -1, -1, -19, 1, 1, -10, 1, 1, 21, 1, 2, 15, -8, -2, 3, -1, -2, -2, -1, -1, 1, 23, 4, 8, -2, -1, -24, -2, -12, -121, -4, 1, 2, 13, -2, 0, -2, 6, -8, 5, 2, -1, -64, -24, -6, 3, 7, -10, -1, 0, -33, 20, -3, 1, 2, 7, 3, 4, 1, -6, 7, 5, -5, -1, 4, -4, -7, -6, 3, -4, 19, -24, 1, -2, 0, 0, -1, -4, -4, 12, 2, -3, 1, 1, -3, -3, -65, 0, 1, -19, 9, 13, -4, -25, 1, 0, 3, -53, 7, 0, -7, 4, 6, -6, 13, -7, 0, -40, 8, -21, -5, -17, 5, 7, -5, 2, -5, 2, -8, -3, -11, -2, 29, -7, -18, 3, -4, 6, 11, -1, -3, 2, 6, -11, -3, -51, -8, 92, 2, -6, -8, -5, -5, -5, -81, 0, -5, -4, -5, 4, 7, 10, -16, -9, 24, -3, -17, -6, -3, 1, -10, -8, 36, 6, -6, -5, -61, -1, -5, -9, -3, -2, 4, -6, 32, -8, -2, -8, -5, -15, -9, 17, -7, 27, -96, -6, -35, -3, -11, -7, -125, -5, -7, 0, -31, -6, -4, -17, -6, -7, -23, 5, -9, -13, 0, -4, 1, 3, 2, -6, -3, 21, -5, -12, -7, -27, 0, -7, 81, -55, 8, -14, -5, -2, -15, -5, -110, -4, -4, -9, 1, -4, 17, -90, -10, -7, -2, -96, 65, -17, 2, -21, 5, -4, -7, 1, -3, -27, -2, -31, 35, -2, 3, 1, -4, -9, 3, 6, 5, -5, -43, -6, 12, -31, -73, -89, -5, -6, -13, -25, -2, -15, -4, -7, -7, 1, 2, -27, -34, -6, 40, -8, -4, -5, -6, 0, -8, -3, -13, -11, -6, -10, 4, -5, 6, -21, -23, 7, 29, -3, -55, -5, -68, 4, 0, 76, 3, 7, -14, -3, 6, -6, 22, -95, -3, -34, -5, -36, -6, 87, -8, -14, 1, -22, 10, 25, -2, -7, 7, -1, -45, 18, -3, -8, -47, -7, -2, -8, -8, -6, 2, 16, -12, -6, -6, -4, -1, 3, -3, 7, -44, 4, 22, -18, -5, -10, -6, 0, 3, -7, -8, -2, -5, -31, -3, 3, -98, -31, -41, -4, 83, -3, 22, -1, -8, -13, 2, 21, -20, -15, -7, -5, -10, -5, -6, 29, -1, 58, -7, -5, -1, 15, -17, -5, -16, -6, 16, -9, -1, -9, -12, -95, 4, 4, -9, 3, -1, -7, -1, -3, 9, -5, 1, -127, -13, -27, -43, 18, -3, 14, 14, -4, -4, -5, 25, 6, -3, -1, -12, 9, -12, -9, -2, -9, -8, 22, -13, 1, -47, 3, -2, -5, -54, 6, -3, 2, -8, -6, -11, 2, -8, -18, -16, 87, -1, 4, -72, 69, 4, -98, -4, -3, -9, -7, -18, -1, -11, -6, 1, -6, -3, -23, -4, -22, 1, 86, 0, -5, -22, 3, -8, -3, -19, -1, -11, -3, -10, 6, 30, -2, 16, 30, -25, -4, -1, -1, 7, -7, -2, -6, 0, -96, -6, -7, -9, -4, 75, -28, 1, 25, -12, 7, -4, -24, -8, 5, -5, 0, -5, -2, 9, -2, 21, 93, -25, -4, -14, -71, 2, -4, -84, -19, -10, 9, -5, 1, -1, 0, 2, -11, 13, -1, 8, -7, -6, -1, 1, -40, 0, -2, -83, 6, 9, -3, -5, -2, -8, -7, -6, 25, 11, -6, 6, -2, -2, 6, -56, 3, 8, 81, 14, 28, -3, -121, -5, 6, 1, -62, -5, -10, -4, 5, -6, -13, -48, -5, -5, -74, -5, -7, -83, -33, 0, -1, -15, 5, -10, 1, -3, -7, 2, -11, 127, -10, -24, -59, 21, -27, -69, 5, 8, -1, 1, -41, -2, -21, 3, 124, -6, 15, -16, 9, 25, -12, -127, -31, 1, 6, -6, 42, 14, -48, -11, 12, 32, -47, -68, 2, -5, 4, -89, 8, 127, 25, 8, -2, 50, -4, -22, 10, 5, 1, 18, -35, 127, -7, -16, -63, 34, -51, 3, 47, 0, 127, -127, -3, -75, 16, -10, -2, -98, 11, -4, 18, 68, -2, -10, -5, 5, 26, -127, 9, -9, -16, -20, 0, -1, -6, 26, 6, -5, 127, 12, 13, 15, -99, -28, 0, 112, 19, 71, 14, 3, -3, -127, 9, -87, -2, 17, 2, -24, 0, 122, -83, -13, 46, -18, -127, 127, 12, 17, 123, 38, 7, -3, 13, 11, -7, -73, -73, 127, -25, -39, 16, 3, 8, -41, 5, 51, 6, -74, 3, -68, -111, 37, -127, -7, -20, 14, 3, 0, -16, -3, 2, 11, -14, -6, -76, 127, -20, 100, 50, 15, 10, 19, -8, -1, -81, -70, -69, -2, -18, 15, -6, -7, -106, -38, 19, 127, -9, -114, -7, -67, -53, -1, 91, -4, 24, 5, 15, 25, 17, 127, -127, 0, -82, -4, 19, -2, 127, 4, 9, 29, -57, 29, 6, 31, -27, 127, 8, -54, 104, 0, -27, -116, -25, 1, -27, -127, -5, -16, 42, 9, 13, 11, -2, 7, 38, 33, -18, -116, 19, 17, -31, -9, 6, 0, -21, -5, -1, -1, 11, -52, -91, -33, 12, -127, -104, -127, -22, 110, -3, 82, -10, -15, -70, -19, 45, -43, 22, 3, -11, 2, 20, 3, 127, 13, 127, 19, 37, -1, 24, 48, -7, -103, -7, -6, 19, 5, -12, -1, -127, 11, 20, -10, -17, -71, -4, -38, 12, -5, 11, 6, -57, -16, 18, -75, 1, 2, 22, 21, 11, -32, 11, 127, -22, -47, -7, -9, 10, -19, 8, 14, -14, 1, 45, -43, -3, 44, -1, -30, -85, -127, 1, -9, -2, -8, -27, -47, -10, 6, -17, -30, 116, 6, 4, -65, 108, 26, -121, 8, -7, 12, -3, -60, -21, 21, -13, 37, 7, 7, -57, -24, 2, 13, 117, -127, 12, -50, 2, -10, 13, 27, 5, -46, 39, -1, 34, 118, 1, 76, 71, 27, 11, -29, 2, -97, -6, 18, -16, 28, -127, -115, -4, -3, 8, 109, -108, 50, 42, 3, 16, -6, -46, 5, -66, 13, -5, -11, 7, -12, -27, 87, 115, 13, -24, -31, -94, -49, 8, 55, -102, 4, 11, 13, 28, -36, -18, 3, -2, 65, 1, 32, -6, 14, 18, 21, -38, 19, -8, -123, 9, 3, -4, 1, -2, -15, -2, -29, 74, 21, 3, 6, 10, 49, -20, -15, 0, 6, 114, 16, 52, -2, 99, -26, 1, -9, -34, -109, 33, -4, 21, 127, 24, -126, 8, -15, -25, -5, 26, -106, -75, 28, 5, -87, 19, 5, 31, -2, -41, 7, -9, -35, 9, -44, -127, -46, -127, -127, 127, 127, 127, -127, -35, 1, 127, 127, -7, -127, 127, 0, 127, 127, 127, 30, -127, 127, 127, 127, 127, 127, -127, -120, 127, -127, -127, 127, 127, 127, -127, 27, 127, 22, 127, 127, 127, 127, -3, 19, 127, 127, 127, 127, -127, 28, 127, 127, -127, -127, -8, 127, -127, 127, -35, 2, 127, -10, 127, -8, 127, -127, 127, 127, 127, 127, 127, -9, 127, 127, 127, -121, 127, 127, -127, -36, 127, 9, -127, 127, 127, 53, 5, 127, 127, 127, -127, -127, 127, -16, -28, 127, 127, 127, -1, 2, 127, -116, -4, 127, 127, -127, 127, 127, -127, -29, 127, 127, 0, -18, 127, -4, -127, 127, 127, 127, 127, 127, 127, 127, -127, -61, -104, -127, 127, 127, 127, 127, 13, -127, 127, -127, 127, -127, 28, 127, 8, 127, 127, 127, 127, 1, 127, 127, 127, 127, -127, -127, -127, 105, -30, -127, 127, 127, 127, 127, -127, 127, 52, -127, -127, 127, -49, -127, 127, -127, 8, 127, 127, -10, 3, -127, 127, -127, -127, -21, 26, 14, 127, 127, 127, -127, 127, 105, -9, -102, 61, 127, 127, 127, -20, 127, 127, -101, 127, 127, 53, 127, -127, 32, 127, -115, 127, 127, -8, 29, -39, 127, -34, -122, 127, -127, 127, 127, 127, 127, 127, 127, 127, 127, -127, 60, 2, 127, -53, 127, 127, 127, -127, 127, 127, 127, 127, -127, -17, -127, 127, -4, -127, -88, -127, -6, -127, -19, -127, 127, -127, 127, 127, -127, 127, 127, -17, 127, 127, 127, -34, -95, 18, -127, 127, -127, 127, 127, 127, -127, 127, -127, 127, 127, 127, 127, 7, 127, 127, 127, -127, 45, 127, -127, 127, -127, 127, 127, -119, -47, 127, -52, -25, 127, 127, 127, 127, -127, 127, -11, -127, 127, -9, 127, 127, -15, 127, 127, -5, -38, 127, -127, -70, 127, -127, -127, -127, 6, 127, 127, -127, 127, -127, 127, -62, 127, -112, -44, -5, 127, -127, -127, -11, 127, 3, 127, 127, 127, 127, -127, -127, 127, 62, 127, 127, 127, -127, -28, 127, 127, -2, -114, -20, -127, -127, 127, 127, 127, 127, -127, 127, 127, 127, -12, 127, -127, -127, 127, 127, -127, 127, 127, 127, 127, 10, 127, -2, -127, -1, 127, 127, -3, -127, 127, 51, 127, -127, 127, -127, 127, -127, 127, -6, 127, 127, -127, -127, -22, 1, 127, -127, -127, -127, -127, 127, 127, -7, 127, 127, 127, 127, -127, -127, 127, 127, 56, 13, 127, 127, 127, 127, 127, 127, -127, 127, -5, 127, -127, 127, 127, 127, 127, 127, -100, -127, 127, -43, -127, 127, -127, -127, -127, -127, -127, -1, 127, -127, -13, 127, -127, -127, -16, -40, -127, 127, 2, 127, -26, 127, -3, 127, -127, -107, 0, 127, -127, -127, 127, -127, -127, -127, 127, 127, -9, 127, -127, 127, -82, -2, -27, -62, -22, -25, -69, 2, 8, 24, 0, -42, -2, -21, 0, -127, -6, 7, -15, 6, 24, -13, 97, -26, 0, 9, -9, 40, 10, -52, -13, 12, 40, -47, -71, -2, 0, 6, 127, 6, -113, 27, 11, -1, 34, -7, -19, 8, 3, 0, 13, -31, -117, -9, -17, -69, 37, -42, 4, 47, 3, -100, 122, 0, 127, 13, -5, -2, -111, 6, -8, 14, 66, 0, -11, -3, 7, 16, -120, 1, -9, -14, 22, 0, -4, -6, 28, 4, -5, -101, 11, 14, 13, -93, -24, 0, -127, 10, 60, 17, 6, 2, 117, 11, -93, 4, 17, 5, -26, 1, 118, -84, -7, 45, -18, 112, -90, 13, -3, -109, 38, 7, -5, 10, 10, -7, -62, -69, -83, -28, -34, 20, 5, 6, -42, 5, 51, 5, -62, 4, -64, 127, 36, 112, -10, -20, 17, 22, 2, -20, -4, 2, 9, -11, -3, -79, 113, -26, 91, 42, 14, 6, 19, -9, 2, 8, -71, -69, -4, -15, 16, -7, -9, 127, -61, 17, -117, -6, -119, -7, -80, -46, 4, -127, -7, 22, 15, 13, 29, 14, -102, 121, -1, -77, -4, 36, -3, -121, 3, 8, 29, -57, 28, -79, 32, -20, -94, 3, -73, -60, -1, -31, 127, -26, 0, -25, -123, -5, -13, 45, 26, 11, 7, -5, 2, 37, 31, -20, -104, 12, 16, -29, -10, 3, -2, -22, -5, -4, 1, 8, -48, 127, -35, 3, -105, 122, 115, -19, -127, -4, 83, -12, -14, -70, -20, 47, -35, 17, 2, -7, 2, 16, 5, 124, 17, -100, 24, 29, -4, 30, 40, -12, -103, -7, -4, 20, 5, -14, -6, 122, 11, 20, -5, -19, -72, -5, -41, 9, -3, 9, 3, -41, -16, 32, 127, 0, 0, 21, 24, 11, -32, 9, -98, -23, 68, -6, -11, 6, -20, 8, 18, -9, 4, 30, -37, -5, 40, -2, -30, -78, 117, 9, -9, -2, -8, -22, -49, -9, 5, -24, -25, -127, 5, 4, -72, -127, 25, 127, 10, -8, 7, -2, -54, -20, 20, -5, 40, 8, 9, -51, -27, 18, 10, -127, -107, 84, -37, 3, -10, 10, 5, 2, -43, 37, 0, 35, 127, 0, -22, 68, 25, 10, -23, 3, 90, -5, 18, -13, 27, 121, -108, -3, -6, 7, -127, -108, 54, 41, 2, 18, -10, 74, 7, -68, 9, 11, -9, 4, -5, -19, 18, -127, 29, -18, -31, -105, -48, 5, 64, 127, 5, 8, 15, 24, -37, -20, 5, -4, -115, 2, 25, -6, 10, 20, 23, -37, 22, -11, 127, 13, 4, -4, -1, 0, -14, -7, -24, 72, -9, 0, 8, 9, 48, -15, -41, 4, 6, -127, 19, 46, -1, 52, -26, -1, -6, -30, -108, 34, -4, 13, 127, 19, 127, 7, -14, -25, -11, 23, -97, -73, 31, 14, -95, 16, 2, 25, -3, -45, 5, -10, -23, 2, -10, -2, -2, 4, 10, -2, -6, 10, 6, -12, 0, -50, -7, -97, 3, -7, -7, -4, -6, -5, 62, -4, -4, -2, -3, 1, 8, 13, -17, -6, 20, -6, -20, -5, -3, 1, 7, -6, -47, 2, -5, -5, 12, -3, -6, -10, -4, -3, 3, -6, -38, -8, -3, -11, -4, -16, -8, 14, -6, -24, 83, -8, 51, -3, -9, -7, -123, -3, -5, 2, -32, -8, -7, -5, -5, -8, -36, 6, -8, -16, -7, -3, -6, 1, 2, -4, -5, -21, -4, -9, -6, -25, 0, -6, -94, -38, 23, -9, -5, 5, 14, -5, -87, 1, -1, -8, 4, -4, -45, -104, -6, -9, -4, 85, -33, -12, 1, -29, 7, -5, -6, 0, -4, -20, -2, -33, -53, -2, 2, -1, -3, -9, 4, 2, 7, -6, -44, -5, 14, 30, 5, 68, -7, -6, -12, -19, -1, -13, -4, -8, -7, 2, 3, -33, -33, -2, 27, 1, -5, -4, -7, -1, -10, 20, -10, -15, -5, -10, 5, -5, 7, 19, -31, 9, -39, -5, -41, -4, -55, -2, 1, -69, 2, 6, -19, -2, 5, -6, 2, 92, -3, -37, -4, -26, -6, -101, -9, -15, 4, -22, 9, 1, 3, -11, -15, -3, -58, -5, -3, -7, 38, -7, -3, -9, -8, -4, 5, 51, -14, -1, -6, -5, -1, 3, -1, 8, -38, 3, 20, -16, -6, -11, -8, -1, 1, -8, -10, -5, 1, 64, -5, 5, -76, -26, 40, -5, -93, -3, 25, -2, -7, -13, 3, 2, -22, -12, -5, -5, -9, -8, -8, 30, 2, -33, -2, -1, 0, 17, -16, -8, -15, -6, 15, -6, 0, -8, -13, 79, 3, -1, -7, 2, 0, -5, 2, -6, 11, -4, 1, -110, -7, -26, 32, 13, -2, 16, 12, -9, -3, -6, -15, 7, 16, -2, -11, 9, -12, -7, -1, -4, -10, -73, -12, 1, -10, 1, -3, -4, 47, 7, -4, 1, -9, -5, -10, 0, -6, -13, -15, -88, -3, 4, -56, -67, 8, 87, -6, -4, -7, -8, -13, 3, -11, -5, 4, -6, -4, -18, -2, -20, -1, -96, 2, 21, -29, 2, -6, -5, -15, -3, -8, -2, -11, 6, 34, -1, -15, 28, -25, -3, -3, 1, 21, -5, -5, -5, -3, 83, -7, -5, -7, -4, -82, -23, 30, -5, -11, 6, 0, -4, -7, 6, -4, 0, -5, -4, 11, -2, -7, -96, -28, -1, -15, -61, 3, -3, 57, 23, -10, 8, -9, -2, -2, -1, 1, -9, -45, 0, 8, -4, -6, 0, 0, -45, -1, -3, 71, -38, 8, -2, -4, -3, -7, -7, -7, 21, 5, -4, 5, -1, -2, 3, -44, 3, 6, -88, 12, 15, -3, 91, -5, 7, -2, -58, -6, -31, -5, 4, -6, -12, 52, -1, -4, -53, -5, -29, -74, -37, -1, -2, -15, 5, -9, -2, -3, -6, 4, -11, 32, 5, 13, -3, 11, 1, 8, 0, 0, 1, 1, -10, -8, 2, -2, 25, 3, -2, -12, -1, -1, -1, -18, -3, -2, -2, -3, 1, -4, 5, -4, -4, 7, 0, -8, -4, -3, 0, -5, -3, -7, 2, -3, -1, -22, -5, -5, -2, -2, -1, -1, -6, 28, -2, 0, 1, 2, -18, -6, 8, -5, -1, -31, -2, -19, -1, -5, -4, -57, -3, -3, 3, -8, -4, 8, 7, 0, -2, -9, 2, -1, -10, 2, -1, -12, 2, -1, 2, 0, 21, -1, -2, -4, -1, -3, -3, 33, -15, 16, 3, -2, -16, -4, -3, -42, 1, -1, -4, 3, -2, -53, -42, -2, -2, -1, -22, 11, -7, 2, 0, -7, -2, -3, -1, 1, -12, -4, -7, 17, -6, 3, -1, -4, -3, 0, 3, -3, -1, -35, -2, 9, -6, 14, -22, 1, -4, 11, -11, -6, 0, -1, -5, 3, 1, 1, -27, -16, -3, 6, 1, -3, -2, -1, -1, -5, -27, 4, 11, 2, 5, 1, -2, 0, -10, -13, 2, 20, -6, -30, -4, -44, -3, 3, 21, 2, 5, 2, 4, 0, 3, 11, -32, 4, -53, 0, -13, -3, 27, -4, -8, -2, -6, 7, 30, -4, -5, 8, -3, -54, 13, -1, -6, -29, -6, -1, -3, -9, -1, 2, 29, -1, -4, 0, -2, -2, -2, 2, 2, -24, 9, 4, -10, -4, -3, 1, 5, 2, -1, -2, -4, 5, -10, 5, -2, -9, 42, -18, 2, 25, 2, -9, 1, -2, -3, 1, -2, -6, -2, -2, 2, -3, 5, -5, 2, 0, 15, -2, -3, -1, 5, 18, -2, -1, -2, 8, 1, -2, -2, -3, -30, 0, -1, -6, 2, -7, -4, 1, -3, -2, -3, 1, -21, 3, -14, -5, -12, -2, -7, 1, -2, -2, -2, 20, 1, -5, -12, -1, 3, 11, -4, 0, 4, 6, 17, 0, 2, 5, 1, -2, -9, -23, -1, -1, 0, -5, 2, -2, -8, -2, 27, 2, 26, -2, 2, -43, 30, -1, -33, 1, 0, 4, -4, 4, -1, -2, -6, 1, -4, -3, -4, -1, -8, -4, 25, -9, 19, -19, 1, -2, -1, -14, -2, 14, -3, -1, 3, 19, -1, 9, 16, -12, -2, 1, 1, 9, -5, -5, 8, -1, -31, 10, -4, -4, 0, 30, -11, -11, 19, -6, 2, -4, -25, -7, -7, -2, 2, -8, 2, 6, 2, -43, 30, -12, 4, 3, -20, -3, 0, 29, -9, -1, -1, 0, 2, 4, 4, -1, -6, -28, -6, 2, -1, 0, -1, -8, 11, 1, -2, -31, 17, 0, 1, -2, -4, -3, 0, 2, 10, 0, 1, 1, 3, -3, 4, -35, -1, 0, 31, 4, 7, 3, 29, -4, 1, -8, -54, 11, -4, 0, 2, 5, -6, -18, 1, 5, -37, -3, 20, -25, -27, 8, 3, 3, 1, -11, -3, 6, -1, 14, -1, 68, -47, 40, 13, 2, -4, -11, -2, 2, -1, -1, 1, -47, 51, -3, 33, 3, -2, -34, -2, 1, -4, -10, -9, -3, 5, -5, 18, 9, -2, -14, 8, 9, -8, -10, 2, 0, 2, -18, -2, -13, 2, 8, -5, 2, -35, -24, -1, -1, -2, 3, -15, 49, -2, -4, -1, 10, -17, -6, 3, 0, 15, -39, -2, -23, 5, -40, -4, -58, 0, -2, -5, -16, -4, 38, -2, 0, 3, -34, -3, 1, -10, -59, -2, -27, 3, 10, -2, -21, 24, -1, -2, 3, -21, -4, 0, 35, 24, -6, -8, -2, -108, -27, 3, -9, 34, 4, -2, 1, -2, 18, -32, -36, 16, -6, -32, 6, -3, 21, -27, 13, -2, -2, 4, -4, -9, -13, 3, 13, -10, -2, 7, 1, -8, -3, 25, 4, 3, -46, -2, 0, -6, 49, -28, 0, -7, 14, 31, -48, -2, -2, -3, 1, 2, 3, -74, 13, 16, -6, 9, 4, -2, -5, -3, 1, -80, -4, -26, -2, 18, 7, -2, 1, -25, -5, -13, 11, -42, -44, -3, -56, 9, 5, 20, 30, 2, 23, -6, 4, 3, 35, -28, 22, -58, 3, -5, 0, 39, -2, -3, 2, -10, 9, 95, 1, -8, 12, 0, -26, -17, 0, -12, -52, 32, 0, 22, -27, -4, 3, 9, -32, -7, 0, 1, 1, 5, 9, 1, 7, 31, -12, -8, -3, -4, -3, -10, 0, -3, 0, 2, -8, -21, 4, 6, -39, 58, -57, -4, 35, -4, 42, -1, -9, -35, -7, 19, 5, 16, -2, -13, 1, -5, 1, 18, -6, -11, 9, 7, -4, 7, 15, -11, -16, -3, 0, 5, -7, -8, -7, -44, 5, 0, 3, -2, 4, -3, -1, 1, 4, 0, -2, -38, -18, 23, -17, -18, 2, 5, 7, 3, -10, -1, 48, 9, -20, 25, -4, -6, 40, -2, -3, 41, -24, 21, -6, 0, 15, -5, -5, -18, -38, -1, 2, 0, -5, -8, -14, 2, 1, 23, 30, 36, -1, 3, -20, 26, 0, -38, -3, -2, 14, -3, -9, 1, 16, 27, -4, 2, 0, -17, -3, 16, -4, 37, -46, 90, -9, -1, -9, 3, 9, 0, -16, -8, -2, 4, -18, -7, 36, -5, -5, -1, 1, -3, -40, -2, -1, 36, 7, -41, -25, -36, -4, 1, 32, -1, 28, 13, -3, 0, 7, -45, 0, -8, -1, 16, -10, -2, 0, -4, -91, 36, 16, -10, -2, -54, 3, -2, 32, -34, -7, 9, -1, 3, -3, 4, -8, -2, -5, -39, -4, -6, -3, 15, -4, -45, 18, 0, -49, 6, 1, -5, -2, -9, -5, 1, 30, 3, 3, -5, 4, 0, 6, 1, -44, 5, 9, 38, -3, 60, 37, 60, -11, 2, -32, -21, -21, -5, 37, 8, 5, -4, -37, 2, 7, -20, -41, 8, -71, -21, 7, 3, 12, 5, -6, -10, 35, -17, 33, -3, -24, -127, 127, 48, -77, -19, -62, 6, 4, 6, -3, 127, -127, -37, 2, -2, -2, 14, -71, -16, 27, -17, 2, -56, -5, 14, -6, 36, 50, -42, -19, 11, -24, -47, 45, 2, -2, 6, -4, 14, -31, 24, 5, 3, 33, -127, 127, 0, 5, -2, 11, -31, 52, -13, -12, -53, -55, 19, 4, -38, -1, -17, 2, -2, -3, 14, -127, -6, -73, 8, -9, -8, 16, 0, 124, 13, 2, 19, -106, 5, 16, 2, -80, -1, -127, -2, 24, 2, -127, -6, 6, 14, 16, -67, -22, 3, 0, 7, 81, 12, 0, 0, 3, -5, -67, 108, 11, 4, -54, 3, 44, -45, -118, 22, -23, -4, 5, 10, 115, 118, 39, 2, -4, 1, 10, 4, 81, 54, -16, -19, -32, 33, 13, 6, 27, 86, 15, 4, -57, 4, -47, 41, 11, 6, -15, -18, 21, 19, -127, 2, -6, 9, -10, -11, -3, -85, 25, 127, -59, 29, 16, 7, -1, -10, 6, -48, -53, -89, -4, 127, 17, -10, -1, -7, 50, -75, 10, -122, -109, -3, -34, 16, -10, 2, 118, 21, -5, -23, 25, -9, 22, -7, 127, -37, -10, 41, -9, -2, 1, 9, -5, -44, 37, -58, 42, -55, -4, 6, -127, -32, 0, -84, -35, 127, -5, 127, -72, 1, -10, 25, -44, 10, 13, 0, -4, 39, 23, -19, 111, 81, 11, -6, -2, 4, -7, -24, 56, -6, -1, 16, -49, 7, -66, 7, -72, 118, 9, -23, -3, 2, 127, -12, -21, -52, -35, 60, -28, 16, 6, -127, 0, -5, 5, -34, -9, 4, -21, 26, 0, 36, 1, -21, -45, -5, -14, 25, -64, -9, -5, 1, -4, 15, 39, -17, -41, 1, -57, 7, 61, 1, 10, -41, -87, 29, -22, 127, -2, 36, 39, 4, -27, 10, -9, -17, -26, 127, -16, -22, 127, -3, 10, 127, -92, 54, -46, -11, 46, -4, -27, -65, 9, 6, -7, -4, -8, -24, 35, 127, 8, 127, 127, -2, 9, 4, -59, -1, 5, 13, 7, -10, -33, -3, -43, -15, 18, 127, 34, -1, 3, -42, -11, -2, 5, -6, -75, 17, -32, 2, -11, -15, -45, 17, -46, 17, -3, 33, -85, 4, -35, -22, 27, 5, -16, 2, 26, -10, 12, 127, 30, -5, -83, -127, 4, 3, 0, -94, 35, 34, 5, 8, 25, -47, 10, -39, 16, 117, -15, 5, -10, -25, 23, -1, 15, -24, -18, -99, -12, 11, 74, -7, 2, 7, 20, 14, -32, -39, 2, -4, -127, -127, 23, -9, 10, 27, 28, 58, 21, 9, -12, 0, 17, -5, -4, 8, -11, -5, 127, -59, 7, -55, 7, 3, 37, -6, -81, 15, 15, -2, 10, 11, 119, 60, -30, 0, -126, -29, -92, 26, 119, 14, -16, 9, -6, 9, -10, -43, -127, 14, -98, -78, -5, -57, 17, 19, 35, 19, 127, -17, 124, -6, -53, -23, 36, 13, -51, -6, -14, 0, 1, 2, 1, 2, -47, 51, -4, -34, 3, -3, -33, -3, 0, -4, 12, -10, -3, 6, -5, -4, 10, 6, -15, 3, 8, -4, -11, 4, -1, 3, 10, -3, -49, 6, 6, -5, 1, -35, -21, 0, 0, -3, 2, -11, -14, -4, -3, -3, 7, -16, -7, 4, -1, -4, 39, 0, -6, 7, -41, -4, -43, -3, -3, -5, -17, -5, 38, -12, -2, 2, -41, -3, 3, -10, 2, -2, -47, 0, 10, -3, -21, -28, 2, 1, 3, -24, -3, 0, -38, 33, -4, -8, -1, 104, 21, 3, -25, 30, 4, -2, 5, -2, -26, -43, -34, 15, -7, 30, -15, -13, 13, 65, 13, -4, -3, 1, -5, -11, -5, 2, -18, -8, -4, 8, 3, -9, -5, 30, 5, 4, -37, -1, 2, 35, 37, 15, 0, -8, 4, 24, -46, -3, -1, -2, 1, 2, 3, -67, 13, 12, -7, 9, 6, 0, -5, -3, 3, 87, -6, -20, -3, 18, 7, -2, 1, 26, -14, -14, -25, -43, -53, -5, -43, 7, 6, -17, 25, 6, 21, -3, 2, 1, -63, 32, 20, -84, 1, 3, -2, -47, -3, -4, 4, -15, 13, -83, 1, -7, -22, -1, -59, -14, -1, -14, -7, 32, -1, 18, -23, -4, 3, -1, 7, -5, 3, 1, 0, 3, 6, 1, 10, 30, -8, -10, -2, -1, -1, -7, -1, -3, -3, -1, -5, 9, -3, 9, -60, -28, 54, -4, -36, -5, 49, -1, -8, -30, -6, 14, 1, 0, -4, -4, 0, -2, -2, 16, -6, -18, 1, 10, -6, 8, 12, -9, -12, -3, 3, 7, -7, -7, -4, 39, 2, 0, 1, -3, 1, -3, -3, 3, 3, 0, 3, -21, -20, -4, 12, -19, 4, 2, 5, -2, -4, -2, -33, 7, -2, 22, -3, -5, 38, -1, -2, 42, -23, 24, -4, 0, 15, -5, -9, -22, 34, -3, 1, -1, -7, -6, -8, 0, -1, 27, 34, -38, -3, 3, -23, -30, 0, 28, -3, -3, 14, -4, -8, 3, 18, 28, 15, 3, -1, -17, -2, -1, -6, -42, -42, -95, -9, -1, -8, 2, -33, -1, -12, -7, 1, 5, -20, -5, -33, -5, -9, -1, 2, -2, 18, -3, 0, 36, 4, 37, -28, -43, -3, 0, -28, -8, 26, 20, -2, 3, 10, 32, -1, -7, 1, 20, -9, 2, 0, -1, 89, -43, -6, -13, -3, -52, 3, -2, 19, 47, -10, 8, -1, 4, -3, 4, -8, -4, -26, -38, -3, -6, -6, 12, -4, -48, 19, 0, 39, 4, 1, -2, -3, -9, -6, 1, 25, -2, -2, -6, 4, 0, 5, 1, -55, 5, 8, -42, -3, 54, 38, 54, -9, 2, -31, -9, -22, -1, 35, 11, 8, -1, 23, 2, 7, -15, -37, 11, -55, -21, 10, 2, 11, 6, -3, -9, 32, -17, 39, -2, -27, -14, 11, -4, 1, 2, 10, -1, 1, 2, 0, -9, -7, 7, -3, -29, 2, -1, -8, -1, 1, 0, 17, -5, -1, -3, -3, 1, -3, 6, -4, -4, 10, 1, -10, -2, -3, 0, 7, -4, -25, 1, -2, -2, 14, -6, -5, -1, -3, 0, -1, -1, -4, -1, 0, 8, 1, -16, -6, 10, -4, 1, 27, -4, 15, -1, -7, -3, -49, 0, -3, -4, -8, -4, 6, -1, -1, -1, -2, -1, -1, -10, 10, -3, 6, 3, 0, 1, -1, -11, -3, -3, -6, 0, -5, -1, -36, -21, -4, 3, -3, 14, 6, -3, -35, 5, 2, -3, 2, -2, 34, -52, -2, -5, -1, 25, -13, 15, 1, 44, -9, -2, -3, 2, 3, -9, -5, -5, -8, -9, 2, 1, 1, -2, 0, 3, -3, 1, -32, -2, 10, 24, -63, 20, 0, -2, -16, -20, -7, -1, -1, -6, 3, 1, 0, -32, -8, -2, 5, -3, -3, -2, -1, 0, -5, 28, 5, 15, 1, 7, 0, -2, 1, 18, -19, -1, -9, -5, -18, -4, -35, -1, 2, -29, 1, 4, 3, 2, -1, 5, 9, 29, 6, -61, -1, -12, -4, -39, -4, -8, 0, -3, 4, -24, -6, -6, -9, -2, -37, -8, -1, -6, -7, -8, 0, -2, 4, -2, 0, -2, -14, -5, -3, 0, 0, -2, 4, 0, -19, 7, 8, -13, -3, -3, 1, 3, 0, -1, -1, -3, 5, 14, 10, -4, -22, -1, 28, 3, -27, 2, -11, 1, -3, -1, 1, -35, -5, -11, -3, -2, -4, 1, -6, 1, -2, -21, 0, -3, -1, 1, 17, 0, -8, -2, 7, 1, 0, -3, -3, 22, -4, 2, -6, 2, -6, -2, -1, -2, -1, -3, 0, -33, 0, -23, 12, -10, -1, -7, 4, -1, 1, -1, -11, 1, 12, -15, -1, 5, 9, -2, 0, 7, 6, 4, -1, 0, -29, 0, -2, -9, 18, 1, 0, -1, -3, 2, -6, -6, -2, 29, -1, -24, 0, 1, -31, -25, -4, 22, -2, -1, 2, -5, -3, -2, -1, -9, 4, -3, -3, -8, -1, -8, -2, -27, -7, -40, -23, 1, -2, -1, 0, -2, 9, -2, 2, 0, 18, 0, -19, 14, -8, -2, 1, 0, 8, -4, -3, 6, 4, 27, 17, -3, -4, 1, -26, 12, -2, 3, -6, 1, -7, -1, -1, -6, -3, 4, -9, 3, 3, 0, 20, -30, -13, -1, 3, -21, 0, -1, -6, 17, 1, 0, 1, -1, 4, 4, -2, -6, -17, -8, 3, -3, 0, -1, -6, 6, 2, -3, 28, -12, 2, 0, -1, -2, -3, 1, 5, 7, 4, 1, 1, 3, -3, -1, -59, -1, 1, -30, 2, 13, 2, -64, -5, 2, -7, -39, 10, 0, 4, 5, 3, -2, 16, -1, 5, -48, -2, -29, -28, -19, 4, 4, 6, 0, -11, 3, 5, -1, 18, -2, 9, -20, 4, 4, -1, 1, 14, -4, -1, -1, 1, 0, -10, 10, 0, 9, 3, -6, -10, -1, -1, -1, 1, 1, -3, -1, 1, -4, 1, 8, -8, -5, 3, -2, -6, 0, -2, 0, -7, -1, 3, 0, -3, 0, -18, -3, -8, -6, -2, -2, -4, 4, 12, -1, -2, 5, 5, -8, -1, 0, -1, 1, -9, -3, -9, 2, -7, -2, -45, -2, -1, -2, -16, -2, 3, -5, -7, -11, 3, 2, -9, -5, -23, -3, -13, -3, -4, -3, 0, 13, 0, 1, -6, -20, -1, -2, 9, -116, 32, -7, -3, -15, -8, -3, -81, 7, -4, -5, 6, -1, 2, -44, -5, -7, 0, -5, 3, -4, 7, 3, -7, -3, -4, -4, -1, -6, -9, -1, 12, -6, -4, -1, -3, -5, 1, 6, 1, -2, -25, -1, 8, -4, -32, -5, -2, -1, -21, -17, -9, -1, -4, -2, -1, 0, -1, -39, -5, 8, -4, -6, -3, -3, -1, 0, -3, -18, 5, -4, -2, -3, 0, 0, 0, -7, -5, -3, 0, -6, 2, -2, -33, 8, 0, 5, 6, -8, -28, -1, 1, 1, 10, -14, 6, -120, -4, -3, -2, 10, -4, -8, -4, -11, 3, 6, -1, -14, 1, -3, -79, 5, -2, -4, -24, 3, 0, 0, -4, -2, 0, -17, -9, -4, -2, -3, -4, 3, 2, -4, -4, 10, -2, -9, -1, -4, -2, -1, 0, -3, -4, -8, -1, -12, 2, -1, -8, 9, -26, -8, 7, 1, -25, 2, -1, 3, -1, -17, -8, -14, 1, 1, -3, -10, -4, 5, -3, 13, 2, -4, -1, 2, 5, 0, 13, -1, 4, -1, -3, -1, -2, -10, -12, -2, -6, 2, 0, -5, 5, -5, 2, 0, -4, -31, -5, -21, -10, -6, -1, -7, -2, -7, -4, -3, 18, 1, -1, 5, -1, 1, 4, -2, -4, 7, 0, -34, -1, 0, -1, -7, -2, 6, -20, -3, -2, -3, -3, 2, -13, -6, -6, 26, 3, 9, -1, -1, -39, 9, -4, -5, -2, -2, 3, -6, -4, 2, -3, -1, -4, -1, -1, -15, 0, -12, -2, 9, 1, 11, -22, 0, -2, -2, -8, -4, 2, -2, -2, 1, 0, -8, 14, -10, -16, -4, 0, -3, -7, -3, -5, 9, 1, -5, -5, -7, -3, -4, 6, -20, 4, 35, -5, 2, 0, -50, 1, -7, -3, 7, -7, -6, -1, 1, -17, 10, -6, 4, 2, -9, 4, -3, -52, -22, -6, -3, -4, -4, 4, 0, -7, -1, 9, -3, -7, -2, -2, 6, -5, -43, -3, -5, -16, -25, 5, -2, -1, -3, -1, -1, 10, 0, 1, -5, 1, -2, -2, -4, -94, 1, 1, 10, 2, -4, 9, -86, -2, 3, 0, -120, 2, -10, 7, -4, 6, -4, -12, 1, 5, -95, -10, -42, -11, -20, 1, 12, 8, 0, -7, -1, 7, 1, 7, -3, -12, -28, 16, 13, 1, -1, 8, 3, 3, 3, 1, 15, -30, -1, -2, 7, 1, -2, -12, -2, -3, -1, 11, -3, -2, -3, -3, -1, 20, 2, -9, -4, -4, 2, 1, -3, -1, 0, -12, 0, 4, 5, -1, 0, 16, -16, 3, -5, -2, -1, -1, -1, -2, -1, 1, -4, -9, -1, -5, 1, -5, 13, -14, 0, -2, 4, -26, -1, -20, 0, 0, 1, 0, -4, 20, -7, -1, 5, 0, 1, 0, -3, -25, -2, -16, -1, -1, -2, 4, 3, 0, 0, -4, -16, 1, -1, 2, -63, 1, 10, -4, 8, -8, -1, -29, 21, -1, -4, 4, -1, 12, -22, -15, -5, -2, -9, 3, 5, 10, 5, -5, -3, -2, -4, 3, -6, 18, 10, 3, -2, 1, 2, 2, -4, 0, 14, 0, -3, -39, -4, 8, 3, 11, 3, 0, -2, 10, -9, -27, -3, 0, -7, -3, 1, 1, -45, -16, 15, -19, -6, -1, 3, 1, 0, -2, -21, -2, 12, -2, 15, 0, -1, 2, -14, 13, -9, -3, -14, -24, -1, -56, 10, -6, 5, 9, 1, 30, -3, 0, -4, 12, -7, 15, -72, -3, -9, -2, 14, -3, -7, -1, -7, 8, 37, 0, -7, 0, -1, -73, -3, -1, -10, -9, 22, -1, 11, -3, -4, 0, -26, -12, -2, 0, -3, -2, -9, -5, 1, 16, 21, -4, -2, -2, -1, -1, 0, -2, 0, -3, -4, 5, -8, 4, -1, -39, -3, -23, -2, 8, 1, -3, -1, -2, -1, -1, -18, -6, -11, -3, -19, -5, -7, -4, -21, -1, 12, -1, -4, 1, 1, -8, -8, -6, 0, 8, -1, -1, -3, -4, -11, -4, 0, 1, 2, -3, -2, -5, -2, 2, -1, 2, -49, -13, -29, -2, 13, 1, 2, 3, -2, -1, 4, 7, 3, 2, 12, 0, -5, 19, -1, -4, 19, -18, 8, 1, -1, 21, -1, -2, 6, -17, -1, -1, -1, -2, 2, -5, 5, -4, 32, 23, 7, 1, 0, -28, 4, -1, -2, -2, -2, 3, -3, -4, 2, -2, 15, 1, -4, -3, -5, -5, -1, -1, 11, 6, 27, -16, -1, -2, 2, -8, -4, 2, -2, -3, 1, -16, -3, 22, -2, -15, -1, -5, 0, -11, 0, -1, 18, 1, -10, 6, -23, -2, 0, 7, -4, 21, -127, -3, -1, 0, -36, -2, 3, 2, 14, -7, 3, 2, 3, -51, 9, -12, 4, -1, -7, 2, -1, 26, -15, -2, -1, 0, 1, 2, 0, -1, -4, 20, -23, 2, -3, 3, 0, -2, 13, 3, -1, -15, -17, 3, -1, -1, -3, -3, -1, 4, -5, 2, -9, 3, -1, -3, 1, -59, 1, 0, 9, 7, 16, 13, -6, 2, 1, -17, -56, 8, 10, 20, 3, -1, -3, 4, 0, 0, -36, -24, 6, -41, -18, -1, -8, -2, 2, -4, -6, 19, -1, 11, 0, -8, -50, 80, 17, -19, 6, 2, -2, -4, 1, 4, 7, -88, -45, -10, -2, 3, -7, -8, -4, 6, -3, -15, -1, -3, -3, -5, 0, -9, -2, -11, -5, 0, -21, 27, -9, -4, 2, -9, -6, -1, 3, -5, -1, 10, -60, 39, -5, -2, -3, 0, -9, 0, -7, -2, -5, -14, 9, -8, -8, -4, -15, -6, -7, 1, 0, -88, -8, -59, -2, -9, 5, -16, -3, 72, -4, -6, 7, -20, 0, -8, -4, -23, 0, -49, 4, 9, -5, 9, -1, -3, -9, -1, -31, -3, -8, 9, -87, -12, -6, -3, 1, -1, 5, -72, 56, -3, -7, -23, -6, -50, -74, -77, 1, 8, -1, 3, -12, 51, 3, 8, -3, -4, 2, 2, -17, 30, 1, -13, -12, 1, 2, -8, -7, 0, 31, -5, -5, -46, -3, 2, -3, -38, -3, -3, -4, -19, -17, -76, -11, -1, -8, -7, 0, 2, -50, -15, 6, 5, 1, -2, -5, -5, 0, -9, 24, 3, -18, -6, 80, 5, -7, 7, 1, 18, -4, -2, -68, -66, -4, -87, 15, 4, -9, 38, 0, 1, 9, 4, 2, 1, -5, 26, 16, -4, -20, -1, -6, -5, -13, -9, -17, 19, 18, -3, -32, -8, -3, -65, 3, -2, -41, -4, 41, -2, 17, -28, -4, 2, 45, -20, -8, -5, -5, -8, 9, 2, 7, 11, 46, 24, -1, -7, -8, -3, 2, 3, -5, -12, -11, -6, 9, 10, -2, -65, -2, 8, -6, 1, -6, 17, -1, -6, -17, 2, 21, -12, -14, -1, -58, -9, -4, -8, -41, -4, 0, -2, 0, -2, 13, 9, -13, -20, -9, 15, -9, 5, -7, -10, -2, 0, -5, 4, 3, 2, -3, -20, -3, 20, -4, -2, -95, -20, -31, -17, 15, 0, -6, 14, -4, -2, 9, 2, 5, 2, 24, -10, 2, 76, 0, 4, 70, -45, 27, -4, -2, -67, 1, -4, 7, 1, 1, -3, 1, -7, -6, 8, 11, -4, 111, 62, -2, -2, 4, -54, -5, -3, 2, -3, -2, -21, -8, -19, -2, -11, 45, 2, -7, -5, -23, -5, -19, -4, -4, 14, 10, -15, 2, -5, -2, -22, -5, -10, 5, -14, 10, -17, -6, -1, 4, -23, 2, 0, 1, 6, -6, -10, 64, -1, -1, -12, -78, -5, -1, -1, -49, 15, 4, -15, 2, 5, -11, -13, -6, -6, 50, -7, 1, 9, -3, 3, -2, -26, -2, 1, -68, -4, -3, 38, 3, -6, 4, -9, 4, -5, 3, 4, -4, -44, -77, 12, -5, -2, -6, 7, 5, -3, -4, -13, -21, 3, 2, -1, -3, -9, -4, 31, 10, 0, -10, 5, 3, 0, 1, -73, 1, 3, 2, 14, -3, 54, 107, -11, 7, -70, -71, -4, -4, 64, 3, -5, -5, 1, -5, -1, -68, -85, 10, -81, -24, -8, -15, -5, 4, -14, -4, 77, -4, 19, -10, 20, -16, 15, 15, -27, -1, 8, 2, 4, 3, 0, 16, -29, 1, -4, -10, 3, -1, -8, -1, -3, -1, 5, -5, -1, -4, -3, 3, 18, 3, -10, -5, -1, -1, 0, -3, -2, 1, 2, -2, -17, 4, -1, 0, 19, -19, 4, -5, -1, 0, -2, 0, 1, -2, 1, 2, -8, 2, -5, 1, -4, -6, 13, -3, 12, 5, -28, -2, -20, 1, -1, -1, -6, -3, 24, -3, -1, 6, -5, -2, -1, -8, -2, -1, -27, 0, -3, 0, 5, -11, -1, 0, -4, -12, 3, -1, -5, -53, -12, 8, -6, -7, 10, -1, -34, 23, -4, -4, 8, -1, -4, -17, -19, -2, -2, 6, -9, 16, 12, -5, -6, -1, -1, -3, 2, -3, 14, 11, -4, -1, 4, 3, 1, -5, -2, 12, 1, -1, -32, -4, 7, -1, 36, -2, 0, -2, 13, -6, -30, -1, -2, -5, -2, 0, 1, -44, -17, 16, -18, -2, 1, 0, 1, 0, -3, 37, 1, 13, -1, 14, -1, -2, 1, 3, 8, -8, 6, -14, -22, -1, -50, 10, -4, 5, 10, 1, 31, -2, 0, -4, -21, 7, 14, -63, -2, -8, -2, -11, -3, -3, -3, -7, 7, -28, 1, -4, -10, -2, -52, 4, -2, -10, 4, 21, -1, 9, 1, -3, 1, -7, -4, -1, -1, -2, -1, -10, -3, 1, 13, 17, -2, -2, -2, -2, -1, -1, 0, 0, -3, -3, 6, 1, 3, -2, -48, 15, 23, 2, -10, 1, 7, 1, -2, -1, -1, -7, -4, 3, -2, -21, -4, -6, -3, -22, 2, -1, 0, -6, 1, 4, -8, -7, -5, -2, 6, 3, 1, -3, -5, 9, -4, 1, 3, 3, -4, -3, -7, -2, 3, 0, 2, -51, -12, -22, 3, 13, 1, -1, 11, -1, 0, 5, -19, 1, -4, 13, 0, -3, 19, -2, -3, 16, -15, 17, 0, 1, 9, 1, -1, 13, 18, 1, -1, -1, -3, 1, -3, 6, -3, 26, 18, -10, 0, 1, -8, -3, -1, 5, 0, -1, 5, -4, -1, -3, -3, 17, -4, -5, -1, -10, -7, 13, 0, -11, 2, -48, -9, 0, -2, 2, -17, -4, 1, -1, -3, 0, -15, -2, -23, -3, -14, -1, -3, -1, 10, 0, -1, 20, 3, 5, 8, -20, -2, 1, -5, -11, 21, -42, -3, -2, 0, 13, -1, 1, -1, 12, -7, 0, 1, 0, 31, -11, -8, 1, 2, -11, 1, 0, 32, 7, -2, 0, -1, -2, 0, 1, 2, -5, -20, -23, 0, -2, 1, 1, -4, 17, 1, -1, 14, -2, 3, -1, -1, -2, -2, 2, 5, -4, -1, -8, 3, -1, -3, 3, -51, 1, 0, -12, 6, 12, 14, 35, 4, 2, -20, -44, 9, 12, 20, -4, 0, -1, -3, -2, 2, -37, -23, 18, -51, -16, -2, -8, 0, 2, -4, -5, 21, -1, 9, -1, -7, -14, 7, 3, -22, 1, 16, -4, 1, -1, 3, 0, -11, 8, 0, -5, 1, -5, -10, -2, -3, -1, -2, 3, -4, -2, 0, -5, -1, 5, -8, -5, 2, 1, -8, -1, -1, 1, 2, -1, -24, 3, -5, -1, -52, -2, -6, -7, -2, -2, -3, 2, -2, -1, -1, 4, 4, -3, 0, 0, -1, -2, 11, -2, 15, -1, -6, -3, -35, -4, 0, -2, -17, -3, 5, 1, -5, -6, 4, -3, -10, -3, 3, -3, -15, -2, -4, -3, -1, -7, 0, 1, -7, -15, -4, -3, 3, -116, 22, -8, -1, 14, 8, -3, -91, 5, -2, -4, 6, -2, -25, -38, -1, -6, -1, 5, -4, -15, 9, 10, -8, -5, -3, -5, -1, -7, -9, 2, -10, -7, -4, -1, -2, -4, 1, 9, 1, -3, -25, -1, 10, 11, 9, 7, -1, -2, 7, -11, -8, 0, -3, -2, 0, -1, -1, -42, -11, 5, -7, -4, -2, -1, -1, -1, -4, 1, 4, -6, -2, -5, 1, 0, 1, 15, -11, -3, -2, -4, -11, -2, -32, 6, 0, -3, 5, -9, -18, 1, 2, 1, -10, 8, 7, -127, -4, -8, -1, -10, -3, -10, -3, -11, 1, -19, -2, -14, -8, -1, -82, -2, -2, -4, -1, 4, 0, 2, -4, -2, -6, -20, -6, -3, -1, -3, -5, 1, 0, -3, -5, 9, 5, -8, -1, -5, -2, 2, -1, -4, -3, -10, -2, -3, 1, -2, -9, -2, 25, -10, -6, 2, -31, 1, -1, 2, 0, 19, -5, -10, 0, 3, -3, -8, -4, 3, -4, -5, -1, -6, -2, 6, 7, 0, 17, -1, 5, -5, -5, -1, -4, 15, -10, -5, -5, 3, -1, -4, 5, -5, 0, 2, -5, -40, -7, -28, 13, -5, -1, -6, -5, -5, -4, -5, -22, 3, 3, 7, -1, 3, 3, -2, -5, 7, 0, 17, 1, 0, 22, -7, -3, 7, 13, 2, -2, -1, -3, 3, -11, -7, -5, 27, 8, -7, -3, -1, -58, -14, -4, 5, -2, -2, 2, -4, -4, 2, -6, 1, -2, -1, -1, -11, 0, -15, -4, -8, -3, -21, -27, 0, -2, -1, -11, -4, 4, -3, 0, 2, -2, -8, -26, -8, -14, -4, 0, -1, 7, -3, -6, 7, -2, 5, -6, -8, -3, -5, -5, -23, -33, 48, -4, 2, 0, 8, -1, -3, -1, 6, -6, -4, 1, 0, 10, -12, -5, 6, -1, -8, 3, -3, -48, 18, -5, -4, -5, 2, 4, 0, -8, -1, -55, -2, -5, -1, 0, 7, -3, -40, -3, -4, 17, 43, 4, -4, -2, -1, -1, -2, 7, -3, 1, -4, 2, -1, -1, 1, -93, 1, 0, -7, 4, 1, 10, -75, -4, 3, -4, -120, 2, -8, 5, -4, 6, -7, 10, 3, 5, -76, -8, -19, -20, -20, 2, 8, 10, 0, -6, -4, 6, 0, 4, -2};

float bias_raw[480]={0.18961742520332336, -0.6650076508522034, -0.19641166925430298, 0.1267043948173523, -0.9735183119773865, 1.9071122407913208, -0.7214220762252808, 0.08136838674545288, 0.12408125400543213, -0.8091599345207214, 1.0460246801376343, 0.7545754909515381, -0.45630934834480286, 0.2749480605125427, -0.45943984389305115, -0.550951361656189, 1.8725327253341675, -0.1355728805065155, 0.21697381138801575, 0.47397831082344055, -1.2652788162231445, 0.7032513618469238, -0.379697322845459, 1.818364143371582, -0.14636924862861633, 0.3801846504211426, 0.7295811772346497, -3.478121757507324, -3.1385741233825684, 1.8051741123199463, 0.05101901292800903, -0.6065320372581482, 0.49746596813201904, 1.070920467376709, 0.46671149134635925, 0.4701268672943115, -0.1034466028213501, 1.4172292947769165, 0.464273601770401, -0.1919981837272644, 0.6466339230537415, -2.0634031295776367, -0.3752281069755554, -0.026772260665893555, 3.4064173698425293, -0.58601313829422, 0.6633782386779785, -0.25303271412849426, -0.23713915050029755, -0.06079226732254028, 0.22728797793388367, 1.765055537223816, 0.1942657232284546, 0.4233767092227936, 0.45882609486579895, 0.03353080153465271, 1.307154893875122, 0.5203235745429993, 1.1896634101867676, -0.634709358215332, 0.4762688875198364, -0.5539459586143494, -0.4378345310688019, -0.20130306482315063, 0.7258793711662292, -0.40024247765541077, -0.3056570887565613, -0.5403205752372742, 0.6993036866188049, -0.2843482494354248, 0.1509455293416977, 0.7803688049316406, -0.6492475271224976, 0.38604986667633057, -0.4304165840148926, -1.58562433719635, 0.3945746123790741, 2.2257237434387207, 1.08795166015625, -0.6064975261688232, -0.2834220826625824, 0.5390840768814087, -0.8634200096130371, -0.11073395609855652, -0.6736791729927063, 1.5706639289855957, -0.5943610668182373, 0.49309036135673523, 0.8453745245933533, 1.6426141262054443, 0.22958418726921082, -0.46902185678482056, 0.02081426978111267, 0.638506293296814, 1.43800950050354, 0.021976947784423828, -1.0346977710723877, -1.490864634513855, -2.4128684997558594, -1.2061847448349, -0.18862497806549072, 1.6074061393737793, 0.4525097906589508, 0.3432671129703522, 2.149942398071289, -0.9261765480041504, -0.4174960255622864, 0.261056125164032, 1.880561351776123, -0.08514571189880371, 2.4178578853607178, 0.80558180809021, -0.6229549646377563, -0.6548705101013184, 1.0138413906097412, -0.5541062951087952, 0.9177022576332092, 1.3287503719329834, -0.8182745575904846, 1.2275493144989014, 0.06097680330276489, 0.06059318780899048, 0.1188323050737381, -0.22772826254367828, -0.19817408919334412, -0.32293379306793213, -1.0145272016525269, -2.944960594177246, -0.6829840540885925, 0.2840803563594818, 0.8425850868225098, -1.869748830795288, -0.16267149150371552, -0.9441946744918823, -0.2331085503101349, -0.6459857821464539, 1.1361331939697266, -0.12228918075561523, 0.7642760872840881, 0.6575478315353394, 1.431239366531372, 0.22823525965213776, -0.5232570171356201, -0.722545862197876, 0.2505819499492645, 0.7870498895645142, 0.49134427309036255, -0.8522124886512756, -0.6843152046203613, 0.3448248505592346, 0.232227623462677, -0.26087114214897156, -0.11011955142021179, 3.037435293197632, 5.040363788604736, 0.5024160146713257, -0.4479954242706299, 0.2753106951713562, 0.6779340505599976, 0.06406038999557495, 1.2665119171142578, 0.23564639687538147, 0.014714360237121582, 1.833411693572998, 0.31241196393966675, 1.3167638778686523, 1.020218849182129, 1.8460111618041992, -0.10750427842140198, -0.9065425992012024, 0.3850213587284088, 0.5548372268676758, 1.7318096160888672, 1.5883859395980835, 0.5538597106933594, -1.559097409248352, -0.6723548769950867, -0.5176575779914856, 0.7899580597877502, 0.27231165766716003, 0.5911893248558044, 0.2138405740261078, 0.05752483010292053, -0.8720695972442627, -0.31020206212997437, -2.9489569664001465, -0.942892849445343, 0.00491208303719759, 0.8902690410614014, 1.004183292388916, 0.944722056388855, -0.7385437488555908, -1.0188111066818237, 1.9057462215423584, -0.10578268766403198, -0.6132399439811707, -0.002981811761856079, -0.33438047766685486, 0.3561955690383911, -0.5438306331634521, -0.3826357126235962, 1.8489243984222412, -1.6869090795516968, 1.3022822141647339, -3.4669432640075684, 1.0621984004974365, 0.27191367745399475, 0.09752193093299866, 1.4535608291625977, 0.4714856743812561, 0.7647133469581604, -0.32220226526260376, 0.6798574924468994, -0.0007974579930305481, 0.43272852897644043, 0.36027032136917114, 1.0406968593597412, 0.4393324851989746, 7.069037437438965, 3.430868148803711, -0.5752122402191162, -0.19099165499210358, -0.3674905002117157, 0.2207697331905365, 0.0445830374956131, -3.2472639083862305, -3.3801281452178955, 3.352559804916382, 1.0608150959014893, -0.8824688792228699, 1.147542953491211, 0.13254311680793762, 0.8913916945457458, -0.3028268814086914, 0.1904589682817459, 1.5718328952789307, -0.006758153438568115, 0.32094791531562805, -0.19955986738204956, -0.3629637062549591, 1.513028621673584, 0.9213380813598633, 2.118907928466797, -0.12696075439453125, 8.726319313049316, 1.1602684259414673, 1.0136878490447998, 3.9986109733581543, -0.4754481017589569, 1.8120570182800293, -0.8957350254058838, 1.9802298545837402, 0.5820047855377197, 1.2646021842956543, 0.8063927888870239, 3.1682446002960205, 0.6234239339828491, -0.5118221044540405, -0.056493014097213745, -0.6988160014152527, 0.014780133962631226, 0.08432535827159882, 0.21266591548919678, 0.8041123151779175, 0.7027314901351929, 0.9547010660171509, -0.47478193044662476, -1.1415090560913086, 1.2967485189437866, 1.016324758529663, -3.165369987487793, 0.44266027212142944, 1.4116325378417969, 0.06761559844017029, 1.257572889328003, -1.98348069190979, 0.3167988955974579, 0.5017879009246826, 0.1355157494544983, -0.5259544849395752, 0.5501399636268616, -1.6417999267578125, -0.5599724054336548, 0.7718998193740845, 0.942115306854248, 0.1971154361963272, 1.099952220916748, -0.12531444430351257, 0.5061472058296204, 0.40656059980392456, -0.26160743832588196, 0.5945989489555359, 0.0885317325592041, -1.1625664234161377, -0.7563024163246155, 0.14695703983306885, 0.2708594501018524, -1.5502192974090576, -0.27498698234558105, 0.366424560546875, 0.9783176183700562, 1.2775195837020874, 1.5224698781967163, 0.6902567744255066, 0.6451719999313354, 0.04119002819061279, 1.0218963623046875, -0.23484650254249573, -0.379835844039917, -0.22314895689487457, -0.32504767179489136, -0.37557747960090637, 0.11018111556768417, 2.1418910026550293, 1.4054428339004517, 0.30121979117393494, 0.7400374412536621, 1.5084013938903809, 0.8617038726806641, 0.43245065212249756, -0.6727667450904846, -0.7319267988204956, 0.4611133337020874, 1.3102067708969116, 0.7288466691970825, 2.105921983718872, -1.0873348712921143, 0.5036106705665588, -0.17408552765846252, -0.5892719626426697, -0.20799344778060913, -0.6760216355323792, 0.2710478901863098, 2.3263328075408936, 0.7932100296020508, -1.0925323963165283, -0.19456163048744202, -0.7124245166778564, -0.42484650015830994, 0.43742677569389343, -0.5515931844711304, 0.5818992257118225, 1.0104321241378784, 0.8289055824279785, -0.7201935648918152, -0.04871103912591934, -3.5562734603881836, -0.18340575695037842, 0.46075424551963806, 0.7940678000450134, 0.5167246460914612, -1.0854917764663696, 0.31609052419662476, -0.45864027738571167, -0.2328154444694519, 1.4515413045883179, 0.7451462745666504, 2.3137855529785156, 0.6140591502189636, -0.3386419415473938, -0.5541940331459045, 0.13089963793754578, 1.7199040651321411, -0.6919615864753723, 0.3370460271835327, -1.1227176189422607, 0.9572702050209045, 0.23447605967521667, -0.718969464302063, 1.1248921155929565, -1.2798960208892822, 0.48334118723869324, 0.35271361470222473, -0.15316033363342285, 1.1185718774795532, 0.7040845155715942, -0.5292866230010986, -0.46352440118789673, -0.3575911819934845, -0.545298159122467, 0.6739218235015869, -0.4857174754142761, 0.5089645385742188, 0.5474534630775452, -0.7847736477851868, 1.1013398170471191, 0.6135996580123901, -0.4286534786224365, 0.512126088142395, 1.2725805044174194, -0.948466956615448, 0.27833622694015503, -0.2913944125175476, 0.6082807183265686, -0.1433003842830658, -0.7786058783531189, 0.17630374431610107, -0.04212072491645813, 2.634589672088623, 1.371683120727539, 1.4669597148895264, -0.5344802141189575, -1.1315839290618896, 1.9628976583480835, 4.119298458099365, 0.96616530418396, 1.81673264503479, 0.016970813274383545, 0.5050389766693115, 1.421919584274292, -0.28391367197036743, -1.107023000717163, -0.04237684607505798, -0.37057140469551086, -0.3463513255119324, 2.3000688552856445, 0.1532224714756012, 0.5859529376029968, 0.014406204223632812, -0.45412465929985046, -1.8821065425872803, 0.6975880265235901, 1.038127064704895, -3.3784632682800293, 0.7098738551139832, -2.7982633113861084, 0.47095853090286255, -0.3446669578552246, -0.41103389859199524, 2.9903016090393066, 0.3981179893016815, 0.0510009229183197, 0.0598374605178833, -0.16085776686668396, 1.240334391593933, 0.3840385675430298, -0.5486226081848145, 0.9376534819602966, -0.793660581111908, 0.4544121026992798, 1.0560276508331299, 1.345853328704834, 1.215924620628357, 2.4788033962249756, 1.4328259229660034, 1.4959306716918945, 1.100822925567627, -0.6379841566085815, -1.2424521446228027, 2.8934526443481445, -0.1657206267118454, 1.5429291725158691, 1.104053258895874, -0.9743999242782593, -0.1560477912425995, 0.5350379943847656, 0.9355501532554626, 2.0281219482421875, -0.5437868237495422, -0.38983428478240967, 0.6151766777038574, -0.016475945711135864, -0.6546179056167603, -0.7069006562232971, 1.6995251178741455, 1.4064021110534668, -0.4881456792354584, 2.727726697921753, 0.6594467163085938, 1.0175718069076538, -0.16624337434768677, 2.0504331588745117, 1.435259222984314, 0.6557939648628235, -0.2189887911081314, -0.3058960437774658, -0.504931628704071, 1.5321444272994995, 0.1942889392375946, 0.7223882675170898};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
int stride_width=1;
int stride_height=1;
TfLiteFusedActivation activation=kTfLiteActNone;
int dilation_width_factor=1;
int dilation_height_factor=1;
const int filter_dims_size=4;
const int filter_dims_raw[4]={1,5,5,480};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={480};
TfLitePadding paddings=kTfLitePaddingSame;
TfLiteType filter_type=kTfLiteInt8;
TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;

struct OpData {
  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;
  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // Hybrid per channel temporary tensors.
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t input_offset_index;
};

void ExtractDepthConvParams(TfLitePadding padding, int stride_width, int stride_height,
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteDepthwiseConvParams* data_params) {
  // TfLiteDepthwiseConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetDepthConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params, char* tensor_data,
                       TfLiteAffineQuantization* quant_struct, size_t bytes_size,
                       TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}
void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to carry information from Prepare() to
  // Eval().
  return new OpData;
}

void Free(TfLiteContext* context, void* buffer) {
  delete reinterpret_cast<OpData*>(buffer);
}

TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;

  // TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;
  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  const TfLiteTensor* bias = nullptr;

  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));

  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);
  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);
  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);
  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);

  const TfLiteType data_type = input->type;

  const TfLiteType filter_type = filter->type;
  const bool is_hybrid =
      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;
  TF_LITE_ENSURE(context,
                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||
                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);
  if (!is_hybrid) {
    TF_LITE_ENSURE(context,
                   filter->type == data_type || data_type == kTfLiteInt16);
  }

  if (data_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }

  // Filter in DepthwiseConv is expected to be [1, H, W, O].
  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));
    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (data_type == kTfLiteInt16) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);
    }
    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);
    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),
                      SizeOfDimension(bias, 0));
  }

  int channels_out = SizeOfDimension(filter, 3);
  int width = SizeOfDimension(input, 2);
  int height = SizeOfDimension(input, 1);
  int filter_width = SizeOfDimension(filter, 2);
  int filter_height = SizeOfDimension(filter, 1);
  int batches = SizeOfDimension(input, 0);

  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;

  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  // Note that quantized inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (data_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }

  if (is_hybrid) {
    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE_EQ(
        context, affine_quantization->scale->size,
        filter->dims->data[affine_quantization->quantized_dimension]);

    int temporaries_count = 0;
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;
    data->input_offset_index = temporaries_count;
    if (data->input_offset_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_offset_id));
    }
    ++temporaries_count;

    TfLiteIntArrayFree(node->temporaries);
    node->temporaries = TfLiteIntArrayCreate(temporaries_count);

    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    const int batch_size = SizeOfDimension(input, 0);
    int scaling_dims[1] = {batch_size};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    node->temporaries->data[data->input_offset_index] = data->input_offset_id;
    TfLiteTensor* input_offsets;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->input_offset_index,
                                       &input_offsets));
    input_offsets->type = kTfLiteInt32;
    input_offsets->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1, scaling_dims)) {
      TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
      input_offsets_size->data[0] = batch_size;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                       input_offsets_size));
    }
  }

  TfLiteIntArray* outputSize = TfLiteIntArrayCreate(4);
  outputSize->data[0] = batches;
  outputSize->data[1] = out_height;
  outputSize->data[2] = out_width;
  outputSize->data[3] = channels_out;
  return context->ResizeTensor(context, output, outputSize);
}

TfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,
                                    const TfLiteTensor* input,
                                    const TfLiteTensor* filter,
                                    int16* depth_multiplier) {
  int num_filter_channels = SizeOfDimension(filter, 3);
  int num_input_channels = SizeOfDimension(input, 3);
  TF_LITE_ENSURE(context, num_input_channels != 0);
  TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);
  *depth_multiplier = num_filter_channels / num_input_channels;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,
                       TfLiteDepthwiseConvParams* params, OpData* data,
                       const TfLiteTensor* input, const TfLiteTensor* filter,
                       const TfLiteTensor* bias, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output));
  } else {
    optimized_ops::DepthwiseConv<float, float>(
        op_params, GetTensorShape(input), GetTensorData<float>(input),
        GetTensorShape(filter), GetTensorData<float>(filter),
        GetTensorShape(bias), GetTensorData<float>(bias),
        GetTensorShape(output), GetTensorData<float>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                           TfLiteDepthwiseConvParams* params, OpData* data,
                           const TfLiteTensor* input,
                           const TfLiteTensor* filter, const TfLiteTensor* bias,
                           TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));
  if (kernel_type == kReference) {
    reference_ops::DepthwiseConv(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output));
  } else {
    optimized_ops::DepthwiseConv<uint8, int32>(
        op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
        GetTensorShape(filter), GetTensorData<uint8_t>(filter),
        GetTensorShape(bias), GetTensorData<int32_t>(bias),
        GetTensorShape(output), GetTensorData<uint8_t>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                                     TfLiteDepthwiseConvParams* params,
                                     OpData* data, const TfLiteTensor* input,
                                     const TfLiteTensor* filter,
                                     const TfLiteTensor* bias,
                                     TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.input_offset = -input->params.zero_point;
  op_params.weights_offset = 0;
  op_params.output_offset = output->params.zero_point;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  TF_LITE_ENSURE_STATUS(ComputeDepthMultiplier(context, input, filter,
                                               &op_params.depth_multiplier));

  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output));
  } else {
    optimized_integer_ops::DepthwiseConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int8>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<int32>(bias), GetTensorShape(output),
        GetTensorData<int8>(output),
        CpuBackendContext::GetFromContext(context));
  }
  return kTfLiteOk;
}

TfLiteStatus EvalQuantizedPerChannel16x8(
    const TfLiteDepthwiseConvParams* params, const OpData* data,
    const TfLiteTensor* input, const TfLiteTensor* filter,
    const TfLiteTensor* bias, TfLiteTensor* output) {
  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;
  op_params.weights_offset = 0;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  reference_integer_ops::DepthwiseConvPerChannel(
      op_params, data->per_channel_output_multiplier.data(),
      data->per_channel_output_shift.data(), GetTensorShape(input),
      GetTensorData<int16>(input), GetTensorShape(filter),
      GetTensorData<int8>(filter), GetTensorShape(bias),
      GetTensorData<std::int64_t>(bias), GetTensorShape(output),
      GetTensorData<int16>(output));

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteDepthwiseConvParams* params,
                                  OpData* data, const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* input_quantized;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &input_quantized));
  int8_t* quantized_input_ptr_batch = input_quantized->data.int8;
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  DepthwiseParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.depth_multiplier = params->depth_multiplier;

  op_params.weights_offset = 0;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);
  if (kernel_type == kReference) {
    reference_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr);
  } else {
    optimized_integer_ops::DepthwiseConvHybridPerChannel(
        op_params, scaling_factors_ptr, GetTensorShape(input),
        quantized_input_ptr_batch, GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<float>(bias), GetTensorShape(output),
        GetTensorData<float>(output), affine_quantization->scale->data,
        input_offset_ptr, CpuBackendContext::GetFromContext(context));
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // auto* params =
  //     reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);
  TfLiteDepthwiseConvParams data_params;
  ExtractDepthConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteDepthwiseConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context,
                    GetOutputSafe(context, node, kOutputTensor, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context,
  //                   GetInputSafe(context, node, kFilterTensor, &filter));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetDepthConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // const TfLiteTensor* bias =
  //     (NumInputs(node) == 3) ? GetInput(context, node, kBiasTensor) : nullptr;
  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetDepthConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TFLITE_DCHECK_EQ(input_type, input->type);

  switch (input_type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteFloat32) {
        return EvalFloat<kernel_type>(context, node, params, data, input,
                                      filter, bias, output);
      } else if (filter->type == kTfLiteInt8) {
        return EvalHybridPerChannel<kernel_type>(context, node, params, data,
                                                 input, filter, bias, output);
      } else {
        TF_LITE_KERNEL_LOG(
            context, "Type %s with filter type %s not currently supported.",
            TfLiteTypeGetName(input->type), TfLiteTypeGetName(filter->type));
        return kTfLiteError;
      }
      break;
    case kTfLiteUInt8:
      return EvalQuantized<kernel_type>(context, node, params, data, input,
                                        filter, bias, output);
      break;
    case kTfLiteInt8:
      return EvalQuantizedPerChannel<kernel_type>(context, node, params, data,
                                                  input, filter, bias, output);
      break;
    case kTfLiteInt16:
      return EvalQuantizedPerChannel16x8(params, data, input, filter, bias,
                                         output);
      break;
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));

  switch (input->type) {  // Already know in/out types are same.
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      context->ReportError(context, "Type %d not currently supported.",
                           input->type);
      return kTfLiteError;
  }
}

}  // namespace uhkmte

TfLiteRegistration* Register_uhkmte_REF() {
  static TfLiteRegistration r = {
      uhkmte::Init, uhkmte::Free, uhkmte::Prepare,
      uhkmte::Eval<uhkmte::kReference>};
  return &r;
}

TfLiteRegistration* Register_uhkmte_GENERIC_OPT() {
  static TfLiteRegistration r = {
      uhkmte::Init, uhkmte::Free, uhkmte::Prepare,
      uhkmte::Eval<uhkmte::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_uhkmte_NEON_OPT() {
  static TfLiteRegistration r = {
      uhkmte::Init, uhkmte::Free, uhkmte::Prepare,
      uhkmte::Eval<uhkmte::kNeonOptimized>};
  return &r;
}

TfLiteRegistration* Register_uhkmte_NEON_OPT_UINT8() {
  static TfLiteRegistration r = {
      uhkmte::Init, uhkmte::Free, uhkmte::Prepare,
      uhkmte::EvalImpl<uhkmte::kNeonOptimized, kTfLiteUInt8>};
  return &r;
}

TfLiteRegistration* Register_uhkmte() {
#ifdef USE_NEON
  return Register_uhkmte_NEON_OPT();
#else
  return Register_uhkmte_GENERIC_OPT();
#endif
}

// Warning: Clients using this variant are responsible for ensuring that their
// models only need the UINT8 type. TFLite's op registration mechanism doesn't
// yet allow for more nuanced registration mechanisms.
TfLiteRegistration* Register_uhkmte_UINT8() {
#ifdef USE_NEON
  return Register_uhkmte_NEON_OPT_UINT8();
#else
  return Register_uhkmte();
#endif
}

}  // namespace builtin
}  // namespace ops
}  // namespace tflite
