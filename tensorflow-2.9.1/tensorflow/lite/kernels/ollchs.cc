/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace ollchs {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

int8_t filter_r   aw[18816]={-73, -58, -11, -41, -55, -24, 83, 18, -21, -24, -30, 2, -25, 42, -48, -28, 46, -66, -69, 109, 12, 30, -67, -60, -55, -127, 70, -57, -61, 40, -9, -33, 53, -18, -48, -25, 31, 18, -39, -127, 81, -31, 89, -117, 2, 66, -23, -83, -24, 2, -16, -55, 0, 76, 77, -28, -10, 127, -25, -23, -13, 11, 5, -6, 21, 11, 42, -51, -18, 1, -20, 9, 3, -71, 7, -16, -12, -31, -29, -76, 12, -8, -30, 7, -42, 21, -54, -43, 14, -9, 3, 122, -58, 51, 25, -72, -20, -9, -49, -30, 41, -19, 127, 53, -61, 1, 37, -41, -56, -77, -9, -28, 32, -55, -20, 28, 85, 22, -104, -118, -15, -97, -26, -98, 37, -39, 29, 22, -81, 92, -77, 14, 31, -39, -40, -16, -95, 18, 127, 83, -42, -5, -58, -2, 18, -14, 65, 64, 14, 83, 25, -2, -72, 74, -31, 37, -70, 48, -21, 127, -53, -15, 11, -23, -30, -112, 37, -74, -7, 0, -84, 53, -20, 37, 0, -73, -4, 120, 41, -127, 49, 25, -70, -44, -89, 24, -52, -85, -14, -113, 16, -20, 67, -78, 71, -32, -54, 62, 9, 33, -29, -21, -122, -31, 25, -50, -17, -127, 77, -76, 59, -24, -33, 16, 92, -118, -38, -31, -18, 1, -45, 81, -88, -4, -63, -24, -55, 19, 52, 9, -46, 8, -14, 12, -25, -56, -70, -24, -26, -117, -38, 18, -127, -10, -24, 66, 39, -37, -56, 8, 80, 27, -33, -44, -43, 6, -74, 82, 4, -3, -51, -1, -41, -57, 60, -87, -6, -73, 17, -127, 13, -25, -78, 22, -1, -45, 16, -45, 5, -19, 44, -15, 6, 6, -23, -3, 45, 16, 0, 2, -13, -12, -61, 19, -79, -57, 2, 3, -115, 77, 0, 27, 41, -30, -5, -127, 22, -3, 43, -71, 39, 5, 3, 14, 61, 19, -5, -7, 24, 25, 44, 0, -20, -4, 22, 31, -56, 127, -38, -28, -19, -46, -75, -103, 46, -45, -14, -69, -77, -1, -56, -62, 9, 68, -101, -50, 63, -127, 38, 0, -11, -35, 61, 12, -5, -39, -65, -17, -52, -102, -82, -71, -65, -15, 127, -13, 8, 18, 51, -17, 36, 4, -14, -16, 8, 16, 97, 15, -51, -34, -8, 49, -92, 15, 3, 12, 6, -125, -62, -117, 19, -47, -23, -64, 33, -64, 35, -34, -47, -12, -99, 5, 28, -127, -108, 16, -102, -22, 105, -72, 87, -58, 7, -2, -31, 34, -34, 0, 50, 64, 76, -3, -98, -19, -31, -32, 49, 14, -10, 84, -10, 41, -28, 27, -51, 6, 60, 4, -63, 23, -59, -90, 73, -49, -1, -58, 17, -127, -7, 48, -42, -44, -127, 53, -30, -33, -16, -70, -71, -56, -34, 54, -32, -29, 64, 9, -40, -26, -31, 31, 39, -8, -39, -23, 31, -3, 62, -33, -27, 27, -37, -33, -38, 49, -29, -50, 108, 1, -1, -17, -10, 112, -27, -36, -2, 52, 36, -36, 23, -127, 30, 15, 67, -1, -125, -35, 119, -33, 18, -26, 6, -35, -16, -17, -55, -71, -115, 19, -35, -58, 48, -4, -83, -60, -79, 49, 38, -120, -127, -108, 5, -14, -17, 22, -103, 18, -20, -7, -37, -15, -31, -2, 22, 68, -56, -21, 26, -127, -17, 31, -94, -54, -27, -34, -90, -2, -19, -15, -18, -42, 127, -92, 67, 19, 18, -75, -63, -11, 11, 1, -14, 38, 26, -53, -6, 64, 37, -23, 73, 120, -18, -36, -31, -90, -50, -49, 53, -8, -2, 86, -67, -75, -73, 9, 41, -1, 5, 26, -24, 33, 2, 66, -1, -127, 64, -38, -66, 41, -8, -86, 25, -79, 67, -39, -103, -37, 47, -96, -38, -30, 6, -33, 127, 10, -10, -36, -63, -65, 85, -28, -13, 74, -17, -27, -53, 8, 3, 7, -17, -105, 14, 25, 52, -46, -32, -29, -33, -20, -32, -72, -109, 127, -9, 100, 23, -86, -32, -1, 16, -48, 2, -43, 27, -45, -28, -86, -39, -90, 106, 42, -80, -14, 47, 64, -114, 13, -54, 83, -38, 83, -41, 46, -59, -106, -127, 8, 46, 51, 25, -95, 37, 21, 20, -72, 13, -108, 0, -97, 42, 23, -9, -45, 43, -40, 31, 127, -49, -3, -14, -91, -7, 30, 33, 114, -83, 12, -62, 61, 27, -13, -13, -107, 16, -5, 17, 26, 19, -76, -57, -9, -2, -3, -29, -49, -109, -21, -20, -50, -6, -127, 48, -8, 64, 10, -24, -25, 26, -79, 12, -30, -22, -63, 31, 55, -33, 1, 127, -17, -46, -17, 53, -53, -80, -13, 4, -27, -34, -62, -20, 10, 10, -12, 57, 0, 7, 4, 15, -35, 5, 23, -41, -10, -18, -46, -15, -13, -10, -26, -18, 0, 10, -19, 127, -15, -6, -16, -23, 0, 12, -14, -26, -18, -3, -15, -17, 3, -8, -21, -11, -15, 3, -24, -9, 8, -69, -4, -24, -13, -77, -11, 0, -34, 31, -54, 35, -27, 69, 61, -16, 9, 119, -127, -28, -62, -22, -27, 27, 51, -122, -31, -10, -9, 86, -9, 82, 36, -34, 55, -6, 28, -24, -79, 37, 5, 11, -17, -13, 42, -1, 127, -45, 98, -9, 31, 0, -41, 37, -27, 0, -15, 34, -16, 24, -33, -41, -7, -23, 9, -51, -25, -39, -5, -127, 20, -44, -13, -18, -105, -12, -54, -31, -57, -62, -8, 35, -70, -18, -9, -30, 4, 7, 22, -21, -33, 13, 20, 13, -51, -20, -23, -34, 23, 47, -5, 69, -7, 12, -30, -127, 14, 6, 10, 47, -34, 46, -87, 20, -12, 60, -102, 44, 22, -60, 127, -40, 120, -24, 23, 13, -48, 73, -40, -21, -2, -17, -22, 42, -119, 40, -27, 80, -27, 35, -58, -75, 80, -80, 53, -73, -51, -56, -50, 88, -78, 32, -127, -61, 51, -39, -12, 6, -3, -41, -59, -63, -109, -10, -33, 37, -24, 8, -9, -20, -9, -21, 22, 27, 19, -43, 31, 2, 11, -66, 25, -50, -7, 14, -29, -52, 127, -31, -21, 48, -53, -45, -124, 34, -7, -27, -74, -8, -16, 12, -85, -56, 48, -71, 26, -21, 21, -29, 19, -32, 127, -16, -23, -53, 12, -33, -1, -51, -52, -53, 4, -6, -16, 96, 10, -127, 11, 7, 91, -28, -7, -7, 58, 31, -103, 58, 23, 38, 28, 18, 4, -99, -4, -18, -44, -104, 56, -5, -97, 16, 91, -5, -17, -32, 49, -37, -20, -16, -15, -127, -89, 18, -40, -22, -34, 0, -61, -3, -4, -3, -1, 34, -7, -21, -53, 13, -23, 6, -62, 23, -71, 110, 5, 38, -19, -5, -25, -10, -40, -28, -58, -2, 20, -45, -63, 28, -4, -119, 111, 4, 21, -2, -40, 8, -127, 32, 6, 120, -2, 89, -29, -39, -22, -41, -13, 2, -39, -34, -29, 22, -53, -31, -42, -8, 13, -28, 103, -35, -7, -28, -124, -127, -46, 28, 7, -90, 32, 29, 27, 21, -51, -65, 54, -21, 127, 48, -72, 4, 73, 35, 61, 60, -47, -75, 29, 45, -15, -95, -28, -65, 120, 9, 42, 43, -44, -76, -33, 20, -3, -83, -35, 19, 88, 54, 7, 87, 20, -94, -127, 61, 14, 50, 35, -85, -58, -26, -96, 10, -87, 68, -34, 8, 64, 10, 57, -20, 56, -33, -45, -1, -13, -9, 31, 28, 62, -52, -90, 3, -7, -118, -127, -44, 60, -35, 7, 13, 77, -20, -34, 75, -61, 8, -45, 3, 17, -79, 4, -49, -95, -28, -127, -12, 9, 37, -52, -27, -44, 12, -5, 18, 18, -53, -16, -35, -109, -30, 58, -60, -42, -3, -38, -22, -47, 34, -52, 6, 83, 32, -65, -93, 6, -83, -50, -52, 37, -66, -77, -97, 76, -102, 11, -85, 108, -127, -60, 61, 127, -13, 55, 46, 30, -66, 39, -25, 72, -3, -90, 10, 15, 107, 108, 26, -46, -44, -20, 53, -54, -15, -13, 3, 7, 11, -12, -97, 127, -41, -51, 16, -98, -85, 56, -2, 41, -8, -82, -76, 83, 11, 14, -13, -67, 12, -42, -28, -1, -40, -66, -16, 29, -79, -4, 32, 23, 49, 59, -33, -2, -15, 2, -24, -18, -33, 38, -22, -39, 14, 12, -15, -18, -3, 50, 1, 0, 15, -59, -76, -127, 71, -9, 54, -127, 25, 127, -32, -25, 116, 70, -41, 11, 22, -26, -1, -109, -76, -42, 12, -12, 30, 21, -8, -45, -49, -68, -64, 30, 30, -37, 21, -1, 7, -19, 1, 37, -1, 40, -31, 7, 3, -4, -127, -20, -32, 13, -4, -20, -27, 74, 4, 24, -13, -28, -28, -83, -16, 16, 23, -33, 74, -9, -4, -5, -28, -12, -26, 18, -20, -72, 44, 11, -3, 89, -3, -19, -127, 18, -23, 6, -7, -7, -18, 48, -17, -46, -13, 21, -19, -34, 90, 9, -29, -59, 9, 16, 7, 30, -102, 6, 2, -84, 97, 8, 6, 23, -28, 127, -27, -17, -81, 21, -32, 74, -6, 7, -27, -54, -60, -26, 14, -55, -61, -13, -36, -127, -4, -36, -50, 47, -28, -8, 46, -50, -56, -12, -103, -64, -78, 40, -56, -44, 0, -36, -25, 13, -8, -10, -45, 55, -40, 90, -20, -47, 41, 33, 83, 28, -40, -3, -21, 56, -59, 46, 50, -127, -59, -124, 21, -46, 5, -10, 16, -37, -2, -5, 59, 18, -47, 35, 36, -5, 15, 46, 3, 12, 8, 2, -31, 0, 22, -41, -7, -127, -35, -75, 37, 21, 96, 26, 49, -25, 65, -97, 17, -4, -3, 57, 74, -8, 29, 127, 2, -64, 110, -63, 38, -123, -40, -13, 2, -91, -23, -4, -83, 28, -47, 32, -19, 55, -111, -83, -50, -25, 4, -5, -117, -26, -18, -33, 8, -89, 36, -69, -48, 27, 26, -5, 2, -29, -24, -22, -127, -30, -44, -1, -78, 8, -17, -45, -54, 18, -27, -56, -19, -127, 127, 34, 23, 2, -49, 2, 76, -114, 4, -82, -85, -20, 31, 83, -2, -26, 34, -55, -73, -16, 6, 41, -44, 32, -3, 82, 34, -11, -79, 22, -38, -113, -51, 28, -90, 97, -47, 13, 8, -127, 24, -26, 37, 16, -32, -107, 10, 11, -45, -46, -126, 95, 0, -29, -78, -50, -51, 56, -11, 121, -102, 50, 24, -35, 41, -127, -38, -67, 23, 30, 50, -1, -25, 81, -52, -46, 46, -83, -88, 2, -39, -16, 6, 6, 87, 8, 68, -61, -1, 16, 10, -33, -70, -29, -37, -82, -19, -99, 33, -127, -71, 44, 14, 90, 16, 24, 29, -18, 98, -11, 5, -60, 127, -24, -10, -25, -18, 45, 69, -80, 25, -13, -21, 26, 14, 18, 72, -8, 33, 22, -30, 61, -19, -61, -44, 6, -57, -30, -41, -75, -107, -127, -51, 6, -53, 17, -2, -121, -43, -24, 2, -79, -102, 23, -105, -74, -44, -3, -105, 24, 34, -41, -77, -119, -22, -43, -24, -125, 10, 13, -21, -20, 14, 9, 127, -124, -11, 26, 11, 15, -41, 77, 45, -26, 27, -65, -67, 17, 37, 16, -12, -12, 3, 63, 57, -1, 2, 44, -60, -94, 127, 7, -36, 57, -16, 21, 3, -63, -20, -84, -19, 13, 104, -20, 29, 19, 7, 3, 42, 34, -33, 59, -36, -5, -38, -26, -44, -14, 68, -7, -66, 0, -38, -48, 38, -114, -28, -127, 11, -48, 6, 42, -16, 13, 127, -28, -7, -3, -32, -67, -4, -11, -18, -3, -23, -8, -19, 15, -27, -23, 11, -27, -30, -52, -32, -2, 48, -31, 2, -22, -23, -127, -18, 34, 1, 28, 99, 8, -10, -31, 71, -65, -47, -3, 11, 0, -7, -4, -62, -12, -12, -23, -14, -5, 13, 35, 27, -27, -70, 12, 37, 67, 18, -14, 0, 78, 58, -43, 64, -18, -57, -49, 20, 48, 78, -127, 52, 0, 15, -119, 40, -5, -61, -36, -127, -18, -89, 8, 49, -69, 91, -59, 4, -47, 95, -115, 24, 69, -72, 35, 48, 2, -38, 85, 35, 46, 28, -53, -32, -4, -29, -45, -36, -28, -46, -117, 89, 5, 120, -95, 63, 16, -16, 25, -127, -26, 40, -68, -29, 62, -42, 67, -42, -21, 12, -37, -8, -94, 29, -17, -41, -22, -11, -40, -32, 20, -55, 127, -51, 23, -105, -39, 10, 12, -22, 23, -108, -7, -15, -2, 6, -66, -21, -28, -55, -21, -21, -66, -65, -4, -114, -69, -55, -22, -48, 37, -73, -38, -91, -74, 101, 11, -115, 20, -12, -89, -81, -127, -58, -48, -25, -55, -76, 18, 2, -95, -79, -5, -9, 29, -24, -38, -10, 30, 12, 45, -31, -47, -41, -49, 102, -69, -51, -27, 14, -102, 31, 7, 15, -39, -101, 49, 15, 127, 81, -67, 14, -51, 28, 22, -63, 40, 22, -36, 44, -60, 18, -26, -17, -14, 39, 13, 23, 50, -10, -20, -127, -26, -5, -44, 42, -30, -53, -78, -70, 4, 17, 23, -4, 54, -23, 23, 18, -5, -42, 12, -44, -127, 28, -51, -88, 13, 30, -82, -57, -45, 7, -118, -65, 10, 54, 34, -34, 6, -11, -44, 77, 9, 23, 90, -10, -10, 3, 93, -104, 11, 74, -1, 72, -37, -1, -57, 26, -127, -10, -35, 4, -62, 18, -25, -60, 26, 37, 15, 77, -37, -11, 8, 3, -65, 0, 8, -51, 20, 13, -13, -31, 127, 24, -20, -69, -55, -12, -84, -6, -2, 16, -10, -23, 127, -65, 29, -14, -60, -8, 2, -6, -22, -6, -6, -15, 14, -21, -49, 3, -6, -29, 9, 36, -52, -37, -5, 9, 7, 12, 44, -2, -26, -14, -7, -61, -5, -58, -27, 5, 13, -28, -35, 91, -43, 42, -27, -127, -60, 28, -22, -35, -86, -34, -26, 4, 23, -86, 50, -56, -2, 12, 40, -42, 43, -62, 9, -29, -43, -79, -21, 15, -127, -66, -66, -91, 13, -11, -49, -17, -5, -42, -38, -42, -14, 24, -28, -22, -40, -15, 48, -8, -100, 11, 37, 58, -81, 83, -33, -24, -66, -6, -19, 110, -15, -41, -88, -127, 46, 70, 57, 60, -68, 8, -31, -29, 76, -17, -74, 4, 22, 67, -28, -8, -95, -90, -123, -33, -6, -16, 26, -66, 36, -41, -23, 16, -104, -127, -44, -54, -66, 25, -17, -44, 67, 54, -126, 79, 44, 3, 54, -72, -86, -15, 4, -16, -39, 66, 7, -9, -39, -127, -26, -83, -40, -24, 13, -34, 71, 43, 19, 26, 4, 58, -47, 112, -42, -7, 64, 20, -25, 69, 27, -31, 127, -21, -75, 33, 30, 6, -47, -50, -49, -50, -10, 77, 104, 126, 41, -2, 12, -3, 0, 86, 15, -2, 72, -7, -38, -15, 22, 33, 18, 28, 32, 42, 56, -4, 7, -1, -68, -127, -106, 47, -4, -71, 54, -25, -17, -57, -93, 11, 70, 0, 127, 52, 14, -80, -7, 98, -20, 77, 7, -32, 66, -59, 4, 22, -121, -48, -18, 71, -79, -98, -90, -7, -45, 6, -37, -105, 80, -23, -31, -15, -1, 2, -39, 68, 46, -101, -18, 3, 65, -3, 127, -104, -101, -55, -87, 34, 43, 47, -38, 73, 12, 36, -11, 15, -19, 17, 18, 6, -13, 27, 21, -21, 65, 47, -23, -24, 37, -19, -34, -6, 19, 18, -127, 82, -36, 43, 16, -5, 119, -28, 68, 7, -49, 93, -44, -41, -40, 117, -37, -29, 127, -11, 50, 19, -5, -4, -56, 37, 56, 72, -9, 29, -32, -24, 14, -18, 127, -17, -37, -4, 37, -43, -28, 25, -29, 4, -6, 0, -13, 9, -5, -1, 16, -19, -25, -10, -19, -28, 0, -39, -18, 127, -2, 3, -52, -49, -69, -50, -5, 93, -10, 19, -76, -4, -16, 37, 8, -1, 39, 113, 95, -42, 38, -32, -74, 37, -66, 5, -43, -19, 11, 83, -23, 8, -8, -54, -24, -15, -34, -41, -127, -49, 1, -52, -28, -10, -52, -37, 17, 2, -38, 6, 13, -15, -2, 4, -4, 78, -22, 48, -22, 19, -11, 61, -22, 11, -24, -38, -53, -29, 42, -63, -49, 17, -43, -79, 76, -6, 64, -14, -36, -49, -127, 9, 1, -19, -29, 21, 19, -35, -127, 12, 38, -4, 51, 69, -86, -52, -86, 3, 3, -58, -73, 37, 22, 52, 100, -97, -23, 16, 14, 125, 4, 45, -127, -27, -111, -53, 18, 39, -115, 77, 11, -108, -73, -18, -17, -70, -45, -30, -30, -59, -58, 67, -10, -34, -3, -2, -45, 23, 15, 31, 5, -18, -23, -12, 7, -43, 84, -53, -22, -60, -112, 12, 9, 1, -21, -81, -4, 19, 54, -45, 62, 8, -93, 5, -127, 37, -24, 37, -55, 39, 17, 16, -101, -64, 33, 30, 127, 97, 8, -44, -37, 1, -4, -26, -35, 56, -62, -14, 45, -34, -86, 68, 40, -15, -18, -52, 94, -120, -14, -85, -10, -41, -98, 51, -83, -50, -121, 127, 6, 7, -57, 113, 53, -20, -6, -114, -78, 108, -118, 122, -94, -31, -103, -37, 32, -117, 28, -16, 14, -34, 25, 24, 28, 3, -39, 10, -16, 10, 45, 4, 17, 17, -127, -1, -43, 29, -18, 45, 64, 11, -34, 39, 40, 127, 23, 34, 4, 37, 76, 17, 46, -28, 68, 40, 35, -81, -37, -50, -10, -58, 103, 30, 68, 80, -16, -25, -88, -9, 18, 5, -108, -74, -99, 93, -88, 70, 19, -43, -23, 108, -27, -40, -21, -122, -28, -127, -20, 36, 82, -73, -91, -35, 43, 29, -113, 6, 81, -90, 67, -16, 15, 70, 25, -27, 1, 0, 26, -28, -13, 14, 37, -21, -108, -46, 10, -67, 55, -9, 77, 13, -127, -76, -61, -27, 77, 28, -32, 107, 1, 127, 47, -70, 116, -56, 74, -26, 29, 23, 99, -90, -22, 13, 78, 98, -59, 51, 66, 53, -27, 5, -72, -115, 63, 17, -24, -49, -39, -25, -41, -26, 14, -13, 46, -54, -127, -55, -42, -57, 84, -47, 6, 3, 5, -35, -34, 55, -35, -23, 105, 21, -97, 23, 11, 5, -16, 10, 45, 1, -14, -44, 33, -46, -46, -24, -6, -80, 59, -58, 33, 16, 80, 10, -53, -6, -127, 44, -82, 38, 4, -30, 74, -9, 48, 44, 33, -18, 74, -127, -93, -65, -72, -62, 34, 16, -37, -67, -6, -61, -16, 4, 50, -92, -123, -35, -112, -90, -69, 12, 36, 2, -22, -64, -22, -34, 62, 2, 60, -38, 14, -4, 12, -32, -58, 49, 19, -38, 53, -45, -28, 11, -65, -97, -54, 127, -50, -56, -90, -53, -37, 32, -95, -70, -127, -42, -21, -58, -17, -81, 61, -82, -63, -3, 28, -54, -76, -4, -102, -12, 21, 3, -10, -92, -23, -26, 50, -27, -15, 71, -19, -22, -41, -68, 57, 20, -95, 46, 83, 23, -16, -42, 0, 21, 36, -52, -127, -36, -84, -16, 10, -10, -7, -2, 31, 40, 3, 23, 22, -1, -5, -20, -7, -48, -28, -13, -4, -23, -25, 17, -22, -56, 117, 20, -39, -2, -48, -6, -127, 41, 64, -74, 5, -7, -31, 37, -41, 1, -26, -47, 20, -77, -68, -43, 64, -22, -59, 51, -35, -11, 2, -20, -11, 42, -24, -100, 49, -127, -45, -21, -100, -9, -47, -1, 45, 33, -20, -103, 83, -47, 42, 66, 83, -18, 29, 44, 2, 25, 86, 6, -127, -77, -76, -19, -112, 115, -66, -11, -2, 127, 24, 17, 2, -7, 53, 12, 13, -14, -72, 8, 27, -5, -46, 25, 34, -14, 48, 8, -16, -1, -58, 2, -32, 26, 1, 45, 124, -41, 7, 33, 13, -100, -109, 92, -127, 13, -35, 85, 31, 52, -55, 76, 104, -53, -41, -61, 116, -108, -117, -78, -33, -49, -59, -127, -30, -31, 22, -50, -61, -54, 44, 10, 11, -31, -111, -11, -81, 64, -78, 7, -27, -49, -92, -55, -114, -94, 9, 2, -8, -37, -65, -82, 25, -56, -4, 16, -45, -39, -81, -127, -16, -105, -54, -72, -89, 17, -28, 45, -39, -16, -24, 49, -13, -55, -18, 3, -71, -92, -13, -53, 89, -69, 39, -39, -5, -37, -44, -4, -14, -21, -92, 127, -21, 10, -34, 31, 21, 66, -95, -24, -74, -30, -23, -13, 100, -44, -58, -27, 19, 127, -4, -37, -12, -36, -14, 5, -2, -7, -93, -13, 17, 10, -27, 22, -19, -7, 4, -7, -35, -11, 18, -4, -22, -5, 27, -74, -32, -55, -53, 42, -16, 14, 35, -21, 115, 54, -50, -98, -48, -92, -122, 57, 59, -127, 8, 0, -15, -74, 0, 0, -51, 3, -88, -44, -83, -18, -40, 16, 32, 41, 2, -37, -42, -13, 2, -27, 44, -80, -70, 5, -1, -83, 127, -38, -78, -36, 10, -15, -105, -3, -12, -39, 11, -9, 54, 60, -110, -44, 39, 60, -80, -14, -6, -127, 36, 86, -7, -18, 6, 16, -72, -87, -51, -55, -92, -22, -13, 5, -90, 49, -36, -50, 49, 15, 31, -28, -29, -18, -15, -21, -7, -29, -27, -54, 21, 85, 4, 25, 86, 12, -29, -30, -43, -127, -45, 31, -13, -20, -7, 20, -25, 43, 5, -28, 17, -25, 8, -10, -65, 14, 127, -12, -102, 41, -7, 16, -47, -7, -79, -14, -94, -31, -56, 7, -7, 14, -30, -24, -26, -34, -34, -21, -69, 6, -59, -60, -64, -17, -89, 22, -67, -60, 5, -31, -45, -78, 18, 25, -127, 63, 38, 50, -75, 36, -26, 4, -23, -28, -17, 15, 59, -11, 25, -27, 4, -26, 33, -29, -61, 4, -31, -52, 89, 16, -16, 30, -40, -55, -127, 26, -42, 21, -96, 46, 33, 1, -70, -32, -18, 2, 6, -43, 35, -1, 38, -61, -16, 83, -7, -121, 112, -41, 27, -45, -98, -80, -107, 127, -18, -73, -32, -77, 20, -35, 6, 22, -3, 53, 127, 60, 3, -29, 42, -85, -65, 70, 26, 16, -1, -36, 37, -27, -90, 50, 22, -24, -61, 15, 49, 33, 25, 29, -43, -44, 78, 11, 82, 19, -38, 35, 37, 29, -99, -17, 5, -11, 73, 68, 43, 70, -127, -8, -58, 38, 36, 62, -68, -13, 38, 85, -46, -127, 8, -10, -3, -36, -4, -40, 36, 28, -8, 26, 6, 37, -78, 36, 15, -12, -40, -41, -107, 100, 11, 76, -37, 12, 67, -38, -29, -29, -35, 6, -7, -75, 30, 20, -37, -18, 52, -34, -32, -68, 34, -63, 3, 0, -7, -127, 26, 81, -5, -37, 68, -74, 5, -33, -27, -83, 14, -8, 32, 19, -94, -2, -16, 40, -30, -31, -2, 28, -54, -54, -40, -26, -51, -52, 127, -64, -34, -84, 17, -8, -11, 22, -23, 1, -74, -28, -15, -18, 58, 12, 39, 5, -74, -23, 32, 33, 45, -26, -19, -101, 30, 42, -115, 50, -127, -12, 94, 11, -40, 43, 28, 13, 5, -30, 79, 31, -21, -37, 52, -2, -83, 7, -3, -23, -44, -25, 52, 37, -127, 28, 38, -34, -14, 22, -2, 13, 7, 1, 82, -10, 48, -12, 46, 37, 39, -19, -43, 38, -35, -75, 50, -117, 2, -10, 127, 21, -44, -2, -28, 98, 52, -7, 46, 18, 3, 42, -28, -79, 3, 74, 19, 31, -30, 88, 102, -7, 52, 70, 11, -86, -126, -28, -4, 48, -127, -14, 12, -22, -58, 8, -81, -9, 1, 9, -5, 127, -99, 16, 43, 54, -101, -89, 60, -27, -71, 54, -33, -63, 1, -62, -74, -12, -115, 54, -57, -86, -66, 38, -48, 127, 0, 19, -49, 3, -74, -3, -25, -27, -96, 4, -67, -15, -59, 11, -42, -17, 115, 21, -15, -51, -3, -2, -106, 14, 57, 127, 8, -9, -30, -53, -42, 6, -26, -26, -4, 15, -41, -28, 2, -48, -22, -26, 7, -29, 35, -52, -26, 21, 9, -76, -64, 49, -41, -91, 23, -68, -76, 105, 82, -105, 57, -36, 127, 79, -29, 102, -8, -103, -63, -15, 33, -84, -5, 32, -69, -70, -20, -67, -4, -48, -10, -60, -11, -52, -98, -18, -32, -23, -73, -84, -21, -30, -106, -69, 51, 17, -77, -26, 14, 16, -49, -109, 3, -40, -44, 45, -10, -57, -127, -2, 10, -5, 55, 49, 48, -5, -6, -2, 18, -56, 2, 39, -26, -13, 1, -1, -127, 1, 1, -105, 12, -20, -9, -20, 0, 14, 7, -37, 26, 7, -25, 41, 18, -111, 3, 28, 0, 63, -103, 59, -10, 99, -29, -1, -66, -40, 71, 24, 94, -127, -120, -57, -27, -63, -60, -17, -56, -8, 19, 45, -127, -40, 44, -5, 44, 19, -17, 17, -65, 48, 13, -28, -29, 35, 13, -5, -79, -18, -15, -24, -54, -40, -3, 68, -18, -127, 2, 52, -44, -53, 5, -14, 31, -8, 35, -7, -3, -26, -81, -5, 16, -20, 27, 6, 45, 78, -95, -15, -2, 76, -19, -61, 35, -63, -23, -23, -4, -58, -73, -1, -11, 9, -120, 44, 1, -3, -59, -49, 5, 4, -127, -37, -19, -45, -20, 41, 24, -37, -10, 56, 43, 47, 39, 78, 54, 51, -3, 22, 69, 101, -66, -13, -8, -67, -106, -26, 120, -45, 127, -113, 49, -48, -51, -32, 2, 79, -104, -21, -11, 14, -112, 88, 99, 18, -61, 109, -37, -82, -43, -41, -108, 91, -13, -3, -10, -9, -37, -17, -94, -2, -127, -33, 26, 93, -47, 76, 87, 85, 30, 63, 34, -34, 6, 47, -63, 41, -25, 127, 1, 11, -30, -52, 94, -47, 31, -22, -88, -103, 21, -46, -102, 47, -63, 78, -1, -52, 15, -12, -16, 41, 38, -54, 51, 10, -50, -9, 36, -40, -48, -13, -5, -66, 96, -37, 1, 62, -127, 3, -36, 48, 7, -40, 92, 21, -75, -19, 96, 4, -36, 4, -1, -14, -127, 10, 25, 110, 8, -92, 8, 42, -23, -57, -55, -1, -29, -20, 65, -48, 19, 31, 50, 11, -26, 11, 45, -55, 70, -18, 127, -26, -21, -51, 73, -61, -68, -19, -57, -8, 51, 29, 8, 35, -79, 15, 9, 54, 35, 57, -8, 124, 6, 33, 55, 33, 33, -7, 3, 15, -96, -13, 5, -59, 96, 60, -7, 10, 47, -5, 127, 35, -95, -47, 7, 102, 75, 19, -88, -42, 33, 58, -35, 90, 61, -51, 112, 65, 124, -44, 3, 26, -12, -12, 0, 65, 43, -15, -6, -31, -127, 58, -69, 16, -5, -53, -27, -43, -17, -59, -13, -79, 48, -5, 73, -2, -113, -72, 45, 3, 127, -31, -2, 126, -52, 12, -39, -19, -84, 28, -2, 28, -32, 28, 46, -78, 0, 43, 30, -127, -43, 39, 25, 70, -52, 45, 99, 82, -64, 70, 30, 43, -43, -34, -20, -53, -105, 12, -13, 32, -2, 99, 30, -93, 31, 46, 52, 37, -59, 23, 34, -18, -127, 3, 28, 12, 12, 42, 17, 67, -38, 69, -15, 68, -39, 14, 26, -16, -24, -94, 66, -69, 89, 4, -72, 117, -104, 93, -51, -127, -116, 126, 74, -19, -55, -91, -70, -84, 19, -29, -68, 78, 28, -71, -86, -78, -44, -17, -10, -62, -42, -54, -26, 0, -44, -25, -41, -88, -80, -70, -26, -127, 9, 7, -123, 104, -4, -47, -37, 6, -10, -69, -99, 2, -78, -7, 24, -45, -66, 24, 12, -65, -37, 3, -8, -39, -79, -58, 8, 32, 57, 99, -2, -120, -3, 32, 0, -30, -127, -67, -67, 30, -97, -41, 71, 51, 5, 19, -9, 10, -77, 56, -5, 36, -25, -10, 49, 8, -51, 52, -50, 35, -127, 49, -22, -51, -57, -18, 38, 48, 45, -127, -81, 3, 21, -24, -44, 56, 14, -16, 49, 14, 25, -18, -3, 33, -40, -53, -40, -42, 18, 11, 8, -33, -44, -43, 0, -24, -3, 68, 77, -78, -1, -41, 14, -1, 5, -14, 15, 22, 30, -35, -42, 24, -104, 52, -9, -94, -43, 7, -51, -45, -127, -8, 40, -1, 1, -4, -34, 127, 3, 2, -14, -19, 7, -21, -14, -3, -72, -8, -3, -26, 10, 5, -14, -11, 69, -29, 25, -54, -4, -13, -107, -5, 9, -8, 30, -127, -15, 31, 70, 46, 31, -26, 87, 47, -10, -72, 53, -16, -10, 25, 2, 96, 33, -13, 35, 26, -42, -55, 31, 57, -15, -14, 34, -127, -44, 41, 62, -12, -92, 68, -29, 5, -36, -22, 12, 80, 107, -31, 106, -126, 1, -38, -55, -79, -65, -52, -28, -47, -26, 13, -76, -43, 34, -4, -45, 55, -16, -48, 50, 35, -64, 0, -15, -1, -105, -15, -57, -80, -11, -5, 36, -45, -14, 24, -42, -43, 127, 73, 127, -45, -73, 16, -23, -51, 20, 13, -21, -14, -25, 3, 17, -2, 17, -64, -36, -20, -56, -81, -46, -24, 10, -53, -20, 72, -32, -60, -48, -25, -68, -8, 27, -105, -81, -90, -8, 39, 13, 15, -49, -8, 1, -36, -7, 56, -12, -66, -115, -127, -109, -27, -52, 14, 39, 20, -62, -17, 57, 23, -18, -31, 32, 1, -8, 10, 35, 12, 65, 7, 35, 28, -11, -86, 127, -38, 7, 17, -73, -11, -66, -11, -71, 127, -48, 2, -4, 25, -47, -44, 21, 8, 21, 11, -43, -51, -17, 0, -24, -14, -23, -9, 11, -13, -22, -10, 20, -23, 0, -63, -38, 37, 25, -127, -20, 36, 3, -106, 39, -19, 95, 27, 81, -100, -14, 62, -49, 38, 8, -64, 30, -29, -32, 33, -61, -118, -117, -16, 47, -10, 0, -29, 23, 5, 44, -10, 11, -22, 47, 17, -40, -28, 2, -15, -127, 15, 2, -119, 26, -5, 2, -25, -34, -2, -38, -41, -25, 19, -13, -7, -57, -2, -17, -86, 24, -65, -40, -36, -2, -5, -13, -8, -5, -69, -23, -54, 85, -46, 6, -21, -127, 5, -10, -37, -10, -15, -6, -16, -127, 12, 4, -3, -27, 62, 0, 7, -49, -11, -21, -11, 12, -24, 16, -3, -22, 14, -25, 2, -17, -9, -14, -27, -17, -7, 69, -17, -127, 42, -31, -36, -13, 24, 57, -12, -4, -58, -44, 50, 11, -14, -49, 12, -18, -43, 3, -1, -7, -44, -13, 16, 2, 36, -60, -108, -7, -11, -33, 2, 22, 32, 16, -68, -127, 0, 2, 36, 19, -121, -54, -37, 1, -2, -27, 43, -57, 13, -65, 123, 42, 100, -55, 39, 44, -68, -10, 29, -55, -31, -5, -11, 77, -127, -33, -120, 101, -58, -39, -53, 63, -8, -28, -27, -64, -32, -108, 45, -12, -29, 33, -96, 28, -2, 77, 9, -66, 35, -71, -27, -127, 92, 11, 55, 31, 5, 67, -71, -8, -34, 12, 41, -58, -68, -8, -43, -43, -93, 15, -82, -82, -68, 104, -41, 127, -86, -60, -36, -20, 21, 102, -78, -109, -29, -25, -85, -42, -41, -18, -18, -26, -58, -63, -50, 50, 65, -24, -87, 58, 63, 28, -37, -127, 0, 65, -22, -24, -92, 68, 47, -28, -61, -50, 37, -33, 69, -33, -29, -90, -17, 1, 44, -4, 0, 41, -42, -34, -70, 30, -60, -61, -76, -1, -63, -63, 104, -20, 2, -85, -7, -96, -31, -41, 20, -39, 17, -69, -6, -83, -23, -127, 115, -95, -68, -10, -12, 37, 59, 68, 17, 56, -15, -101, -32, 35, -36, 20, 54, 39, -120, 68, -38, -127, 108, -82, -22, 71, 4, -106, 30, -18, 14, 1, -84, 83, -36, -118, 9, -59, -1, -127, -51, -69, 15, 57, -42, 5, 91, -56, -59, 35, -55, -94, -55, 35, -7, 40, -52, 3, -14, -26, 10, -73, -6, -43, -14, -13, -27, -39, 22, 55, -65, -56, 110, -68, -58, 10, -19, 31, 49, -21, -127, -22, -43, -12, -1, 15, -90, 22, 40, -31, -65, -3, 24, 90, -11, 34, -9, 62, -62, -71, -27, 55, -69, 3, -29, 69, 97, -127, -43, -24, 54, -15, 24, 98, 10, 76, 4, 31, 7, -53, 37, 34, 1, -122, 5, 53, 77, -22, 25, 11, -16, 29, 31, 73, 26, -127, 111, 31, 27, 94, -81, -37, -12, -13, -5, -113, 3, 15, 14, 9, -29, -3, 8, -33, 14, 118, -24, -34, 9, -4, -19, -49, 67, 38, -127, -48, 62, 17, 46, 6, -61, 1, -36, -35, 66, 19, 8, 81, 28, 42, 5, 22, -28, 15, 87, -3, 127, 21, -44, -42, 18, -68, -15, -119, 4, -48, -34, -25, -1, 2, 12, 15, -6, 0, -16, 22, -7, -127, -37, 31, 2, 17, -17, 8, -26, 33, -15, 26, 7, 0, 27, -58, 34, -15, 10, -42, -16, -41, 1, 22, -27, -40, 42, -8, -27, -7, -127, -61, -1, -47, -44, -3, -63, 9, 4, 1, -12, -45, 43, 10, 74, 29, -27, -50, -53, 7, -16, 17, 127, 46, 8, 72, -14, -43, -28, 20, -25, 34, -4, 28, -19, -76, -17, -65, 46, -3, 11, 47, 9, -11, -60, -65, 70, -3, 34, -60, 33, 23, -26, 21, 25, -46, -63, -34, -26, -68, -22, -22, -67, 10, -34, 127, -7, -56, -75, -65, 99, -18, 55, -36, 36, -21, 2, 23, 3, -31, 16, 83, 32, 12, -22, 11, -88, 36, 28, 33, 112, -15, -26, -14, -42, -127, -10, -76, 117, -1, 36, -66, -46, -34, -17, -36, -12, -57, 72, -36, -31, -28, -17, -53, -41, -39, 5, -12, 32, -39, -13, 28, -7, -127, 7, -58, -1, -42, 10, 9, -127, 14, -18, 38, 12, 48, 2, 51, 25, 68, -62, 2, -61, 24, 16, -32, 47, 12, 30, -26, -32, -36, 36, 0, -10, -14, 21, 107, 1, 24, -6, 26, -127, 74, 20, 76, 28, -127, -48, 30, 96, 19, -54, 70, -100, -1, -64, -71, -89, -78, -112, 63, -4, 34, -38, -8, -74, 69, 26, -63, 3, 57, -127, 80, 23, -108, 3, -25, -30, -81, 30, 25, -78, 23, 64, -71, -97, -78, 16, 26, -6, -20, -12, 15, -57, 11, 54, -18, 14, -82, 32, -8, 44, -127, -42, -25, 53, 34, -21, 44, 20, -13, 8, 57, -67, -40, -32, 29, 53, 23, 54, -106, 72, -34, -21, -12, 22, 23, -13, -19, -83, 10, -5, 15, -85, 17, -99, 41, -37, -7, -9, -73, 10, -94, -24, -125, 22, -127, -46, -121, -20, 16, 3, -81, -30, 43, -5, 12, -35, -27, -46, -4, 21, 16, -20, -33, 1, 43, -34, -83, -27, -59, -81, -127, 14, -30, 12, -127, -35, 32, -30, -76, -23, 78, -10, 32, 26, 3, 101, 72, -97, -14, 17, -54, -23, 6, 51, -5, -34, -29, 32, -48, 14, -23, 41, 70, 46, -8, 57, 42, 6, -6, 8, 16, -5, 5, -30, 39, -20, -50, -8, -30, -77, 56, 16, -54, -59, -127, 41, -84, 5, 42, 42, -66, 89, -20, -46, -31, 105, 87, -65, 102, 33, -52, -27, 21, -42, 35, 11, -47, -26, 34, 75, 60, 2, -127, 20, -117, 6, 10, 18, 14, 12, 2, 44, 15, 55, 3, -28, -15, -25, -55, 39, 4, 13, 25, -26, 15, 3, 52, 34, 27, -81, -127, -26, -55, -26, 13, 127, -1, -1, -21, 38, 4, -24, -68, -50, -34, -67, -40, -6, 25, -31, -17, 65, -13, 31, 53, -16, 39, -1, -66, 29, -61, 114, -81, -43, 42, -95, -68, 51, 27, 34, -17, 11, 123, 30, -127, -52, -7, 76, -70, -34, 46, 2, 5, 42, 19, -113, 61, -71, 40, 32, 84, 34, -22, -105, -12, 40, 29, -39, -36, 17, 52, 10, -40, -43, 30, -23, -6, 55, -57, -114, 3, 55, 36, -40, -35, -22, -127, -50, 44, 25, -127, -46, -2, 88, -17, -53, 5, 25, -32, -1, -18, 9, -16, -97, 48, -25, -60, -40, -4, 59, -41, -106, -83, -41, -18, 74, -1, 4, 7, 19, 12, 26, -28, 17, 16, 16, 14, 6, 31, 40, 6, -22, -53, -23, -16, 2, 2, 9, -7, -51, -127, -28, -1, -42, 35, -72, 127, 5, 5, 11, 48, 8, 79, -111, 79, 22, -125, 4, 89, 21, -99, -69, -28, -107, -78, 13, 82, -16, -103, 57, -12, -100, 102, 60, -8, -56, -44, 62, -31, 5, 79, -31, 17, -17, 45, -21, 32, -44, -2, -24, -25, -94, 64, -24, 1, 66, -83, -21, -127, 73, -26, -20, -54, 56, -18, -92, 32, -21, -56, 58, -27, -56, -12, -40, 23, -14, 74, 0, -12, -17, 29, 0, -48, -122, 6, 54, 7, 127, -48, 44, 16, -127, -39, 4, -21, -49, -4, -13, 29, 26, -101, -73, -29, 38, -55, -52, -54, -88, -5, -58, -20, -34, -49, -86, -22, -32, 19, 35, -23, -127, 8, 47, 1, -8, -58, 42, -32, -34, 4, 57, -5, -53, -50, -29, -5, -70, 26, 26, 33, 23, -7, -36, -5, 45, 5, 42, -121, -59, -7, -24, 57, -10, -64, 86, -2, 44, -124, -10, -76, -1, -57, -23, -70, 3, -69, -29, -108, -12, -70, -127, 24, -39, -11, -108, 127, -43, 59, -32, 27, 73, -51, 16, -30, 72, 9, 17, 28, 22, 11, 94, -46, -71, -15, -11, -11, -83, -54, -58, -41, 46, -6, 127, 3, -30, 35, -44, -109, 4, -69, -35, -20, 57, -48, 43, -11, 8, 4, -84, 9, -4, -57, -13, 75, -18, -51, 13, 21, 39, -45, 84, 53, 16, 7, 103, -17, -60, 36, 34, -22, -25, 40, 41, 36, -50, -54, 68, 17, 0, 21, -15, 29, -53, -127, -110, -37, 4, -89, 42, 51, 118, -15, -14, -19, -15, 40, 18, 15, -28, -53, 26, -36, 25, -100, 33, 22, 32, 30, -34, 32, 57, -127, -57, -99, 41, -36, -70, -40, -25, 31, 60, -6, -66, -20, -18, 24, 11, 45, -92, -58, -84, -127, 35, 34, -100, -34, -27, -7, 10, -95, 28, -69, -18, 33, 53, -24, -99, 86, -52, 48, -15, -22, 86, 76, -19, -127, -55, 13, -47, 38, -49, 68, 24, -9, -2, 23, -82, -27, 85, 18, 114, -71, -10, -5, -96, -16, 22, -51, -43, -53, -27, -103, -119, -113, -21, 84, 43, -41, -29, 15, 15, -86, -29, -127, -26, -14, -33, 2, 60, -75, 9, 7, -85, -103, 40, -72, -35, 127, -6, 82, 20, -121, -97, 15, -40, -23, 0, 42, -39, -61, -19, -58, -41, -127, 28, -37, 0, -12, 127, -23, -40, -4, 40, -40, -45, -29, 10, 0, 12, -59, -51, -17, -49, -53, -46, 2, -55, 2, -25, 49, -29, -36, 7, -24, -19, -19, 20, -51, 117, 39, 52, 79, -9, 61, -31, 81, 4, -30, -64, 74, -25, 10, -7, -1, 23, 25, 32, 54, -11, -86, 5, -127, 14, -59, -85, -115, -127, -16, 48, -2, -37, -8, -33, 1, -30, -13, -11, -34, -91, -9, -47, 9, 86, -12, 25, 38, -11, -40, -60, -7, 13, -11, 13, -71, -20, -16, -85, -59, -8, 24, 21, -2, -97, -102, -37, 15, -37, -46, -7, -2, 81, -37, 63, -14, 46, -77, -72, -14, 127, -1, -5, -11, -49, 9, -34, 5, 4, 13, -33, -54, 0, -43, -1, -82, -14, -18, 50, -47, 127, -32, -35, -65, -67, -90, -16, -15, -19, -25, 1, -127, 55, -39, 12, 21, -3, -15, -2, 54, -54, 53, 19, 103, -55, -23, 54, -59, 13, 103, 21, -44, -1, -115, 1, -89, 10, -72, -21, 1, 56, -2, 73, -1, -104, -53, 27, -35, -38, -127, 32, -40, 20, 12, -44, -10, 40, -39, 26, -8, -26, -14, -47, 93, -23, 94, -78, -4, -101, 43, -14, -19, 33, 74, -7, 92, 23, 10, -21, 83, -33, 50, -2, -14, 27, -125, 16, -19, 127, 28, 29, 27, 43, 22, 32, -5, 12, -33, 21, -51, 58, -20, -53, -5, 26, 37, -9, 47, -66, -44, -43, -30, -109, 127, -23, -23, -20, 4, -90, -91, 63, -12, 17, 69, 127, 17, 11, 35, -10, 30, 15, 47, 16, -73, 49, 46, -18, -2, -47, 42, 5, 76, -6, 18, -68, -76, -50, -44, -18, 9, -46, -28, -13, -14, -37, -1, -9, -45, 63, 48, -23, -66, 33, -15, 8, -34, -21, 21, -50, -20, -20, -65, 1, -127, 69, -17, 57, -21, 12, 35, 17, -15, -40, -19, -16, -27, 12, -21, -16, -23, 28, -7, 6, -41, 18, 8, 2, -21, -40, 19, -20, -127, 0, -25, 5, -34, 12, -53, 26, -35, -4, 15, 41, 6, -21, -4, -18, 28, -42, -73, -16, -5, 38, -55, 18, 34, 45, -27, 47, -127, -24, -22, 52, 47, 27, -34, -13, 32, 22, 5, -92, 28, 31, 50, 19, -71, 87, 14, 43, -59, 42, 5, -7, 107, -17, 1, -103, -127, -63, 13, 43, -9, 6, -37, -9, -73, 20, -33, 5, -92, -16, -69, -52, -46, -20, -1, -33, -106, -30, 18, -67, 38, 0, -76, -90, -127, -86, -65, -5, -21, 9, 34, 64, -16, -63, -24, 85, 7, -67, 44, 13, -127, -48, 66, -11, -59, -43, -59, -54, 55, -9, 14, -5, 5, 28, 29, -29, 110, -43, -29, 66, -26, 19, -34, -51, -17, -8, -8, -48, -127, -16, 0, -30, -52, 15, -21, 9, 28, -11, -73, 3, -30, -11, -51, -14, 6, 119, 38, 20, -4, -6, 26, 92, 24, -32, 69, 11, -37, -28, 8, -1, -5, 8, 24, 68, 56, -24, 9, -9, -108, -127, -63, 100, -63, -3, -9, -5, -16, -43, -25, -4, -28, -1, -20, 0, 6, 8, 127, 0, -33, -8, -11, -40, -6, 0, -16, -29, -43, -22, -24, -13, -30, 110, -23, -127, -9, -42, -53, -51, -101, -14, -34, -63, -18, 4, 9, 19, 8, 68, -67, 106, 38, -19, -31, 34, -11, -62, 96, -50, -4, 17, 42, -23, 8, 8, 71, 29, -13, -11, 29, 31, 15, -51, 19, 22, -23, -76, 1, -33, 54, 1, -42, -40, -127, 54, -29, 17, 105, -58, 84, -25, 53, -7, 58, 64, -10, -33, 127, -12, -36, -29, 77, 42, 74, -35, -65, -44, -28, 30, -113, -87, -52, -39, 56, -29, 76, 127, -93, -38, 14, 19, -99, 123, 10, 5, -12, -11, -88, -4, -50, -14, 24, -121, -5, -40, -27, -53, -35, 21, -57, -64, -90, -20, -108, 18, -102, 16, 21, -49, -5, 24, 0, -17, 28, 5, 36, -48, 4, -46, 16, 28, -13, 0, 82, 12, 15, -52, -59, -32, -127, 38, -25, 21, -53, -23, 2, -31, -40, 4, -34, -20, -27, -92, -22, 16, -59, -18, 29, -8, 13, -5, -7, 27, -13, 1, -127, -58, -74, 79, 34, -6, -14, -33, -127, 2, 23, -34, -12, 7, -9, -73, -43, 10, -25, -32, -67, -68, 27, -30, -18, 18, -31, 24, -30, -39, -53, 17, -62, 20, -29, 35, -41, -8, -12, -24, 0, -22, -4, -33, 21, -29, 51, -25, -33, -22, -27, -127, 71, 28, -28, -14, -50, -1, -12, 22, 10, -14, 10, 34, -22, -5, -26, 1, 21, -29, 44, 33, 14, -37, 22, -53, 61, 35, -1, 7, 36, -25, -66, 5, -127, -86, -70, -28, 16, -18, 20, 11, 5, 50, 82, 24, 38, -6, 58, 32, -21, 38, 127, 37, -89, -18, 30, -112, 10, 15, 9, -21, -62, 50, 43, 54, 86, -70, -23, -33, -60, -8, 49, 42, -54, -70, 17, 17, -50, -48, 8, 2, -127, -3, -9, -87, 41, -14, -71, -5, 22, 16, 2, -56, 57, -77, -31, 19, -53, -2, -62, 5, 49, -110, 4, 2, -26, -58, -49, -68, 127, -71, 24, 93, 6, 56, -11, -46, -106, -43, -57, 16, -93, 5, -105, -12, 127, -65, -58, -87, -17, -114, -82, -4, -37, -25, -35, -28, 39, 8, -66, 17, -9, 21, -38, -54, -43, -59, -27, 104, -78, -32, 86, 14, 18, -18, 11, -85, -24, 1, 40, 75, -31, 61, 32, 61, -50, 91, 34, -7, -88, -52, -34, -127, -14, -5, 19, -37, -44, 34, -29, -12, 11, -37, -42, 64, 3, -20, 0, -4, 22, -66, 9, -63, -54, 17, -4, -120, 124, -51, -3, -21, -19, -40, -127, 4, -6, -40, -43, -127, 5, -45, -19, 2, 26, -80, 62, 44, 24, -22, -3, -33, -8, 94, 23, 5, 86, -30, -28, -21, -75, -43, -106, -22, -79, -24, 8, -2, 0, -17, 55, -10, 15, 13, -12, 126, -36, 23, -15, 16, -51, -37, -127, -17, 30, 41, -10, 27, 22, -87, -26, -4, -96, -94, -32, 37, -7, 70, -51, 13, -86, 30, -70, -7, -90, 3, 40, 42, -17, 15, 30, 23, 25, -54, 52, -12, -67, 127, -89, -26, -121, 9, 19, -127, -11, 13, 33, 3, 38, -41, 28, -5, 2, 1, 35, 40, 55, -9, 26, -22, 12, -10, 44, 23, -26, 27, 2, -25, -45, 12, -81, 40, 39, 19, 16, -18, -11, 55, 1, -71, 17, -20, -30, -127, -10, 104, 77, -45, -109, -9, -56, -47, -65, -25, 57, -49, -89, -49, 64, -50, 3, 40, -3, 67, -44, 16, -37, -23, -106, 26, 2, -49, -18, 20, 2, -59, 7, -18, -11, 46, -12, -127, -43, -82, -22, -13, -90, -39, -2, 52, -34, -84, 51, -7, 20, 16, -52, -11, 3, -7, 84, 127, -48, 39, 16, 51, 3, -9, -106, 52, -69, -34, -10, -15, -2, -10, -38, 15, -4, 35, 1, -51, -7, 21, -11, -27, -26, -37, 17, 45, -36, 17, -127, -15, 9, -40, -20, 44, 37, 46, 1, 25, -8, -40, 87, -27, -97, -77, -4, -39, 19, 127, -40, 7, -28, -79, 4, -14, -62, -62, 25, -62, -39, 29, -5, -69, 9, -5, 57, 73, 8, -3, 61, 6, -58, -54, 60, 60, 107, -33, 46, 80, 19, 75, -31, 55, 13, 77, 26, 127, 52, 33, -40, -16, 4, 30, 41, -81, -94, -51, -109, 83, -48, 2, 1, -3, 127, -17, -60, -27, -45, 45, 33, 19, 20, -71, 63, -30, 36, -40, -82, 85, -110, 44, -111, 10, 15, 19, -8, -42, -8, 27, 102, -47, -45, -26, 31, 1, 30, 15, -11, -35, -26, -29, 76, -21, -127, -45, 18, -88, -42, 89, -23, -23, -4, -2, 70, -7, -6, -25, -66, -4, 59, 11, -86, -5, -16, -51, 95, 77, 48, 127, -72, -31, 66, -47, -4, 0, -69, 97, 13, -102, -19, -38, 127, 8, -29, 86, -11, -20, -38, 62, -35, 43, 31, -9, -46, 48, -62, 15, 16, -60, -65, -51, -36, -52, -7, -62, 13, 127, 10, -38, -4, -36, -86, 17, -55, -30, 49, 8, -120, -28, 2, -67, 57, 44, -2, -56, -8, -24, 21, -2, 37, -75, 2, -15, 18, -6, -111, 39, 2, -32, -3, -6, -9, -3, 45, 39, 23, -58, -57, -52, 82, -2, -23, 77, -48, -21, 45, -127, -28, -21, -12, 50, -28, -55, -8, -121, -4, 24, 32, -46, -68, 1, 20, 15, -127, -34, 55, -20, 39, 43, -35, -25, 6, 48, -9, -25, -22, -43, 24, 34, 21, -49, -18, -7, 29, 1, 80, -20, 31, 7, 59, 17, -36, -34, 5, -25, -127, 8, -7, -127, -20, 53, -4, -60, -33, -45, -33, -4, -2, 112, 34, 2, -26, -40, -11, -2, -22, 25, 15, -21, 11, -40, -17, -45, 75, 86, -17, -5, 93, -51, -127, -8, -49, -44, -81, 17, -78, 14, 0, -127, 3, 6, 16, -7, 8, -6, -2, 14, 72, -2, -17, 4, 11, -21, 16, 1, 5, 2, 7, -21, 0, -33, 39, -5, -21, 3, 63, 83, 25, 5, 4, -24, 19, -1, 102, 37, -9, 12, 8, -14, -18, -1, -1, 40, 87, -4, 66, -6, -122, -5, -127, 42, -14, -41, 78, -52, -19, 23, 7, 39, -17, -40, 4, -20, -48, -16, 17, -21, -127, 6, 2, -59, -48, -52, -7, -77, -15, 7, -6, -62, 18, -20, 35, -8, 5, 62, 13, -26, -39, -12, 10, -105, 17, -7, 12, -49, -7, -23, 1, -12, 0, -127, -63, 36, 14, -7, 27, -1, 81, 35, -98, 64, -56, -28, -33, -8, 1, -15, 9, -62, 16, -42, 52, -37, -34, -80, -34, -127, 9, -38, 22, -4, -92, 25, -89, 44, -6, 55, -3, 47, -7, 11, -19, 1, -8, 7, -56, 2, -127, 25, -14, 62, 29, 105, -10, -31, 72, -32, 3, -49, -47, 61, -40, -14, -79, 16, -98, -82, 20, -70, -52, 8, 101, 8, -109, 127, -84, 23, -86, 30, -18, 10, -72, -13, -55, -52, -54, 21, -78, 31, -58, -37, -5, -43, -32, -27, -21, 39, 40, -44, -96, 33, 16, -127, -23, -10, -22, -10, 1, 27, 3, -39, -54, 20, -94, -59, -103, -95, -51, 49, 9, 1, 125, -25, -10, 42, 20, -32, -10, -43, 13, -44, -49, 9, 127, -20, -32, 19, -17, -40, -51, -7, -48, -26, 10, -25, -43, -5, -45, 43, 15, -82, -20, -34, 68, -84, -26, -65, 30, 13, -78, -21, -16, 21, -33, -98, 91, -42, 127, -66, -108, -77, 18, -23, -19, 18, -12, 98, -51, 21, -27, -11, 4, -2, -11, 3, 30, 44, -38, -67, 10, -96, 76, 24, 36, -6, 67, -19, 0, 48, -98, -5, -127, 62, 4, -61, -1, -23, 13, 2, -50, -114, -94, -39, 18, 15, -61, -23, -3, -10, -43, 125, -62, 70, 28, -66, -106, -26, -63, -38, -3, -127, -44, 4, -21, 3, 29, 4, -49, 44, 28, -4, 127, 56, -18, -89, 8, 15, 46, -32, 13, 33, 40, 21, 9, 32, -37, 37, -42, 71, 1, 40, -74, 1, 24, -27, 37, -78, 21, 37, 43, 50, -49, -23, 91, -35, 87, -6, 40, -50, 82, -16, -114, 102, -127, -73, -116, 43, 11, -10, -43, -62, 38, 3, 5, -59, 18, -31, 104, -7, -120, -41, 43, 21, -50, -56, 11, 11, 59, 17, 56, 17, -127, -44, -116, 44, 6, 76, -71, -27, -27, -117, -113, 9, -82, -13, -3, 43, -46, -64, -73, -55, -17, -38, -3, -67, 0, -17, -15, -60, -127, -75, 8, 31, 8, 127, -33, 42, 40, 3, -48, -24, 14, -44, 60, -11, 1, -11, -32, 25, 93, 25, 44, -47, 53, -37, -66, 1, -86, -110, -26, 20, -33, -62, 95, -2, -9, -97, -66, 11, 1, -55, 42, 37, -53, -17, -34, -44, 37, -53, -37, 73, -51, -118, -20, -7, -67, -27, -6, -127, -61, -44, 54, -67, 18, 20, 26, -78, -41, 38, -10, 22, -127, -21, -8, 56, -104, 18, 4, -73, -63, -47, 13, -2, -46, -8, 19, -48, 4, 44, -85, -27, -25, -8, -8, -8, -58, -33, -66, -5, -127, -89, -82, -69, -25, -12, 37, 53, -36, 12, 35, -36, -54, 34, -25, 25, -55, 98, 13, 32, -12, 19, 63, -43, 8, -22, -4, -55, -46, -2, -12, 15, -12, 60, -10, -23, 78, 64, -1, -15, -33, 2, -127, 26, 17, -50, 29, -55, -116, 53, -58, -46, -50, -9, -47, -66, -26, -78, -51, 31, -56, -120, -42, -7, -41, -94, 31, 39, -109, 51, -50, -41, -127, 43, -62, -11, -7, 0, -65, 32, 0, -32, 98, -127, -35, -23, -8, 28, -28, 16, -28, 27, -2, -70, 38, -98, -29, -17, 33, -26, 35, 11, 51, 30, -16, 6, -29, -73, -54, -67, 3, 24, -50, -32, -26, -39, -2, -27, 23, 121, -53, -44, 4, 8, -127, 5, -12, -60, -23, 1, -75, 30, 5, 7, 35, -7, 18, -26, 99, 17, -44, -40, 9, -24, 28, 29, 13, 39, 34, 36, -41, 9, -76, -23, -127, 76, -19, -28, -22, -16, -14, -93, 11, -42, 106, -8, -34, -108, -51, 43, 97, -35, -48, 3, 11, -42, -39, -15, -4, -61, -67, -127, -9, -33, -122, 64, -84, 57, 28, 17, 13, 33, -12, -18, 12, 61, -1, 10, -46, 25, 0, -9, -18, 47, 23, 19, 28, 107, -127, 18, -37, 65, 65, 109, 5, -32, 60, -37, -44, -69, -33, -7, 30, -20, -127, -12, 16, 37, -31, -13, -5, 5, 94, 26, 5, -53, -103, -73, -20, 13, 9, 6, -28, -15, 12, -47, -23, 17, 30, 1, 69, 12, 11, 14, -16, 12, 2, -23, 26, 71, 0, 20, -22, -19, -127, -11, -63, 90, -51, -13, 84, -38, -71, -11, -120, -48, -70, 78, 45, 51, -11, 21, 63, -64, -70, 64, 6, -30, -127, -25, -17, 70, -113, -64, 15, 51, -7, 70, 103, -2, 9, 41, 110, 25, 11, 40, 62, -5, -83, 77, 114, 7, 43, 105, 51, -21, 100, 15, -49, 7, -127, 27, -44, 13, 58, 36, 6, 83, 7, 0, -50, 41, 36, -12, -3, -10, 40, 14, 16, 15, -28, 22, 28, 2, 105, -23, 5, -29, -72, -28, -127, 22, -20, 32, -84, -18, 28, -20, -57, -61, -6, -26, -15, 22, 15, 4, 25, -9, 69, -15, -36, -127, 47, -38, -14, -18, -58, -18, -18, 41, 1, 8, -11, -10, -30, -11, -15, -127, 73, -65, -30, -8, -3, -61, 53, -4, 96, -21, -5, -42, -25, -2, -69, -71, -23, -55, -54, 51, 9, 28, -68, 14, -2, -15, -97, -14, 35, 11, 4, -24, -58, 13, -4, -56, -127, 59, -3, 98, -2, -46, 45, 23, -80, -23, -94, 69, -95, -36, -84, 21, -41, -58, -6, -29, 6, -30, -2, 79, -9, 29, -42, -83, -25, 11, -76, -21, -19, 127, -99, 42, -68, -43, -30, -17, -84, 34, 53, 32, -25, 26, 25, -37, 32, -15, 58, -18, -17, -5, -11, 5, 17, -7, 6, 5, 27, 6, 48, 16, -127, -21, -34, 14, 69, -59, 62, -127, 18, 71, 102, -64, -20, 5, 73, 30, -65, -35, 25, 28, -80, 12, -8, -41, -88, 19, -27, 1, -18, 40, -59, -10, -11, 20, -93, 50, 77, -3, -49, -82, -40, 51, 39, -50, -67, 19, -20, 26, 127, 80, 8, -43, -1, -10, 12, -19, -81, -30, -61, -18, -33, -86, 18, -32, -16, -36, -48, -75, -99, 22, 120, 3, -65, 63, -91, 23, 41, 59, -21, -15, -18, -11, -82, -76, -127, -11, -27, -1, -76, -71, 14, 15, 71, 59, 93, 37, -72, -49, -64, 35, -44, -56, -85, 29, 65, -86, 38, 49, -77, 8, -38, -127, -38, -93, 64, 16, 20, -31, 60, -115, 1, 11, 1, 21, -76, -3, 15, 44, -52, -65, 7, -43, -30, -20, -20, 26, -21, 19, 23, -80, -43, -57, 127, 28, 15, 29, 45, 51, -23, -67, 7, 8, 29, -112, 9, 1, 36, -70, 59, -17, -80, 17, -32, -127, -35, -72, -19, -65, -43, 47, -5, 3, -35, -25, -22, 8, -9, 24, -21, -16, 43, -6, -11, -26, -47, -47, -33, 22, 96, -4, 35, -3, 82, -33, 45, -41, -50, -29, -127, -9, 7, 8, 14, -34, -39, 30, 36, -87, 1, -2, 52, 45, -32, -6, -26, 48, -89, -2, 0, -127, -56, 15, 0, -22, -63, -13, 32, -30, -13, 3, -59, -121, -20, 38, -17, 57, 22, -25, 39, 25, 106, -42, -1, -59, -16, 48, -2, -70, 95, -33, 12, 0, -44, -5, -127, 30, -88, -9, -9, -8, 70, -28, -37, -5, -24, 11, 43, 18, -45, 19, 72, 12, 37, -42, -12, -81, -127, -8, -29, -85, 98, -6, -31, 7, -108, 2, -58, 14, 23, 56, 3, 127, 18, 3, 7, -10, -58, -73, -21, -58, -10, -42, -79, -40, 5, 39, 2, 30, -53, -20, -51, -58, -29, 1, -30, -50, -58, -43, 111, -77, 70, -110, 72, 9, -35, -85, 8, -81, 99, 3, -24, -77, 83, -52, 31, -98, -80, -127, -63, 88, 27, 54, 3, 68, 70, 110, -44, 78, 58, 30, 84, -46, -36, 52, 45, -26, -47, 6, 43, 19, 127, -11, -68, 66, -18, 13, -39, 75, -90, -36, -15, 3, -63, 61, -30, 41, 58, 92, 82, -11, -32, 37, 127, -11, -43, -11, 43, -23, 57, -26, -42, 29, -16, 6, -72, 50, -89, 5, 127, -70, 60, 57, -53, 46, -87, 92, 39, -57, -108, -64, 71, -88, -11, -39, -37, -86, -78, 10, -72, 60, 1, -75, -69, -119, -77, -1, -25, 12, 32, 26, -81, -52, 27, 1, -44, 0, -22, 26, 41, -8, 32, -4, -33, 81, 70, -10, -1, -37, -96, -45, -127, 64, -15, -127, -31, -27, -9, -39, 25, -35, 18, -11, 1, -41, -48, -9, 79, -1, -2, 6, -23, 2, -65, -11, 19, 12, -40, -24, 1, 5, 25, 10, -57, -35, -15, -127, 9, -3, 45, -16, 119, 0, 31, -118, 26, 11, -78, 20, -56, -69, -65, -18, -42, -62, -60, -9, -119, 12, 3, 31, -29, -74, -25, -52, 25, -4, -22, 32, 58, 21, 19, -3, 48, -59, -97, 91, -26, -46, 127, 6, -46, 10, -37, -40, -6, 75, 7, 12, -62, 56, -23, 3, -63, 51, 24, -32, 14, -18, 47, -52, 48, -31, 10, 55, 7, -114, 127, 21, 11, -11, -51, -8, -110, 28, -74, -64, -49, -107, 35, 34, -19, 27, -97, 20, -103, -41, -25, 27, 63, 39, -24, -67, -9, -14, 127, 8, 97, -61, -4, -62, -89, -19, 13, 2, -37, -75, 61, -23, 95, -45, 12, -69, -82, 127, -58, 82, -90, -34, -72, -78, -2, -49, -43, -15, -3, -25, -127, -62, -10, -4, -74, 29, -30, -19, -39, -26, -29, -53, -19, -106, 31, -8, -99, -2, 127, 18, 81, -96, 24, -55, -11, -90, -12, -77, -60, -15, -18, 4, -60, -1, 63, -34, 29, -109, -9, -29, -16, -92, 6, -29, -55, -32, -67, -50, -39, 8, -26, 58, -127, -75, 33, 11, -23, -6, -21, -81, -90, -56, -80, -48, -20, 101, 33, -94, -9, -77, -24, -41, -63, 84, -6, 72, -127, -13, 39, -94, -25, -22, 36, -43, 38, -55, -114, 31, -35, -4, 20, -30, 34, -80, -53, -25, 22, 26, 98, -20, -51, 29, 16, 20, -5, 34, 55, -106, -49, -85, -127, 74, -56, -18, -7, -15, -15, 12, 2, -58, 7, 4, 72, 15, -27, -31, 38, -16, -12, -95, 15, -73, -127, -3, 12, -103, 81, 6, -56, -34, -25, -15, -93, 14, -1, -17, 14, -36, -67, 21, -22, -27, 81, 127, -35, -18, -14, -23, -6, 1, -12, -3, -56, -27, 38, -55, -29, -7, -6, -12, -41, -20, 9, -45, -57, 10, -54, 21, -43, 76, -19, 9, -31, -36, 72, -18, -48, -25, -127, 33, -68, -90, 118, 46, 31, -29, -55, -31, -120, -20, -9, -95, 55, 10, -4, 46, 13, -28, -24, 14, -3, -5, -17, 0, 35, -5, -127, -22, -2, -66, 17, -18, 64, 16, -106, -57, -55, -15, 65, 45, 12, -127, 30, 13, 33, 32, -36, 40, -12, 2, 15, -13, -18, 23, -3, 12, 9, 51, -93, -39, -34, -96, -32, -98, 50, -92, -66, 2, -36, 34, -3, 12, -3, 38, 40, -11, -7, -22, -26, -27, -1, -33, 40, -24, 20, -63, 53, 11, 10, 9, 2, -2, -127, 35, -10, -114, 127, -64, -7, -37, 30, -67, -9, -21, 29, -105, -80, -5, -95, 35, -1, 26, 22, 92, -98, -71, -103, -104, -89, -79, -84, -25, -56, -17, -66, -127, 28, 65, -51, -54, -17, 26, 4, 0, 8, 43, -41, -73, -4, -2, -53, -16, 8, -5, -19, 18, -4, -59, -21, 58, -84, 20, 60, -4, -5, -23, -20, 10, -51, 14, 127, -110, -75, 74, -51, -30, -23, -27, 0, -16, -79, 31, -36, -45, -73, 23, 65, -6, -60, 46, -12, -32, 8, -13, -35, 127, -2, -16, 11, 126, -39, 72, 59, -29, 96, -6, 49, -33, 70, -33, 23, 51, 41, -11, 1, 65, 68, -23, -84, -67, -61, -36, -53, 127, -60, -21, 29, -65, 15, -42, -70, -13, -95, -123, -88, -51, 42, -3, -12, 17, -11, 16, -36, 52, 65, -88, 60, -40, 75, 28, 24, -69, -16, -110, -127, 51, -36, 32, -30, 0, 76, 20, 68, -90, -68, -41, -121, -118, 14, -5, -11, 51, -62, 81, -77, 0, 7, 43, -83, -127, -86, -84, 49, -1, -51, 51, -120, 57, -42, 92, -124, -20, 46, 92, -81, -6, -99, -63, -108, -117, 86, -69, -40, -18, 24, -16, -6, -104, -28, 13, -3, -31, 38, -14, 8, -48, 9, -40, 44, -38, 18, -66, 29, 11, -127, 8, 8, 77, 22, -59, 52, -115, -8, 9, 4, -34, -118, -17, 59, -5, -127, 37, -27, 60, -2, 7, 9, -54, -98, -72, -94, -17, 34, -33, 110, -40, -35, 30, -33, -49, 2, -18, -17, -81, 13, 1, 16, 17, 29, 102, -22, -22, -38, 92, 56, -6, 127, -73, 36, 11, -53, -70, -101, 45, -110, 103, -75, 10, -31, -11, -105, 80, 11, 16, 58, 5, 89, -12, 52, -62, 13, 42, 60, -56, -69, -22, 67, 121, -101, 33, -71, 127, -1, 2, -39, -127, 14, -6, -32, -27, 92, 5, 98, -46, -36, 17, 101, -45, -65, 17, -23, 50, 99, -1, -64, 45, -102, -18, 19, 94, 11, -15, 48, 17, -55, -42, -42, 12, -56, 35, 68, 26, -29, -63, 73, -94, -5, 34, 6, -59, -13, -26, 20, -16, -127, 46, 22, -55, -28, -22, 24, -83, -19, 44, 53, -27, 68, -47, 63, 0, -51, -20, 61, 7, -120, -12, -16, -127, -30, 19, -55, 3, -52, 0, 6, -6, 12, -127, 42, -44, -7, -10, -63, -106, 30, -115, 12, -20, -19, 32, 25, 50, -53, 41, -80, -46, -54, 19, 7, -9, -76, 11, -48, -104, 16, 86, -2, -92, 30, 28, 7, -38, -3, 36, 73, -12, -18, -1, 26, -28, 0, 49, 9, -31, 52, -20, -127, -4, -70, -25, -5, 82, -57, 67, 8, -26, -5, 15, -15, -20, 49, -28, 30, 51, -54, -24, 47, 23, 88, 7, -28, 16, -78, 26, -127, -46, -76, 0, -25, -23, 7, 37, -61, -12, 54, 8, -51, 15, 44, -27, 77, 15, -49, -34, -22, 26, -16, -57, 40, -24, -53, -45, -57, 121, -127, 42, 6, 53, -73, 79, -40, -49, -21, -16, -1, -72, -51, 21, -4, 4, -56, -28, 24, -24, 21, 22, 17, -127, 16, 38, -5, -10, -95, 35, -19, 19, -13, 86, -10, 40, 16, -20, 82, 8, 36, -33, 45, 97, -33, 5, -16, -23, 127, -83, 31, -78, 58, -3, -56, -56, 33, 33, -7, 29, 85, -35, -9, -3, 15, 42, 16, 2, 15, -76, 41, 29, -11, -97, -47, -12, 2, -27, -38, -70, -16, -18, -48, 34, -127, -16, 16, 11, -12, -56, -23, -56, -37, 55, -17, -89, 10, -22, -9, -5, -52, -49, -111, 35, -26, -15, -3, 86, -127, 34, 28, -32, -78, -7, 58, -11, -20, 31, 54, 55, 9, -40, -22, -7, -39, 29, -25, 21, -72, 25, 43, -9, 49, 73, -29, 52, 8, -13, -56, -69, -127, -17, -69, 18, -12, -41, -1, -20, -13, -36, -51, -27, 17, -1, 27, 20, -50, -33, -49, -20, -64, -78, -25, -21, -18, -41, 7, 6, -31, 127, 43, -46, -66, -12, 36, -91, 38, -9, 6, -51, 21, 8, 49, 24, -17, 10, -12, 39, -10, -43, 38, 36, -127, -33, -61, 33, 29, -14, 95, 4, 13, 2, -25, -6, -11, -70, -16, -16, -47, 22, -40, -10, 2, -7, 127, 49, -37, 4, 23, -55, 18, -21, 32, -47, -72, -23, -17, 3, -85, 84, 26, -23, -25, 5, -21, -42, -4, -14, 15, -15, 22, -78, -26, 28, 7, -61, 28, -127, 5, -27, 12, 13, -92, -62, -5, 42, -13, -71, 12, 60, -56, 86, -20, 18, -43, -52, -7, 35, 17, 11, 6, -19, 3, 49, 25, -62, 22, 37, 47, -95, -127, 2, 47, 12, 63, -2, -25, -26, 5, -47, -20, 47, -48, -11, 48, 32, 36, -67, 11, -17, -77, 38, -12, -76, 57, 4, -10, -127, 1, -33, -85, 51, -2, 72, 3, 8, 28, 14, 12, -21, 7, 32, 80, 14, -127, 27, 40, -38, -1, 20, 8, 7, 4, -25, -22, 21, -4, -21, -26, 41, -24, 27, 50, -54, 9, 30, 30, -127, 9, -18, 20, 51, -62, 32, 119, 63, -78, 69, 26, 19, -57, -10, 13, -28, -114, -10, -35, -8, 6, 56, 43, 2, 21, 62, -25, -127, 23, 26, 55, -9, -48, -20, -1, 4, -127, 124, 58, -70, 60, -34, -108, 37, -13, -78, -19, 107, -34, 60, 21, 14, 4, 3, -36, 93, 71, 13, 71, 11, 33, 28, 43, -29, -86, 23, -15, -35, 47, -32, -42, 88, -127, 32, -74, -11, -124, 64, -16, 13, -48, 33, 79, 47, 15, -30, 60, 18, 10, -97, -28, -41, 68, -116, -1, -77, 97, -25, -3, -56, 0, 10, -127, 46, -32, 0, -33, -37, -26, -20, 5, 20, -8, -48, 67, 7, 18, -54, -25, -74, 127, 30, -15, 20, 3, 12, -15, -35, -11, -5, -25, 42, -17, -28, -8, -83, 18, 26, -21, -45, -17, 60, 71, 72, -8, 80, 119, -26, 16, -47, 75, -42, -20, -45, -15, -88, -127, -19, 3, 23, 26, -62, 54, -22, -49, -1, -5, 24, -7, -36, -12, -5, 2, 19, 11, 45, 2, 27, 0, 71, -12, 9, -71, -61, -127, -28, -62, -45, -16, -53, -26, -85, -83, 111, 106, -56, 59, -42, 59, -73, 33, -23, -48, 23, -127, 60, -9, 34, -54, -43, -124, 86, -78, -17, -84, -57, -60, 35, -40, 4, -59, 17, -32, -17, 29, -28, 27, 6, -64, -38, 42, -23, -21, -41, 4, -37, 75, -5, 39, -15, -73, -2, -5, 127, -20, 25, -24, -127, 5, -13, -5, 35, 22, -21, 56, 5, 116, -47, 19, -46, 8, 10, -11, -47, 41, -27, -9, 2, -23, -10, -31, -8, -56, -24, -16, -14, 55, -18, -28, -65, 72, 30, -22, -44, 12, 26, 127, -79, -41, 58, -64, -73, 63, -21, -74, -115, -29, -72, -48, 35, -52, -51, -12, -112, -10, 41, -60, 24, 54, -23, 51, -2, -37, -4, 18, -19, 16, 16, -59, 127, -23, -15, 57, -1, 10, -41, -51, -13, 16, -34, 71, -52, 7, -23, -1, -1, 48, -26, 117, 33, -58, 61, -7, 108, -28, -19, 25, -78, 6, -55, -108, -127, 21, -12, -25, -57, -35, 26, 26, -12, -11, -28, -4, -76, 38, -18, 77, -19, -58, -24, -22, 1, 6, 9, -9, 25, -64, 2, -12, -13, -73, -127, 22, -12, -7, -46, 127, 21, 12, -59, -66, -12, -60, -21, 9, -3, -66, -8, -15, -61, -23, 0, -1, -41, -73, -102, -33, 40, -29, -41, -14, -23, -27, 4, -85, 18, 14, 11, -10, 15, 37, 3, -9, 2, 28, 38, -15, 5, 15, -28, 45, -71, 127, -19, -25, -58, -111, -66, -62, 2, -17, -54, 127, 124, -28, 20, -19, 40, 40, -86, 91, 7, -28, -22, 33, 48, -112, -13, 41, -95, -78, -27, 75, 23, -66, 72, -9, -6, -61, 1, 11, -31, -24, 66, -64, -37, -4, -10, 11, 34, -127, 25, -6, 16, -6, 22, 16, 2, -90, -13, 65, 48, -59, 28, 53, 6, 35, -82, 3, -64, -26, 0, -17, -127, -2, -90, -2, -30, -10, 35, -79, -36, -87, 3, -83, -74, 21, -26, -56, -75, -47, -11, -55, -22, 10, -55, -28, -38, 38, 54, 5, -53, -3, 17, 61, -100, -111, 37, -74, 6, -92, 50, -34, -43, -126, -127, 36, -119, -65, -49, -23, 39, -67, -34, 34, 32, 56, 7, 37, -11, 28, -19, 38, -10, -6, -35, 30, -35, 16, -71, 20, -10, 69, -28, 20, -23, -26, -43, -127, 26, -27, -43, -73, -47, 23, 82, 27, -103, -51, -39, -43, 32, -28, -127, -80, 17, 61, -69, 111, 19, -49, -32, 46, -62, -66, -113, 64, 44, -13, 68, -127, -48, 29, 66, -70, 51, -3, 53, 90, 2, 16, -57, -43, -18, -83, 14, -30, 30, -12, -4, 63, 8, -114, 65, 52, 52, -6, -25, -62, -127, -26, -28, 36, 14, -67, 16, -27, 34, 71, 35, -9, -9, -8, 10, -1, 28, 7, -3, -84, -64, -12, -82, -3, 24, 11, 30, 36, -27, -60, 23, 32, -78, -38, 3, -13, -21, -55, 8, -127, -14, -93, 4, 16, 88, -48, 2, -97, -29, -54, 40, 45, 39, -14, 54, -18, -6, 75, 16, 14, 42, 4, -3, 127, -19, -37, -36, 9, -101, -11, -85, 19, -43, 9, -6, -9, 18, 1, 10, 39, -4, 0, -64, 44, -16, -28, 25, 24, -15, 13, -109, 54, 75, -29, 17, -12, -30, 49, -49, -102, -9, -86, 62, 127, -79, -53, -55, 95, -18, 65, -15, 100, -23, -13, 29, 13, -23, 5, 22, 52, 3, -73, 11, -127, -39, 57, -59, 64, 71, -125, -84, -85, -57, 6, -84, 68, -19, -30, 37, 70, -53, -11, 4, -83, -41, -59, 17, -36, -100, -40, 55, 39, -65, 6, -32, -127, -51, -9, 92, -29, 16, -17, 30, -42, 101, 20, 21, 13, 17, 3, 0, 19, -16, 44, -46, 87, -27, 10, 15, 40, -18, -24, -92, -8, -10, 127, -41, -17, 22, -61, -51, -62, -12, 77, -79, 19, -15, -3, 70, 34, -24, 40, -88, 23, 3, 22, -77, 9, -4, -50, -33, 36, -56, 36, -24, -66, -8, -127, -76, 26, 29, 42, 72, -69, 41, 34, 10, -9, 13, -8, 58, 31, -22, 4, 49, 36, -127, -19, 45, 36, -54, -31, -39, 23, 16, -36, 49, -27, 30, -50, -23, 8, -38, 22, 9, -52, -49, -2, 31, 10, 1, -51, 48, -21, 43, 15, -31, -2, 66, -127, -13, -51, -5, -4, 9, 55, -23, 7, -62, 10, -112, -31, 44, -33, 96, -17, -26, -5, -32, 114, 9, 88, -50, -17, -31, -18, 11, 48, -113, 43, 18, -127, -4, -60, 11, -75, -37, 2, -48, 41, -18, -127, -64, -80, -78, -16, -7, 3, -65, -94, 10, 15, -61, -32, -72, 36, -89, 0, -28, -58, -22, -22, 8, -5, -7, 3, -48, 55, -44, -30, -20, 127, -6, -41, 48, -64, 25, 16, -75, 14, 7, -4, -29, -15, -46, -53, -45, -33, -15, -12, -11, 62, -29, -68, -57, -6, -50, -88, -5, 26, -54, 22, 39, -103, 3, -18, 48, 9, -77, -54, 13, -81, -38, -1, -14, -113, -74, -83, -127, 14, 9, -33, -27, 48, -47, -3, 17, -3, 10, 2, -44, -33, 29, 1, -8, 60, 23, -13, 1, 50, 55, -28, -61, -14, 20, -127, 25, 34, -42, -22, 14, -124, 17, 9, -2, -89, 123, 127, -81, 12, -76, -18, 71, -33, 2, 21, -15, 15, -2, -13, -54, -13, -52, -19, -19, -32, 86, -23, 71, -2, -31, 11, -16, 22, 20, 29, 25, -5, -33, 18, -15, -41, 28, -27, -26, 114, -18, -48, -68, -70, -19, -127, 66, 43, 94, -19, -119, 25, -34, 13, 52, 23, -30, 23, -32, 87, -23, 17, -48, -3, 67, 60, -127, 76, -97, -9, -4, -118, -108, -93, 124, -100, -23, 5, -90, -7, 64, 48, -91, 32, 6, 109, 8, -67, -44, 71, 46, -127, 42, -58, -96, -93, 7, -14, -119, -58, -39, -35, -103, 36, -6, -7, 36, -30, 8, -48, 5, 43, -23, 54, 57, -2, -17, 18, -42, 86, 50, -2, 6, 34, -22, -87, -6, -127, -89, -88, -25, 32, -22, -37, -112, 15, 38, 24, 48, 82, -57, 127, 36, 4, -55, 63, -112, 13, -13, 32, -23, 43, -36, -55, 44, -26, -74, -92, 72, -72, -24, -5, -32, 127, 31, -18, 14, 51, -52, -55, -7, -46, 2, 6, -27, -63, 59, -26, -25, 33, -25, -13, -39, -11, 31, 18, 3, -32, -54, 60, -16, -127, -40, -71, -7, -9, -23, 66, -12, -75, -44, 22, 39, -13, 29, -72, -9, -31, -88, -70, -68, -40, -63, 24, -40, -33, -47, 61, -61, -1, -33, 17, -26, 8, 22, -4, 4, -27, 24, -10, 94, 36, -23, 22, 40, -63, -18, -127, -58, -26, 62, -66, 56, -44, -32, -68, 38, 2, 10, -30, 31, 2, -36, 22, -8, -70, -127, -84, 5, 26, -116, -37, 22, 4, 7, -26, -14, -88, 37, -4, -23, 96, -66, 24, -40, -127, 38, 6, -68, 12, -63, -64, -48, -67, -93, -15, -113, -87, 46, 70, -95, 34, -30, 4, -81, -49, -24, -58, 21, 30, -63, -61, -3, -7, -10, 55, 25, -32, -20, -60, -32, -121, -74, 41, 45, 127, -9, -46, 0, 13, 38, -8, -72, 35, -66, 46, -11, 25, -20, -30, -37, -45, 94, 11, 29, 105, -1, 9, 44, 127, 41, -43, 59, -61, -22, 45, -19, 56, -15, -87, -37, -67, -22, -59, 71, -75, -21, -7, -37, -14, -15, -37, 5, -1, 1, 27, -2, 11, -127, -29, -8, 32, -20, -20, 50, 27, -1, -20, -51, -31, 29, -85, 32, 10, 53, 44, -65, 58, 80, 113, -75, 82, 5, 10, -18, -22, 75, -38, -99, 59, 25, 77, 92, 36, -39, 76, -127, 65, 70, 68, 113, -50, 28, -32, 17, -9, -53, 21, 50, 31, -18, 63, -23, -3, -44, -27, -37, -27, 13, -14, 13, 127, -3, -116, 5, -39, -41, -72, 22, -20, 28, 0, -49, 30, 29, -32, -24, -27, 52, 37, 0, -127, 17, -11, 84, -36, 22, -20, 68, -77, 7, -55, -15, 9, -47, 50, -120, 12, -21, 18, -18, -19, -102, 24, -32, 52, -66, 44, -44, -27, -9, 32, 13, -127, 56, -48, -56, 17, -5, -121, -22, -56, -3, -55, 30, 38, -34, -14, -52, 13, 21, -74, 24, -47, -7, 47, -127, -60, -11, -38, -34, -83, 19, 28, -11, -36, 15, 9, -77, -39, 42, 11, -75, -47, -30, -20, -5, 0, 25, 31, 127, -39, -1, -39, -10, -53, 5, 21, -7, -16, -30, -1, -42, 23, -10, -41, 45, -46, 11, -12, -71, -11, -38, 15, -48, 4, -20, 40, 25, 63, 58, 44, 14, -55, 16, -9, 37, -60, 57, 1, -43, 127, -70, -10, -93, -113, -42, -40, 20, 0, -45, -1, -56, -58, 81, 93, -123, -41, -23, 34, 12, -84, -39, 48, 31, -94, 6, 4, -127, 20, 7, 39, -53, -85, 7, -110, -7, 17, 4, 21, -4, 96, 24, -38, -21, -41, -127, -3, -21, -4, 16, -21, -9, 8, -1, 32, -1, 16, 35, -15, -10, -15, -20, -11, -26, -21, 13, -37, -11, -8, -19, 22, 9, 25, -20, 21, -31, -62, -22, 16, 8, 20, -32, 12, -17, 127, -11, 6, -27, -2, -15, -61, 6, -38, 91, 60, -57, -42, -46, -13, -117, 21, -11, -54, -35, -38, 1, 20, -11, 27, -75, 5, -10, 12, -44, 16, -3, -127, -68, -22, 30, -32, 15, 12, 127, -20, -33, -50, -57, 20, -32, -6, 8, -74, 0, 4, -36, -40, 20, -21, 11, 52, -66, 16, 0, -79, -14, -66, -8, -23, 6, 7, -20, 4, -3, -26, -50, 25, 34, -7, -4, -13, 43, 32, -20, -98, 51, -17, 127, -21, -11, 6, -43, -47, -39, -48, -15, -9, -127, 39, -45, 3, 8, -62, 8, -17, 32, -4, -2, -74, 31, -56, 54, -96, -15, 9, 40, -126, -32, -79, -61, 0, 5, 98, -19, 49, 33, -50, -120, 89, 15, -68, -115, 27, 36, 20, -79, -127, -41, -86, 71, -31, -9, 43, 23, -126, -2, -83, 13, -70, 84, 37, 12, -66, 109, -15, -9, 127, 55, 85, -76, 3, 51, -7, 53, -127, 19, 11, -66, -39, -41, -41, -12, 16, 14, -55, -12, -76, 21, -15, -10, -108, -5, 71, 127, -14, 44, -8, 49, 31, -45, 63, 8, -74, 18, 24, 1, -97, -44, 75, -127, -16, -11, 22, -24, -43, 71, -52, -16, -8, -58, -21, -100, -30, -88, 6, -22, -34, -45, 9, -118, -79, -101, -35, -38, -107, -35, -87, -127, -9, 0, -90, -67, -13, -98, -20, -67, 26, 79, -18, -70, 11, 31, 0, -26, -11, 45, 16, 9, -65, 31, -16, 12, 2, 24, 39, -18, -127, 11, -78, -11, -45, -2, 26, -38, 24, 50, -32, -6, -94, -10, -27, 0, 47, -48, -11, -51, 14, -102, 4, 16, 20, 112, -85, -13, 68, -9, -88, 87, -127, 28, -44, 44, -17, 2, -47, -25, -5, -1, -3, 16, 56, -9, 20, -24, 86, 0, -4, -75, -20, 44, 9, -20, 77, -10, -10, 78, -87, 43, -127, 54, -13, -19, 53, 26, 29, -29, -29, -20, -44, 33, -28, 14, -127, 57, -35, 15, -39, 16, 15, 71, -75, -31, 17, 6, -23, -35, 75, 35, -3, 12, 39, -15, 13, 14, -5, -20, -27, -29, 66, 1, -53, -10, 30, -15, 36, 35, 57, 17, -11, 26, 60, -2, -60, 52, 37, 127, 10, -29, 16, 16, -35, -71, 39, -56, -11, -107, -92, 52, 2, 36, -16, -2, -31, -50, -46, 30, -31, -12, 30, 45, -40, -86, -95, 2, -127, 40, -38, -127, -20, -39, 4, 19, -24, 23, 45, 9, 96, -60, -1, -50, -42, 47, 14, -11, -5, -38, 0, 10, -24, -37, -37, 30, -68, -21, 34, -127, 0, -7, -4, -39, -5, 12, -7, -16, 0, 56, 15, 37, -55, -23, -9, 65, -47, 11, -18, -6, -24, -7, 47, 30, 14, 126, -29, 21, -27, 20, -38, -70, 70, -12, 63, 15, -38, -48, 9, 41, 15, -18, -11, -25, 127, 9, -17, 12, -69, -26, -90, 96, 22, 41, -74, -75, -12, -75, -73, 36, -57, -100, 3, 4, -115, -92, 20, -83, -42, 50, -67, -50, -20, -89, -88, -78, -114, -42, -127, 14, 76, 12, 23, -1, 21, 16, -11, 9, 46, -38, 60, 82, 25, 33, 23, 19, 127, 24, 14, 26, 10, -60, -24, -39, -85, -20, -82, 78, -5, 51, -76, -27, -115, -31, -47, 75, -39, -12, -19, -97, -80, -113, -91, 8, -127, -69, -33, -26, -45, 77, 37, -58, -89, -45, 59, -35, -103, -52, 5, -58, -5, -24, -68, -53, -3, 23, -22, -29, -46, -3, 15, -30, -5, -1, -18, 127, -39, -18, -11, 11, -20, -12, -23, 22, 24, 93, -19, -35, -14, -51, 1, 127, 9, -61, 121, 22, 113, -104, -51, -84, 96, -47, 37, 24, 72, -103, -77, 78, -35, 94, -119, 24, -62, 18, 16, 100, -34, 40, 20, 6, 4, -38, -39, 0, -88, -18, 1, -2, -20, 4, 17, -15, 68, -48, 71, -35, -127, -56, -113, 24, -23, -67, -64, 42, -78, 34, -52, -6, -97, 49, 36, -82, 3, -74, -53, 16, 28, -3, -31, 59, 8, 9, 20, 10, -58, 11, 28, -30, -127, -67, 6, 19, -66, -51, -72, -31, 68, -57, -2, -43, -25, -70, -69, 24, -74, -97, -48, -14, -43, -15, -2, -30, -99, 15, 127, -69, -78, 117, 29, 69, 41, 59, -28, 3, 60, -113, 47, 4, -56, 12, -85, -127, -55, 103, -42, 35, 64, 19, -23, -14, -84, -69, -92, 17, 16, 8, 34, 14, 32, 4, -19, -10, -75, -35, 127, 67, -52, 47, -71, 21, -58, 22, -33, -67, -47, 122, -14, -16, -8, -63, 9, -58, -54, -48, 10, -52, 2, 29, 76, 3, -6, -20, 10, 27, -36, -54, 43, -33, -127, -5, -34, -114, -23, 30, 14, -27, -35, 1, -90, -21, 31, 4, 12, -12, 26, 55, 27, -64, 1, -26, 37, 45, -64, -66, 41, -23, -44, -28, -42, -77, -109, 42, -21, -127, 61, -61, -34, -9, 60, -61, 85, -16, -6, 11, -9, -34, -86, 34, 1, 17, -12, 34, -33, -12, 2, 66, 38, -11, -50, -70, 11, -27, -127, 64, 7, -36, -48, -62, 28, 20, 42, 1, 125, -93, 76, -127, -13, 18, -87, -66, 106, 63, -21, -76, -86, -51, -9, 97, 116, 9, -93, 64, -123, 69, 99, -27, -10, 10, 28, 37, 35, -12, 59, -127, 42, -48, 5, 11, 12, -31, -41, 16, -25, -39, 8, 105, -17, -38, -41, -9, -18, -33, -40, -42, 39, -25, 18, 36, 42, 26, 18, -7, 127, 38, -80, 14, 39, 37, -54, 13, 35, -48, -4, 3, 18, -3, -95, 26, 0, 50, 48, 91, -13, 15, 38, -13, -46, 18, 7, 3, 72, -5, -73, -76, -17, -49, 127, 59, 41, -48, 91, -22, -45, 23, -125, -19, -56, 79, -39, 21, 5, -72, 29, 7, -16, -7, -60, 33, -18, 42, 113, 21, 45, -2, 11, -26, -17, 18, -48, 53, -51, -9, -127, -54, 62, 7, 37, -21, 12, -84, -67, 52, 22, 48, 72, -34, 112, 40, -70, -17, 62, -4, -127, 20, -82, -97, 31, -54, -31, -22, 7, -36, -53, 48, -2, -52, -43, -41, 19, 61, 33, 47, -63, -70, 69, 47, -45, 21, -77, -23, -127, 51, -19, -79, 37, -81, -74, -48, -43, 60, -62, -14, -38, -100, 60, -65, 20, 1, 28, 59, 32, -51, 66, 46, -72, -87, 77, 57, -127, 6, -37, -108, -18, -14, -99, -66, -39, 22, -45, -47, 18, -70, -64, -73, -11, -99, 7, 60, 17, -127, 2, -122, -86, -51, 22, -11, -37, -69, -38, -30, 126, 12, 68, 75, 25, 51, -96, -16, -35, 27, -5, -24, -15, -21, 10, -1, -48, -1, -9, -24, -28, -20, 13, -4, -54, 21, -35, -32, 41, -6, -20, -42, -127, -36, -83, -40, -43, 127, -35, -36, 27, -32, 54, -32, -11, 19, 61, 2, -24, -118, 19, -26, -13, 4, 21, -87, 48, -26, -55, 10, -17, -34, -65, 19, -77, 70, 5, 80, 0, 14, 10, -26, 3, 0, -18, 1, -12, -14, -15, 17, 13, 15, -15, -99, 74, -39, -31, -13, -80, -28, -127, 22, -32, 18, -36, -125, 5, 64, -42, 27, -11, -19, 28, 14, 34, -12, 20, -105, -46, 21, -14, -23, 65, 11, 35, 61, -25, -14, -82, 127, 15, -65, 27, -53, -50, -46, -74, -10, 35, -13, 125, 39, -111, 39, -30, -40, -53, -41, 127, -67, 31, 106, 3, -84, 29, 97, -95, -87, -41, 32, 18, 22, 19, 91, -15, -89, 53, 4, -14, 0, 46, 45, 24, -48, 14, 69, 37, -44, 20, -33, 67, -32, -127, -87, -81, -8, -119, -37, 34, -31, -113, -45, 28, -23, 22, 127, 69, 15, -29, 2, 35, -20, 7, -38, -26, -16, 13, -68, -39, -17, 20, -64, -93, -22, 8, -32, 43, -13, -35, 10, -5, -127, -38, -15, -1, -26, -39, 65, -53, 44, 36, -46, 25, 91, -116, -30, -83, -15, -13, 12, 66, -2, -9, 24, 95, 12, 28, -21, 54, -17, 127, 9, 76, 21, -61, -62, -4, 46, -27, 57, 38, 19, 52, -3, 87, 34, -60, 14, -44, 82, 81, -25, -50, -29, -83, -14, 2, 61, -51, -12, 24, -118, 59, 32, 31, -127, -41, 41, -43, -36, 82, -11, -114, -6, -20, 32, -118, 61, -126, -47, -45, -30, -93, 43, -28, 13, 25, 93, 108, 20, -127, 87, -11, 107, 48, 5, 12, 21, 53, 32, 38, 15, -86, 37, 43, 52, -50, -38, 70, -20, 2, 97, 8, 1, -43, 12, 3, 37, -15, 127, 77, -12, -69, -17, 31, -55, -109, -24, -17, -63, 70, -49, 13, -37, 10, -54, -18, 15, 3, -13, 28, -38, -56, -13, 64, -34, 8, -14, -26, 11, -2, -1, -30, -22, 8, 127, 3, 0, -11, -10, -7, -27, -88, -32, 8, -3, -15, -1, -47, 37, -70, -53, 102, 98, -127, -70, 20, 18, -12, 53, -24, -19, -69, 45, 55, 15, -31, -40, -13, 69, 103, -43, 12, -63, 7, -37, -44, -59, -27, -1, -22, 1, -50, 66, -82, 78, -52, -48, 18, 103, -127, -74, -113, -3, -4, -20, 85, -63, -43, 127, 22, 59, -20, -115, 6, 91, 34, -66, 20, -55, 72, -54, -9, -83, 16, -21, -97, -51, 16, 2, -6, -32, -92, -42, -64, 6, -32, 55, -25, -9, 9, 2, -51, -18, 32, 12, 55, -4, 54, -13, 12, -8, 44, 72, -20, -30, 84, 5, 9, 21, -39, -2, -49, 127, -82, -85, 6, -11, -29, 41, 127, -5, 26, 7, -55, -29, 22, 29, -29, 59, -16, -41, 41, 4, -65, -54, -9, -72, -54, -76, -47, -105, -88, 50, -12, -22, 8, 72, -15, -86, 42, 62, 28, -29, -68, 25, 61, -71, 2, 24, -7, 127, -5, 3, -52, 26, -19, -11, -1, -125, -21, 30, 48, 29, 18, 26, 58, -11, -42, -18, -2, -7, -28, 6, -3, -40, -37, -46, 10, -53, 105, -6, -29, -100, -112, 2, -127, 49, 23, -30, 11, 16, -43, -47, -34, 22, -23, -51, 53, 18, -26, 34, -54, 28, -65, 24, -38, 48, -43, -32, -72, -54, -127, 109, 42, -7, -78, 4, 24, 43, 29, 49, -61, -53, 70, 0, 73, 23, -19, 34, 29, 33, -88, -1, -1, -16, 44, 66, 30, 54, -127, 11, -52, 40, 11, 36, -52, -66, -47, -49, -32, -60, 60, -9, 13, -49, -46, -29, -1, -47, -40, -72, -31, 15, -77, -16, 48, -17, -127, -34, -71, -40, -30, -38, 2, -5, 37, -39, -68, -67, 50, -1, 1, -5, -8, -12, 36, 36, -108, -42, -55, -67, -41, -34, 39, -21, -68, 127, -6, -5, -27, 25, 30, -96, -5, 11, -36, -43, -66, 7, -6, 29, -82, -14, 71, -127, 27, 42, -34, -37, 16, -38, 29, 62, -46, -47, 12, -99, 36, -81, 31, -65, 60, -17, -35, -2, 63, -13, 115, -33, -16, -14, 32, 65, 75, -26, 14, -37, 33, -34, 23, 70, -41, -5, -68, 127, 50, 6, -22, -28, -73, -72, -29, -19, -21, 81, 83, 86, -127, -66, -68, -14, -42, -32, 23, 0, -6, 1, -80, -44, -19, 21, -78, 58, 28, -83, 2, -6, -46, -21, 21, -17, -56, 110, 127, 21, 26, -113, -48, 18, 28, -65, -87, -9, -5, 61, 60, 3, -34, -41, -14, -37, -22, 31, 95, -54, 1, 31, -18, -1, -5, -75, 41, 5, -49, -63, 87, 66, -55, 16, 33, 25, 5, 20, 55, -32, -44, 127, -105, -29, 7, -22, -39, -41, -12, 11, 25, 8, -3, -18, 34, 26, -21, -78, 47, -34, -127, 36, -16, -80, -36, 50, -106, -32, -70, -11, -80, -27, 24, -39, 57, -47, -27, 98, 45, -56, 21, -55, 45, 15, -47, -88, 34, -10, -92, -83, 12, -127, 71, 22, 49, -55, -63, 23, -106, -28, 52, 2, -19, -67, -42, 26, -53, -52, -41, -29, -3, -58, -13, -53, 36, -58, -38, 25, 8, -10, 95, -73, 70, 46, -61, -57, -127, 1, -25, -52, -107, 14, -52, -66, -21, -11, 48, -54, 56, -19, -42, -127, 22, 62, 71, -120, -30, 63, -13, 37, 53, -68, -16, 111, 5, 25, -33, -33, 65, -53, -34, 46, 19, 3, 127, -113, 55, -5, -105, 42, 55, 0, -27, 47, -60, 23, -3, -63, -107, -73, 16, -16, -30, -24, -31, -16, 93, 127, 35, -18, 66, -15, 23, 37, 52, 40, -59, 45, 15, -5, -26, -46, 93, -1, 95, -6, 3, -91, -66, -15, -14, 1, 72, -48, 52, -36, -68, 11, -32, -16, 46, -127, 24, 2, 27, -85, 56, 15, -7, 34, -69, -47, 54, 31, 52, -33, -61, -32, -14, 9, -10, -76, -27, -10, 18, 21, -34, -61, 30, -32, 26, 25, -6, -55, 21, -28, -81, 33, -25, -77, 8, -44, 127, 66, -113, 0, 48, -34, -7, 8, -32, -23, -98, -68, 93, -28, -1, -67, 33, 35, -29, 127, -68, -67, -3, -23, -53, -19, -30, -12, 36, -27, -54, -38, -10, 2, 13, -86, -1, -64, 2, 7, -44, 58, -104, -9, -22, -114, -113, 43, 14, 120, -122, 113, 103, -101, 30, -36, -2, -127, -78, -74, -29, -4, -99, 40, -71, -88, 47, 2, -19, 40, 38, -28, 51, -13, -36, -127, 4, 6, 34, -21, 20, -79, 27, -43, -2, 15, -36, -7, -31, -4, -42, 76, -29, -13, 47, 62, 9, 127, 2, -72, 53, -11, 27, -17, -16, -17, 17, -31, -50, 10, 58, -32, -36, -20, -40, -13, -53, -2, -92, -13, 33, -20, -51, -12, 37, -40, 4, -22, -44, -29, 19, -6, 3, -98, -94, 18, -112, -73, 49, 1, -127, -95, -51, -46, -98, -85, -1, -12, -4, 59, -8, -4, 5, 0, 8, -27, -1, -31, -41, -6, -23, -5, -13, -38, 15, 27, 60, -19, 36, -12, -61, -31, -127, -2, 0, 43, -36, 2, -32, -102, -70, 82, -14, -94, 4, -35, -14, -66, -93, -58, 97, -21, -2, 5, -5, -50, -46, -5, -65, -35, -97, 63, -127, 113, 1, -9, 48, 69, -27, 4, 62, 31, 126, -51, 45, 87, 8, 7, 66, 10, 24, 68, 85, 127, 38, 79, -36, -34, -23, 61, 37, -54, 11, 33, -45, 49, 80, -46, -30, 6, 58, -127, 40, 26, 27, -15, -38, -17, 11, 13, -15, 73, -32, -11, 6, -9, -27, -24, -91, 127, -51, 34, -59, -35, -27, 107, -58, -28, 8, -81, 31, -16, -26, 29, 19, -19, -99, -39, -10, 10, -50, 28, -53, -89, -82, 67, 40, 11, 8, -69, -75, -10, -49, 10, 90, -20, -106, 127, -82, -1, -15, -38, 18, -69, -8, -14, 26, -42, -40, -12, 3, -3, 5, -5, 58, 47, -8, -52, 60, 8, -19, 85, 59, -9, 80, -68, -28, 69, -117, -122, 78, 105, 78, -42, -49, -50, 3, -34, -2, 93, -88, 127, 26, 83, -87, -61, 12, 7, -33, 127, 71, -19, -38, -16, -104, -63, -2, 3, -58, -8, -76, -7, -26, 67, 25, -88, -24, -57, -3, -66, -9, -47, -59, 20, 16, 38, 14, -42, -79, 13, -4, -109, -40, -106, -85, -127, 30, 60, 65, 26, -77, -102, -13, 25, -15, -40, -12, -31, -74, 64, 47, -33, -58, -24, -64, -111, 19, -33, -46, -105, -77, -83, -87, 22, 18, -97, -80, -30, -34, 6, 28, -52, -127, -36, 63, 86, -1, -43, 36, 96, 16, 5, -53, -80, -25, -48, 12, -35, -121, 64, 9, 70, -87, 0, 19, -127, -110, -16, -41, -5, -100, 59, -14, 50, -50, 62, -28, -6, -75, 10, 20, -7, 16, -28, 79, 3, -62, -86, -15, 8, 114, -32, -28, -10, 71, -10, -71, 42, -107, -36, -55, 127, -37, 19, 15, 3, -12, -20, 1, -26, 1, -15, 6, 10, 30, -24, 37, -71, -105, 17, -23, 11, 103, -18, -9, -87, 22, -78, -127, 46, -6, -23, -1, -9, -23, 12, 33, -32, 51, 35, 33, -35, 20, -22, 17, -19, -59, 4, 29, -79, 127, -50, -25, 33, -46, 27, 22, 16, -7, -16, 61, -20, -64, -2, -79, 8, 31, 51, -57, 37, -38, -38, -14, -37, -9, -13, 84, -16, 21, 127, -26, -18, -7, -40, -12, 0, -55, -127, -47, 12, -61, 59, 6, 51, 55, -34, 26, -2, -89, 7, 0, -9, -71, -5, -48, -80, 66, -28, -91, -52, -121, -27, -41, -2, -34, -34, -16, -55, -40, 11, 44, -44, 12, 23, -31, -25, -40, 50, 27, -33, -91, 28, 40, 6, 43, -24, -15, 6, -127, 46, -39, -16, 9, 110, -107, -71, 37, 40, 43, -47, -4, 71, 89, -5, -108, -48, -48, -101, 79, 40, 34, -2, 120, -68, -32, 18, -110, 127, -16, 46, -77, -6, 33, -26, 0, 127, -50, -27, -39, -8, 23, 121, -8, 31, -51, -54, -30, -42, 4, -30, 3, -36, -58, -6, -3, 11, -20, 6, 24, -42, -103, 20, -69, 10, -16, 3, -29, -58, 56, 58, -35, -12, 6, -127, -37, -23, 25, 96, -25, -20, -13, -13, -52, -20, 9, -28, -36, -38, 31, 114, 2, 32, 25, -37, -19, -27, -20, 4, -127, -62, 26, 28, -58, -20, -17, -74, 45, 12, 27, -57, 5, 15, -92, 1, 45, -18, 127, -11, -38, 15, -18, 14, 8, -8, 16, -53, -15, 12, 9, -19, -5, -22, -50, -4, 9, 0, -8, -10, 37, -21, -38, -78, -50, 14, 49, 69, 29, 0, -2, 27, 7, 23, -20, -65, -83, -45, -5, 43, -66, 6, 23, -36, 39, 1, 32, 23, -114, -82, -103, 127, 11, 69, -25, -68, 38, 15, -2, -20, -29, 22, -11, -58, -51, -28, 43, 127, -34, 34, -8, -68, -2, -4, -88, 96, -47, -51, -63, -9, 92, -9, 127, -64, -9, -1, -114, -41, -77, -31, -25, -18, -47, -54, -27, 10, 18, -11, 45, 24, -15, -12, -43, -29, 9, -25, -40, -29, 38, 76, -6, 2, 46, 14, 57, -56, 44, -8, 48, 14, -69, -5, -11, 9, 26, 5, -42, -76, 127, 45, -6, -75, -39, 17, -93, 71, 26, -17, -124, -37, -6, -66, -23, -41, -45, 12, -41, -19, 7, 23, 127, 54, -26, -1, 18, -67, -3, 0, 41, 5, 6, -8, -59, 3, -8, -9, 44, -81, 14, -4, -40, -14, -12, 16, 6, 46, -58, 28, -33, 20, 25, 3, 23, 4, -34, -32, 36, -127, -10, 14, 84, -19, -45, -14, -18, -115, -15, -21, 0, 101, 36, -67, 66, -5, 43, -4, 37, -89, 14, 60, 21, 19, 32, -26, -17, 9, -81, -47, -82, 22, -127, 1, 14, -17, -56, 106, -2, -24, 61, -75, 49, -93, -90, -32, 12, 64, -59, 10, 21, -78, -34, -51, 8, -81, 127, 63, -39, -22, -53, -2, -8, -2, -102, -70, -28, 33, 13, 7, -69, -74, -26, -7, -61, -5, -45, -27, -26, 2, 2, 18, -45, 8, -127, -105, -21, 51, -31, -11, -11, -18, 11, 28, 1, -14, -25, 8, -9, -3, -52, -15, -39, -26, 75, -10, 15, 127, -31, -30, -25, -22, -49, -66, -18, 13, -36, 7, 16, -39, -29, 3, 25, -54, 32, -25, 11, -11, 25, 8, 10, 31, -127, -31, 39, -40, -51, -4, -16, -40, -62, -15, 2, -21, -31, 56, -30, 44, -29, -10, 94, 13, -11, -90, 6, 23, 9, -74, -57, 26, 58, -93, 55, -31, 45, 4, 15, -15, -92, -44, -127, 104, 42, 87, -40, 127, -13, -88, 12, -14, 28, -79, 79, 38, -123, -23, 60, -22, 2, 46, -14, -58, 59, -74, -28, -11, -92, -92, -77, -56, -98, -49, -76, -127, -61, 30, 43, 6, 1, -56, 94, 36, 1, -111, -12, -69, -28, 107, -76, 121, -42, 53, 27, -86, 0, -56, 31, 70, -12, 60, -45, -53, -6, 7, -45, 39, 76, -82, 82, 57, 101, -26, 45, -69, -42, 29, 26, -11, 47, -50, -35, 11, -40, -11, -97, 65, -127, -27, -127, -85, -6, -43, -66, -98, -20, -1, 59, -18, 16, 5, 106, -12, 21, 49, 0, -80, 72, -65, -94, -54, 2, -22, -73, 45, -3, -127, 36, -41, 13, 45, -42, -68, 8, -32, -27, 2, -62, 8, 17, 71, -80, 52, -27, -5, 1, -73, -9, 23, -77, -13, 51, 7, -10, 14, 82, -47, -18, 48, 73, 19, 48, -65, 109, 74, -68, -69, 18, -7, 53, -43, 17, -28, -80, -19, -127, -83, -76, -50, 87, -98, -72, -28, 8, 20, -33, 3, 52, 11, -91, -16, -20, -14, -29, -27, 38, -15, -89, 45, 11, -127, -64, 1, -86, -81, -70, 2, -75, -83, -47, -9, -16, -23, -32, -40, -23, 24, -39, -17, -44, -7, -25, 1, 127, 2, -28, -30, -11, -45, 11, -16, 34, -33, -27, -13, -5, 4, -28, -21, -42, 10, -24, -2, -88, -34, 42, 3, 13, 30, -107, 97, -95, 48, -5, -23, -62, 83, -99, -15, 67, -22, -127, -19, 108, -27, -37, 52, -113, -12, 9, 11, -78, -6, 4, -18, 15, -43, -70, -24, -49, 26, 71, 2, -1, -12, -127, 12, -82, 11, -54, 36, 48, 78, -28, 79, -13, -32, -9, -4, -74, -40, 52, -54, 62, 32, 127, 57, -3, 12, -33, -11, 30, 31, 86, 18, -5, 71, -88, 29, 26, 60, -71, 41, -46, -43, 12, -4, 16, 127, -52, -2, -28, -1, -102, -43, 21, -26, 33, -40, 20, 17, -69, -55, -65, 42, -64, -1, -32, -105, -37, -127, -7, -20, -10, -19, 26, -105, -11, 48, -7, -56, 14, 40, 38, 58, -47, 2, 0, -80, 40, 20, -72, 17, -81, -51, -59, -20, -21, -7, 46, -93, -1, -15, 27, -62, -73, 44, -60, -15, -127, 99, -63, 43, -90, -12, 15, 62, -48, -79, -81, 1, -111, -44, 69, -26, -14, -19, 6, -32, 43, -38, -26, -9, -30, -16, -25, 16, -127, 50, 7, -23, -15, -2, 21, 7, -97, -43, -55, -77, -47, -70, -16, -43, 1, -10, 1, -36, -49, -60, -50, -97, 18, -10, -11, -79, -77, 127, 46, 102, -29, -76, 54, 87, -106, -19, -45, -42, -69, -66, 73, -8, -86, 74, -63, -10, -13, 51, 117, -18, 19, -13, -85, -72, -37, 36, 117, -127, -62, -98, 58, 4, 29, 49, -9, -67, -42, 55, 118, 9, -125, -33, 123, -104, -95, 21, 81, 95, 29, -20, 67, 92, -111, -16, 104, 57, 127, 95, 42, 74, -103, -27, -12, -75, -99, -80, -14, 63, -17, -38, 127, -37, 15, 1, 10, -37, -49, 53, -2, -1, -61, 56, -19, 18, -112, 44, 65, -77, -72, -54, 15, 2, -60, -10, 11, 1, -74, -23, -92, -21, -24, -18, -108, 63, 24, -38, 99, -18, -80, -36, -17, -44, -13, 15, -52, -49, 36, -53, 25, -8, -127, -61, 23, 5, -59, -7, -32, -17, -17, -5, -49, 1, 20, -17, 29, -7, -21, -7, -19, -13, -75, -60, -22, -32, -8, -28, 22, -26, -55, 127, 38, -18, -27, 39, -37, 34, 1, 27, -59, 81, -53, -10, 90, -45, -116, -19, 101, -88, -37, -3, 10, 52, -105, -27, -127, 3, 48, 49, 104, 45, -68, 43, -61, -43, -25, -18, -34, 101, -10, -6, 3, -40, 127, 18, 10, -76, 46, 14, -9, -85, 30, 16, -76, 24, -82, -63, -124, 77, -19, -43, -68, -84, 29, -13, -36, -45, -84, -55, 15, -69, 18, -73, 2, -66, -2, -77, -14, -127, 85, -4, -21, -47, -48, -84, -90, 10, -26, 44, 18, 127, 14, -24, -26, -2, 50, -6, 21, -14, -12, -24, -63, 27, -69, 28, 29, -61, 87, -58, -42, 10, -62, -66, -63, 82, -37, 31, 0, 58, -13, 25, -23, 5, -7, -20, 27, 13, -77, -16, 24, -26, -78, -16, 34, 11, 43, -25, 23, -16, -81, -28, -127, 86, -2, -1, 64, 25, -16, 15, -14, -42, -45, 26, 13, 24, -76, 25, 50, -8, -24, 12, 30, -21, 24, -19, -50, -97, -127, -6, 56, -7, -35, -24, -28, 26, -30, -14, -33, -41, 38, -3, 28, 13, -43, -59, -27, -17, -127, -74, -36, -37, -30, 34, 2, 11, -81, 29, 51, -66, -67, -16, -10, -8, -34, -4, 20, 53, -61, 17, -34, 25, 9, -10, 16, -3, -127, 40, -13, -34, -11, -2, -30, -35, -73, -12, -77, -56, 14, -20, 9, -31, -22, -59, -52, -3, -25, -17, -5, -14, -37, 11, -26, -62, 13, -3, 127, -24, -10, 75, -20, -22, -43, -27, -23, -9, 39, 9, -22, -127, -25, 7, 36, 54, -1, -34, 2, 29, 93, -58, -39, -64, -8, 15, 1, -122, 21, 8, -7, 10, -50, 0, -85, 17, -5, 46, -39, -5, 3, -17, -127, -63, -76, -86, -70, 7, -12, 62, -74, 121, -8, 2, -18, 22, 75, -122, 12, -12, -68, -95, -120, -61, -15, 11, 82, -54, 6, 80, 65, -73, 41, -12, 62, 2, -99, 63, 111, 55, -11, 55, 1, -127, 0, 29, 63, -36, -111, -10, -47, -10, -10, 74, -26, -48, -45, 57, -35, -45, 13, 37, 90, -41, -84, -91, 5, -59, -44, -94, 73, -92, 109, -51, 88, 31, -72, -48, -52, -11, -127, -9, 0, -29, -37, -7, -28, 38, -59, 13, -23, -48, -39, -4, 2, -23, -35, 15, 31, 72, -38, -21, -3, 7, -127, -40, 27, 44, -8, -21, -38, 3, 10, -127, 23, 4, -25, 12, 41, -1, -10, -15, 1, -14, 11, -39, 23, 19, 20, 13, -16, 15, -32, 4, -10, -3, 26, -23, -8, -30, 35, 75, 40, 29, 9, 17, 1, 88, -24, -18, 46, 20, -29, -5, 43, -86, 28, 55, 0, -57, -106, -12, -127, 24, 67, 12, -20, -30, 15, 30, -67, 12, 17, -7, 34, -127, -11, 8, 1, -27, -29, 42, 78, -1, -8, -74, -32, -70, -10, 39, -17, -54, 86, 10, 20, 127, -16, -19, -35, 16, 10, -43, -10, 7, -6, -1, -6, 2, -12, 2, -18, 12, 19, -28, -4, -3, -87, 15, -87, -12, -21, -99, 54, -97, -7, -34, -10, -90, -37, 25, 4, 9, -71, 127, -55, 38, 3, -11, 38, -28, 38, -93, -116, -80, -29, -42, 1, -14, -93, -2, -19, 33, -21, 7, -30, 14, 26, -21, -5, 22, -32, 1, 42, 0, -69, 32, 13, 47, 55, -36, 29, -41, -44, -21, -127, 14, -33, -42, 1, 12, 35, 33, -15, -66, 10, -74, -17, -44, 6, -14, 88, -69, -99, -41, -50, -127, -18, 12, 52, -100, -66, -5, -112, 8, -8, 39, -54, -122, -43, 58, 11, -81, 56, 46, 49, -106, -51, -106, 44, 40, -63, 71, 37, -10, 2, -59, -114, 127, -33, -102, -58, 19, -90, -13, -5, 74, -24, 28, 1, 29, -18, -66, -26, -31, 54, 26, 21, 80, 70, 18, -24, -7, 5, 38, 24, -115, -127, -47, -61, 8, 64, -35, 22, -29, -39, -43, -52, 20, 45, -40, 3, 46, -34, -36, 32, -6, -28, 114, -16, 119, -97, 3, -46, -64, -67, 56, 9, -127, -53, 127, -44, 21, 30, -22, 32, -39, -52, -54, -2, -9, 2, -20, -25, 44, -78, 17, -11, 16, 87, -61, -66, -44, -107, 45, -123, 19, -24, 10, 19, -3, -11, -5, 51, -50, 21, -24, 68, 15, -69, -127, -22, 25, -84, 23, 2, -78, -114, -14, -108, -62, 9, 37, 9, -56, -45, -127, -10, -90, -37, 7, 51, 16, -65, -30, -25, 34, -6, -33, 27, 31, -84, 19, -19, -39, -18, -22, 36, -54, 51, 42, -4, 44, 70, 4, -21, 23, -52, 48, 39, -74, -76, 89, 10, 98, 15, -14, -58, 22, 10, -86, -127, -26, 15, -30, 5, 83, 31, -76, -79, 34, -125, -15, -76, 22, -36, -38, -71, 127, 38, -1, 38, -50, 27, -92, -42, -7, 106, 42, -28, 4, 61, -18, 17, -47, -71, -60, 9, 36, -66, 23, -61, -35, 27, 17, 39, -3, -88, 49, 28, 37, -17, -25, 65, -19, -12, 7, 13, -1, 36, -12, -44, 35, 23, -127, -27, 2, 12, -8, -88, 31, -8, -34, -109, 3, 13, -25, 0, 30, -14, 11, 9, 24, 32, 79, -35, 5, 127, -38, -29, 14, -76, -110, 68, 40, -62, -109, 0, -127, 22, 93, 1, 77, 48, -4, 60, -30, 43, 10, 26, -24, -57, 78, 18, 23, 67, 11, -34, -49, -36, -3, -69, 36, -84, -127, 91, 16, -4, -50, 5, -72, 43, -31, -14, 27, -70, 1, -53, 110, -72, -11, -2, 59, -124, -10, -70, -77, -39, 4, 78, -62, -21, 10, 4, 20, -18, 14, 19, -8, -22, -36, 10, -26, -12, -7, -17, -68, 23, -31, -3, 12, 84, 4, -62, -7, -127, 33, -65, 42, 15, 34, 4, -79, 22, -36, -23, 51, 127, -6, -95, 77, -48, 26, -18, 6, -18, 62, -34, 0, -17, -6, 8, 13, 31, 45, -48, 25, 69, 5, 74, 127, 50, 26, 60, 51, 16, 31, 33, 4, -38, -1, 28, -6, -45, 55, 43, 102, 40, 20, -26, -64, -39, 25, -33, 29, 38, 79, 106, -55, 0, -90, -66, 35, -4, 71, 55, -52, -91, 24, 53, -22, 79, -42, -127, 18, 10, 78, -8, 48, 8, -21, 11, -66, -99, 79, -67, -6, -96, 13, -50, 111, -1, -18, 71, 16, -35, -47, 33, -66, 48, -45, -45, -127, 7, -4, 85, 2, -40, -49, -52, 114, -11, 32, -44, 19, -39, 14, -75, -40, 32, 81, 63, -57, 1, -74, 95, -66, 39, 44, 0, -19, 1, 32, 50, 94, -100, -64, -127, 72, -24, 98, -70, -26, 44, -4, -9, 15, 30, 12, -2, 6, -127, -27, -25, 12, 15, 27, 19, 2, -8, 1, -65, -30, -32, -18, -42, 16, -81, 52, -58, -61, -37, 27, -1, -19, 24, -26, 57, 4, -46, -30, 6, -23, 89, -24, 8, -127, 51, -60, 1, 24, -27, -37, 64, 14, -12, 5, 41, -50, -54, 26, -81, -6, -79, -18, -65, -68, -49, -108, -127, 1, -77, -30, -22, 45, -60, -14, -14, -7, -20, -59, 13, -19, -42, -50, 20, 14, -31, 24, 45, -13, -12, -43, 28, 22, -77, -43, 33, -7, -68, 24, -12, -95, 24, -6, -7, -54, -127, -52, -33, -48, -14, -3, 9, -54, -81, 2, -44, -15, -127, 58, 24, -20, -72, -89, -66, 57, -36, -9, -29, -7, -23, -20, -19, 26, -60, -39, -4, -15, 13, -12, 37, 20, -31, 4, 33, 44, 9, -10, 54, 20, 17, -17, -17, -14, -78, -38, -1, -127, -3, 0, -48, -51, -115, 20, -27, 11, 13, 87, -39, -65, 101, 23, -33, -41, 43, -7, 34, -38, -125, -75, 42, -53, -47, -59, 68, 12, 27, -24, 82, 63, -109, -86, 44, 127, 17, 6, 6, 107, 6, 79, -5, -25, -20, -7, -40, 6, -127, 0, 42, 49, 3, -56, 60, 54, 28, -30, 71, -6, -18, 80, -108, 41, -11, -59, 68, -26, -66, -11, 53, 5, 47, 40, 19, -53, -81, -11, 33, 16, -113, 9, 16, -127, -60, 3, -5, 16, -39, 31, -43, 25, -50, 10, 58, -39, -29, -127, 10, -15, 97, 3, -37, 37, -64, 27, -32, -37, -44, -6, -1, 117, -65, -77, 24, -51, -71, -5, -3, -122, -77, -127, -26, 8, 45, 30, -50, 125, -123, 96, -69, -61, -122, 57, 97, -65, -35, 22, -36, -8, -31, 11, -54, 2, -43, -83, -20, -18, 52, -57, -75, 12, -4, 52, 5, 78, -17, -8, 55, 1, -65, -63, 52, -45, -118, 30, -30, -55, 85, 5, -54, 28, -127, -57, -59, 7, -34, -21, -122, -79, 24, 74, 46, -83, 15, -14, 28, 31, -92, 29, 61, -21, 98, 77, 26, 83, -127, 1, 30, -59, -116, -45, -39, 123, -107, 109, -34, -1, -21, 34, -36, -1, 47, 31, -14, -39, -32, 27, 9, 1, -49, -55, 94, 14, 29, -92, 40, 21, 118, -60, 65, 49, -127, 47, 55, -98, -41, 15, 35, -39, 5, -95, -4, 86, 34, -2, -4, -19, 35, -34, 0, -12, -50, 16, -56, -74, -127, -45, -54, 35, 25, -127, 67, -9, -16, -17, -34, -54, 65, -65, -14, 42, 14, 77, -56, 14, -66, 6, 33, 15, -92, -97, 34, -92, -85, -26, 23, -81, -78, -39, 3, -31, -13, -65, -47, 29, 45, -26, -58, 127, -65, 7, -5, -3, 16, -37, -60, 11, -2, -17, -45, -25, -16, -15, -11, 24, 53, -24, 45, 11, -61, 4, -39, -41, 67, -28, 81, -23, 8, -44, 5, -21, -29, -36, 1, 127, -16, -26, 2, -14, -82, -23, 21, -10, -20, -78, 73, 22, -23, -30, -22, -102, 45, 4, 37, -41, -81, 86, 111, 127, -70, 26, 12, -48, -101, -37, 39, -10, -108, -77, 84, -17, -61, 20, 12, 62, -22, 0, -1, 24, 1, -22, 58, -8, 17, -2, -14, 50, 2, 39, 10, 54, 16, -32, -70, 36, -127, -60, -32, 6, -36, -24, 30, -83, 0, 3, -33, -25, 18, -14, -5, 27, -23, 78, 26, -54, 4, -24, -26, 127, -39, -27, -58, 17, 15, -43, 39, 21, -37, 71, -70, -32, -21, -26, -75, 8, 28, -46, 59, 3, -107, -43, -9, -94, -104, 73, -19, 23, 2, -53, 15, -6, -127, -54, -56, 55, -80, 52, -30, 39, -29, -39, 55, 16, 5, -12, 35, -39, -42, 4, 9, 26, 30, -33, -5, -39, 28, -15, -50, -8, -30, 51, -127, 4, 12, 60, -39, 25, 40, 45, -70, -71, 4, 7, 36, -47, -77, -13, 9, -53, -28, 46, 8, 70, 68, 0, 20, 7, -127, -90, -35, 97, 7, -62, -41, -99, 29, 1, -57, -13, -29, 58, 6, -53, -102, 34, -40, -27, -25, 3, -6, -36, -89, -23, -57, -127, -41, 61, -39, 1, -6, 7, -12, -3, -32, -127, 19, -13, 58, -59, -30, -31, -37, 86, 71, -78, -6, 31, -68, 17, -35, -2, -40, -16, -3, -22, -45, -92, -58, -98, 44, -1, 23, -12, -15, -27, -66, 127, 123, -113, -18, -117, -95, 22, -23, 14, 3, -21, -5, 92, -11, -9, -18, -46, 10, -12, -63, 19, -95, -38, -11, 49, 77, -10, 31, 9, 14, -38, 36, -57, -39, 8, -33, 40, 91, -53, -71, -109, 53, -7, -38, -127, 35, -104, -105, 62, -43, 127, 2, 13, -28, 13, -25, -2, 15, 43, -16, -31, 36, -25, 32, 6, 18, -16, -15, 8, 14, -15, -127, 7, -118, 15, 3, -56, -127, -13, 16, -110, -26, -9, -69, -4, -65, -30, -20, -1, 29, -7, -7, -34, -42, -37, 118, -25, -73, -27, -68, -118, -85, 81, -7, 32, -53, 49, 10, 35, 1, 5, -23, 23, 9, -12, -44, -4, 4, -34, 14, -16, 3, -41, 89, 0, -16, -39, -12, 5, -127, 4, -12, 74, -127, -25, 8, -70, -31, -32, 10, -48, 22, -44, -19, -12, -20, 50, 23, 37, 0, -21, 50, -29, 15, 3, -124, -51, -93, 96, 16, -27, 27, 80, -37, 29, -13, -43, -14, -5, 28, -32, -127, -61, 57, -54, -40, 44, -8, -30, 0, 4, 12, 37, -50, 23, -37, 63, 14, -66, 13, -53, 20, -49, -37, -12, -15, 24, -8, 16, -77, 97, 48, 62, -90, -24, -22, 127, -107, -29, -99, 45, -80, -25, 19, -62, -20, 88, 31, 127, 3, 49, 14, -35, 54, -50, 30, -26, -43, -61, 70, -15, 7, -35, 4, -26, 106, 31, 55, 16, -95, 1, -111, 39, -21, -64, 106, -64, 96, 94, -127, -22, 9, -80, -60, -14, -26, -19, -42, -22, -81, 17, 51, -28, 11, -89, -78, -1, -2, 42, -80, -85, 25, -51, -70, -21, -61, -40, 15, -109, -33, 6, 46, -66, -45, 4, -34, -3, -88, -108, 65, -16, -55, -60, -38, -56, -127, -77, 11, 52, -119, -19, 44, -63, 14, -35, -68, -120, -1, 30, -3, 9, -33, 93, -33, 119, -37, -40, 25, 107, -127, -94, -79, -58, -31, -18, 115, -28, -9, -47, 105, -61, -6, 1, -12, -127, 35, 18, 31, -53, -49, 18, -66, 117, -67, 17, 22, 84, -41, -10, -3, -39, -54, -37, 112, -17, -60, 64, 18, 89, -26, 12, 0, 45, 15, 6, 53, 19, 75, 8, 67, 18, 46, 30, -35, 88, 45, -26, -11, -74, -127, 21, -23, 39, 45, -68, 32, -11, -40, -49, -18, -57, 20, 7, -21, -58, -10, 16, -45, -4, 6, 7, -62, 67, -80, -10, -43, -8, -127, -40, -25, -116, 22, -62, -46, 27, -32, -50, -35, 65, -12, -42, 34, -49, -11, -60, 11, -15, -127, 10, -33, -46, 12, -23, -63, -29, -9, 68, 26, -19, -29, -77, 8, -31, 2, -25, 21, -127, 22, -27, 43, -38, 23, 1, 12, 6, -51, -94, 30, 24, -62, 29, -37, -55, 14, 19, 62, -13, -9, -30, -16, -55, 33, -55, 37, 44, -54, -79, -60, 1, -11, -19, 54, -96, -79, 12, 8, -42, 91, -12, -23, -127, -44, -21, -12, 52, -14, 32, 96, -83, 17, 67, 62, -82, 74, -13, 127, -10, -110, -35, 45, 17, -53, 19, 33, -112, -90, 45, 54, -29, -122, -28, 47, -10, 55, -80, 28, 41, -73, 60, 31, -70, 72, -55, 55, -53, -8, -98, 101, 20, 2, 48, 4, -83, 122, -17, 75, 60, -85, 85, -127, 35, 3, 7, -127, -90, 61, -26, -38, -125, -10, 59, 77, 102, 1, -42, 14, 10, 38, 100, -10, 17, 14, 40, -57, -3, -84, -25, -2, 80, 11, 59, 16, -20, 1, -10, 78, 39, -43, -17, 28, 33, -20, -100, 21, 56, -18, -110, 21, -74, 82, 8, -55, -40, -127, 52, -56, 38, 111, -65, -31, -13, 37, -22, -6, -31, 9, 23, -10, -16, -7, -13, -3, -47, 42, 102, -17, -72, 5, 12, -36, -127, -18, -33, -37, 75, -14, -13, -76, 10, -53, 13, -38, -55, 2, -41, -21, 0, 1, -77, -53, -33, 77, -12, -72, 19, -83, 0, -46, -47, -127, -55, -25, -14, 23, -10, -72, -51, -26, 22, 22, 104, 26, 3, 73, 19, -104, -112, -34, 8, -107, 1, 1, -127, -8, 33, -98, 27, -57, -65, 1, -95, 46, -8, 15, -17, -52, 33, 28, 76, -27, -7, -20, 34, -34, -11, -42, 85, 32, -39, 15, 0, 62, -2, -43, -4, -71, 42, -127, 57, 24, -55, 50, -102, -20, -11, -21, -32, -71, 7, -61, -109, -127, 13, -7, -12, -42, -9, 9, 35, -87, -69, -32, -6, -67, -39, -12, -36, -93, -25, 121, -33, -20, 25, 19, 80, -53, -62, 49, -18, -101, -4, -43, 58, -57, -99, -35, 67, -79, -19, -32, -117, -14, -127, 98, 89, 66, 14, 4, 3, 18, -51, -9, 33, 45, 1, 47, -16, 22, -26, 37, -43, -34, -8, -6, -20, 127, -1, -33, -18, -4, -58, -98, 17, -16, -82, 64, -7, -7, 20, -13, -26, -27, -127, 89, 15, -62, -13, -2, 101, 13, 67, 2, 14, -19, -55, -62, -48, -3, -84, -21, -8, 27, -26, -30, -6, 44, -27, 35, -24, 2, -45, -39, 127, -41, 20, -26, -28, -16, 5, -54, -8, -2, 8, -24, 35, -26, -16, -14, -4, -57, -40, 45, -127, 31, -51, 19, -53, -28, -43, 21, -41, -74, -113, 28, -24, -45, 45, -9, -71, -16, 6, -72, -9, 4, -31, -119, 44, 44};

float bias_raw[672]={-0.0575769878923893, -0.023882215842604637, -0.006702350452542305, -0.0014440538361668587, -0.056920070201158524, -0.009184976108372211, -0.014215976931154728, -0.02904774248600006, -0.050788674503564835, -0.07542882859706879, -0.005047450307756662, -0.017250655218958855, -0.044455718249082565, 0.0037060545291751623, 0.018941130489110947, -0.049145136028528214, -0.057143960148096085, -0.015341333113610744, -0.052174635231494904, -0.011770899407565594, -0.010310126468539238, -0.005419321358203888, 0.0036730461288243532, -0.024131974205374718, -0.047878410667181015, -0.09719218313694, -0.05548178032040596, -0.03177160024642944, -0.07224076241254807, -0.036373499780893326, 0.015957089141011238, -0.013395513407886028, -0.03194938600063324, 0.02546120434999466, -0.06073417514562607, -0.0030444185249507427, -0.05295879766345024, -0.0002446030266582966, -0.05011613294482231, -0.02821594662964344, -0.036400940269231796, 0.032311830669641495, -0.029625557363033295, -0.10596031695604324, -0.030667411163449287, -0.03763754293322563, 0.007827340625226498, -0.02416703850030899, -0.014964302070438862, -0.013465838506817818, -0.016521921381354332, 0.008197811432182789, 0.008306054398417473, -0.04932438209652901, -0.0082427728921175, 0.004084151703864336, 0.006554740481078625, -0.04967818409204483, -0.03985840082168579, 0.00013064959784969687, -0.019949914887547493, 0.006922855973243713, -0.00499551510438323, -0.07977365702390671, -0.04545355588197708, -0.011603934690356255, 0.010614081285893917, -0.10128593444824219, -0.031850118190050125, 0.009455125778913498, -0.012531493790447712, -0.05953000485897064, -0.030428607016801834, -0.035082656890153885, -0.02530563622713089, -0.013741260394454002, -0.03119434416294098, 0.00413631834089756, -0.024144688621163368, -0.0411696657538414, -0.02174113690853119, -0.052237145602703094, 0.012411723844707012, -0.047198306769132614, -0.034126460552215576, -0.009519945830106735, 0.0024242333602160215, 0.01087039802223444, -0.00908786803483963, 0.0040602656081318855, -0.029961420223116875, -0.03040136583149433, -0.002790887840092182, -0.07314089685678482, -0.026376591995358467, 0.016552262008190155, -0.010787763632833958, -0.025189876556396484, 0.05265849456191063, -0.12512052059173584, 0.00692575192078948, 0.04587026685476303, -0.04746116325259209, -0.0059315781109035015, 0.08196098357439041, -0.08411063253879547, 0.03833016753196716, -0.05011469125747681, -0.006706341169774532, -0.08174993097782135, -0.0006384564912877977, -0.03936856985092163, -0.04150548577308655, -0.0063768113031983376, 0.005013789515942335, -0.001391429454088211, -0.07842414826154709, -0.06588341295719147, -0.04380979761481285, -0.034722644835710526, 0.01153701264411211, -0.018506821244955063, -0.03245756775140762, -0.019290020689368248, -0.0417378693819046, -0.0735531896352768, -0.004462143871933222, -0.016622409224510193, 0.0178134273737669, 0.021995527669787407, -0.0055509707890450954, -0.04747586324810982, -0.020369958132505417, 0.005037993658334017, 0.00811130739748478, -0.015885531902313232, -0.007795401848852634, -0.019754860550165176, -0.018895940855145454, -0.0875597596168518, 0.034053850919008255, -0.03859151154756546, -0.049203407019376755, -0.023312311619520187, -0.01493900828063488, -0.007062437478452921, -0.043220363557338715, -0.015021714381873608, -0.035584863275289536, 0.009639924392104149, 0.0029136117082089186, -0.014376160688698292, 0.022974668070673943, -0.01923549361526966, 0.005272826179862022, -0.0040378146804869175, -0.0030247080139815807, -0.01400368008762598, -0.04596228897571564, -0.07783914357423782, -0.06223434954881668, -0.04911857843399048, -0.012921938672661781, 0.00568159157410264, -0.03367985785007477, 0.01041924487799406, -0.008223269134759903, -0.003967224154621363, -0.03998871147632599, -0.007410761900246143, -0.02124057523906231, -0.03267908841371536, -0.033372607082128525, -0.014697058126330376, -0.03132867068052292, -0.025540988892316818, -0.02977238968014717, -0.0361064076423645, -0.016569850966334343, -0.03414296358823776, -0.06334389746189117, 0.006929063703864813, -0.053503140807151794, 0.009569470770657063, -0.05365738645195961, -0.0924394354224205, 0.004087178967893124, 0.016133027151226997, -0.08916078507900238, -0.0022788431961089373, -0.055234938859939575, -0.03777872771024704, 0.010535232722759247, -0.014014323242008686, 0.012542021460831165, -0.0575634203851223, -0.016176553443074226, 0.029180515557527542, -0.028490956872701645, -0.024934617802500725, 0.004569237120449543, -0.06280576437711716, 0.01942623406648636, 0.02985641360282898, 0.007490009069442749, -0.0034069318789988756, -0.014272548258304596, 0.03431939706206322, -0.021141910925507545, -0.09146259725093842, 0.08241251111030579, -0.008882234804332256, -0.009623144753277302, -0.014510962180793285, -0.04796253517270088, -0.008675704710185528, -0.049812790006399155, -0.015006373636424541, -0.018693912774324417, -0.00486195832490921, -0.033123016357421875, 0.03203820437192917, -0.009661010466516018, -0.039976563304662704, -0.03553956747055054, -0.04285309463739395, 0.027498718351125717, -0.05721638724207878, -0.03709016367793083, -0.05684864521026611, -0.0009375027730129659, -0.042745064944028854, -0.011602193117141724, -0.04704723879694939, 0.024734238162636757, -0.031397465616464615, -0.03073589690029621, -0.006419615354388952, -0.008283226750791073, -0.057208459824323654, 0.017818721011281013, -0.09333233535289764, 0.004133819602429867, -0.051917172968387604, -0.026519877836108208, 0.020084120333194733, 0.058287281543016434, -0.040315233170986176, 0.006488800048828125, -0.05002119019627571, -0.10131533443927765, -0.01745855249464512, -0.010359995998442173, 0.041727010160684586, -0.024717185646295547, -0.012586668133735657, -0.06262893974781036, 0.011924472637474537, -0.07744067162275314, -0.025137068703770638, -0.01879517361521721, -0.006746734958142042, 0.02944033220410347, 0.002349141053855419, -0.04273124039173126, -0.018893040716648102, -0.006912847515195608, -0.01575400121510029, 0.052824556827545166, 0.0012418226106092334, 0.010628015734255314, 0.004437699913978577, -0.05948009714484215, 0.0007115200278349221, -0.01020783931016922, -0.03185554966330528, -0.011127343401312828, -0.032646890729665756, -0.004933584947139025, 0.021555539220571518, -0.026975885033607483, -0.027270762249827385, -0.027215972542762756, -0.05003207549452782, -0.06641758978366852, -0.03697485104203224, -0.018254458904266357, -0.02940797619521618, -0.011130152270197868, -0.02586532197892666, 0.012990967370569706, -0.020295163616538048, -0.0054384926334023476, -0.049539294093847275, 0.004757991526275873, -0.028128601610660553, -0.04140704125165939, -0.039405494928359985, -0.00061232183361426, -0.040808096528053284, 0.016935354098677635, -0.026069244369864464, -0.008648947812616825, -0.06519000232219696, 0.0009248580900020897, -0.023256942629814148, -0.0069180820137262344, -0.019711202010512352, 0.018438832834362984, -0.0027337372303009033, -0.005750362295657396, -0.007295310031622648, -0.07186903804540634, -0.05542408674955368, 0.004951815120875835, -0.04061169549822807, -0.010923676192760468, -0.08653119206428528, -0.00694065447896719, -0.04062889143824577, -0.01749682053923607, -0.005386196076869965, -0.0214032344520092, -0.02431127056479454, 0.017992960289120674, -0.013608148321509361, -0.017077788710594177, 0.0032688933424651623, 0.010179386474192142, -0.02836904488503933, -0.009398621506989002, -0.014023599214851856, -0.011792064644396305, -0.0305023193359375, -0.002389648463577032, -0.0018847471801564097, -0.05002898350358009, -0.036125026643276215, -0.041777100414037704, -0.036863554269075394, -0.02409210242331028, -0.01726600155234337, -0.014835759997367859, -0.028228821232914925, -0.008139271289110184, -0.026015618816018105, -0.030000219121575356, -0.0390346422791481, -0.032965824007987976, -0.004495372995734215, -0.0003520880127325654, -0.019694164395332336, -0.03527948632836342, -0.0295089203864336, -0.02516193874180317, -0.028634998947381973, -0.015480468980967999, -0.012963182292878628, -0.03444910794496536, -0.02058929204940796, -0.013809971511363983, -0.06025182083249092, -0.024587854743003845, -0.020616643130779266, 0.0007041112403385341, -0.020866625010967255, 0.01897350698709488, 0.0028321570716798306, -0.04650264233350754, 0.006839561276137829, -0.03477964922785759, -0.016741205006837845, -0.057346753776073456, -0.03536999598145485, 0.03606198728084564, -0.02106531709432602, -0.0032243654131889343, -0.008170842193067074, -0.02515273727476597, 0.011211869306862354, 0.011702906340360641, -0.0036962179001420736, 0.00415536854416132, -0.020180100575089455, -0.02041291445493698, 0.0002878200029954314, 0.016149014234542847, -0.017193131148815155, 0.006775646936148405, 0.0005600338918156922, -0.0060771312564611435, -0.08069392293691635, -0.012062184512615204, 0.021321700885891914, -0.022925911471247673, -0.0624922439455986, -0.004307812545448542, 0.0002319140185136348, -0.025634445250034332, 0.012670919299125671, -0.05362248793244362, -0.041929200291633606, -0.030802007764577866, 0.0011365992249920964, -0.020552322268486023, -0.028493573889136314, 0.011041916906833649, 0.020704282447695732, 0.00838492438197136, -0.0025330078788101673, -0.013742344453930855, -0.06996017694473267, -0.09183303266763687, -0.07635585963726044, -0.008446999825537205, 0.03525463864207268, -0.01114001777023077, -0.009144414216279984, -0.002742031356319785, -0.007437342312186956, -0.03136756643652916, -0.013106712140142918, -0.026809362694621086, -0.027228154242038727, -0.012637465260922909, -0.06972784548997879, -0.052542198449373245, 0.01206936128437519, -0.02104176953434944, 0.01550974603742361, -0.03387752175331116, -0.017666472122073174, -0.009003994055092335, -0.07964217662811279, -0.024439306929707527, -0.027219019830226898, 0.003984188195317984, -0.04574957489967346, -0.009640746749937534, -0.008930878713726997, -0.03714695945382118, -0.018471116200089455, -0.026565495878458023, -0.04402216151356697, -0.004286784678697586, 0.015755387023091316, -0.0691147893667221, -0.01838967390358448, 0.031376082450151443, -0.008860492147505283, -0.014381757006049156, 0.018708791583776474, -0.03921036794781685, 0.016402652487158775, -0.029585687443614006, -0.008022126741707325, -0.06761771440505981, -0.013349201530218124, -0.08052141964435577, -0.032330989837646484, -0.003940614406019449, -0.02872220054268837, 0.0017712676199153066, -0.06548871099948883, -0.024338815361261368, -0.0032592127099633217, 0.00796786043792963, 0.022137146443128586, -0.024912144988775253, 0.010826903395354748, -0.04575701802968979, 0.05022058263421059, -0.0002238285233033821, 0.0013491364661604166, -0.019068950787186623, -0.03174871951341629, -0.026092909276485443, -0.010767907835543156, -0.07886040955781937, -0.003766490612179041, -0.016113733872771263, -0.017884638160467148, -0.006137767340987921, -0.012694970704615116, -0.026780089363455772, -0.039430517703294754, 0.022504175081849098, -0.038881558924913406, 0.013593577779829502, -0.0025457406882196665, -0.048990391194820404, -0.03566855937242508, -0.05700446665287018, -0.015259450301527977, -0.002970493398606777, -0.04448046535253525, 0.031276192516088486, 0.006487553473562002, -0.0004412354319356382, 0.022860530763864517, -0.03866728022694588, -0.05732636898756027, -0.014771332032978535, -0.016325633972883224, -0.014664852991700172, 0.007368113379925489, -0.0049507818184792995, -0.007816202938556671, 0.010042647831141949, -0.059437669813632965, -0.008427961729466915, -0.010567249730229378, 0.038249582052230835, -0.04443858191370964, 0.006224759854376316, -0.048267025500535965, -0.04526996240019798, -0.01509437058120966, 0.002546705538406968, -0.06546642631292343, -0.015819795429706573, -0.0386415533721447, 0.04834895581007004, -0.05918346345424652, -0.025785310193896294, -0.02311527542769909, -0.0023121305275708437, 0.0019712462089955807, -0.0046446011401712894, -0.049400947988033295, -0.04688359051942825, -0.008741039782762527, -0.006539184600114822, 0.009135188534855843, -0.06894652545452118, 0.0023442283272743225, -0.015412873588502407, -0.01515310537070036, 0.008979284204542637, -0.002938903169706464, 0.03517225757241249, -0.07950013130903244, -0.02464483119547367, 0.01196277141571045, -0.04441819339990616, -0.002084756502881646, -0.04778909310698509, -0.011811584234237671, -0.03975452855229378, 0.03734949976205826, -0.04960794746875763, -0.08811497688293457, -0.0011286286171525717, -0.0056248614564538, -0.019465623423457146, -0.01373414508998394, 0.036339521408081055, -0.03282973915338516, -0.04397565871477127, -0.012463333085179329, -0.06273745745420456, -0.04879152029752731, -0.028653521090745926, -0.04198295995593071, 0.021929286420345306, -0.06945555657148361, -0.03242538124322891, -0.06422679126262665, -0.05623858794569969, -0.06135407090187073, 0.016875343397259712, -0.008438985794782639, -0.01150122657418251, -0.004728332161903381, -0.035888008773326874, 0.0017247766954824328, -0.011590230278670788, -0.0405355729162693, -0.010688706301152706, -0.0563381090760231, -0.01885444112122059, -0.003771913005039096, -0.020917575806379318, -0.09855970740318298, -0.01654866337776184, -0.024669397622346878, 0.027922801673412323, 0.008351881988346577, -0.03108958527445793, -0.016131464391946793, 0.003407187294214964, -0.05785279721021652, 0.014592710882425308, -0.03891194611787796, 0.002766115590929985, -0.007720897905528545, 0.010787571780383587, 0.012509546242654324, -0.006514749955385923, -0.020723342895507812, -0.007408812176436186, -0.04742345213890076, -0.1039932370185852, -0.013317372649908066, -0.0188633743673563, -0.008379154838621616, 0.011240910738706589, -0.028049038723111153, 0.023129668086767197, 0.05873963236808777, 0.016946857795119286, -0.02648754045367241, 0.006670111790299416, -0.013223955407738686, -0.025715135037899017, -0.000708971347194165, -0.06560390442609787, -0.005411495920270681, -0.07422181963920593, 0.009726414456963539, 0.006730358116328716, 0.017283979803323746, -0.020219389349222183, -0.03777490556240082, -0.029301663860678673, -0.025162285193800926, -0.017091242596507072, -0.035080716013908386, 0.0043936800211668015, -0.014620020054280758, -0.05573542043566704, 0.010756820440292358, -0.019612004980444908, -0.005508172791451216, -0.029439104720950127, -0.05459693819284439, -0.010281584225594997, -0.005067101679742336, -0.06585898250341415, -0.024765074253082275, -0.05199384316802025, -0.07422119379043579, 0.012807448394596577, -0.06898417323827744, -0.0005411999300122261, -0.022303428500890732, -0.022936806082725525, -0.05576615035533905, 0.018335094675421715, -0.06572069227695465, -0.05055426061153412, -0.011263225227594376, -0.0069772955030202866, 0.018251225352287292, -0.019756438210606575, -0.05574826896190643, -0.038262005895376205, -0.04387930408120155, 0.027182115241885185, -0.002461907686665654, 0.006037806160748005, 0.0009220525389537215, -0.030302440747618675, -0.03654896840453148, -0.018943628296256065, 0.020156819373369217, -0.0892697274684906, -0.018328247591853142, -0.021579675376415253, -0.04031170904636383, -0.03718027472496033, -0.03930235281586647};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActNone;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={672,1,1,28};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={672};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteInt8;
const TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;
// const float scales_filter=;
// const int32_t zero_points_filter=;
// const float scales_bias=;
// const int32_t zero_points_bias=;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void ExtractConvParams(TfLitePadding padding, int stride_width, int stride_height, 
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteConvParams* data_params) {
  // TfLiteConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params,
                       char* tensor_data, TfLiteAffineQuantization* quant_struct,
                       size_t bytes_size, TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  const float* input_data = GetTensorData<float>(input);
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  const bool need_non_dilated_im2col =
      params->stride_width != 1 || params->stride_height != 1 ||
      filter->dims->data[2] != 1 || filter->dims->data[1] != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // std::cout << "codes runs here #-1" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #-2" << std::endl;
  bool has_bias = false;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));
  // TfLiteTensor* filter;
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  auto filter_input_channel = filter->dims->data[3];
  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;
  // std::cout << "codes runs here #-3" << std::endl;
  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }
  // std::cout << "codes runs here #-4" << std::endl;
  const TfLiteTensor* bias = nullptr;

  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.
  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }
  // std::cout << "codes runs here #-5" << std::endl;
  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8));

  if (is_hybrid && filter->type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  // std::cout << "codes runs here #-6" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  data->supports_multithreaded_kernel =
      (kernel_type == kMultithreadOptimized) &&
      (context->recommended_num_threads != 1) && !is_hybrid &&
      (params->dilation_width_factor == 1) &&
      (params->dilation_height_factor == 1) &&
      (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);

  int channels_in = filter->dims->data[3];
  int channels_out = filter->dims->data[0];
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  // std::cout << "codes runs here #-7" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #-8" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    // std::cout << "affine_quantization->scale->size: " << affine_quantization->scale->size << std::endl;
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #-9" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter->type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }

  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }

  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #-10" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }

    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }

    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #-11" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  // std::cout << "codes runs here #4" << std::endl;
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }
  // std::cout << "codes runs here #5" << std::endl;
  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }
  // std::cout << "codes runs here #6" << std::endl;
  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // std::cout << "codes runs here #7" << std::endl;
      const float* filter_data;
      if (data->need_hwcn_weights) {
        filter_data = GetTensorData<float>(hwcn_weights);
      } else {
        filter_data = GetTensorData<float>(filter);
      }
      // int index;
      // for (index = 0; index < 432; index++){
      //   // std::cout << "filter_data[" << index << "] = " << filter_data[index] << std::endl;
      //   std::cout << filter_data[index] << ", ";
      // }
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(filter), filter_data, GetTensorShape(bias),
          GetTensorData<float>(bias), GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = params->stride_width;
      op_params.stride_height = params->stride_height;
      op_params.dilation_width_factor = params->dilation_width_factor;
      op_params.dilation_height_factor = params->dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #0" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #1" << std::endl;
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;

  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data), 
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;
  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  // std::cout << "codes runs here #10" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_ollchs_REF() {
  static TfLiteRegistration r = {ollchs::Init, ollchs::Free,
                                 ollchs::Prepare<ollchs::kReference>,
                                 ollchs::Eval<ollchs::kReference>};
  return &r;
}

TfLiteRegistration* Register_ollchs_GENERIC_OPT() {
  static TfLiteRegistration r = {ollchs::Init, ollchs::Free,
                                 ollchs::Prepare<ollchs::kGenericOptimized>,
                                 ollchs::Eval<ollchs::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_ollchs_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {ollchs::Init, ollchs::Free,
                                 ollchs::Prepare<ollchs::kMultithreadOptimized>,
                                 ollchs::Eval<ollchs::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_ollchs_CBLAS_OPT() {
//   static TfLiteRegistration r = {ollchs::Init, ollchs::Free,
//                                  ollchs::Prepare<ollchs::kCblasOptimized>,
//                                  ollchs::Eval<ollchs::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_ollchs() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_ollchs_MULTITHREADED_OPT();
#else
  return Register_ollchs_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
