/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace gbdckb {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

int8_t filter_r   aw[18816]={-61, -8, 44, -74, 99, 76, -76, -8, 12, 3, 111, -23, -80, 61, 14, 47, -80, 44, -33, 59, -36, -127, -4, -97, 2, 45, 5, 23, -13, -86, -2, 31, -118, 127, -38, 50, 30, 82, 67, -26, -126, -13, -17, 39, -31, -19, 32, 25, 7, -27, 13, -14, -19, -39, 78, -23, -5, -4, 42, -1, -44, -15, -24, 11, 127, 28, 70, -20, -17, 17, -3, 34, 18, -78, -29, -67, -4, -44, 51, -13, -26, 25, 32, -63, -50, -63, -89, 51, -59, 1, 78, -8, -25, -15, -2, 37, 8, -56, -48, -108, -72, -26, -94, -80, -113, -15, -98, 71, 127, -1, -10, -70, 18, -69, -42, -40, 37, -23, -73, -89, -60, -42, 24, -102, -27, 15, 1, -76, -43, 74, 4, -40, -28, 4, -127, -5, -9, -3, -40, 63, -42, -43, 85, -38, 7, -11, -25, 21, 39, 23, 3, -41, -24, -40, 27, 9, -7, -127, -51, -52, 10, 6, -6, 53, -43, -10, 53, -50, -6, -87, 71, -69, -110, 45, 2, 35, 27, -75, 60, 12, -127, -54, -101, 118, -24, -59, -62, 48, -1, 10, -71, -21, -46, -47, 25, -34, 53, -8, 17, -61, 124, -126, -47, 121, 46, 122, 39, -74, -74, 71, -6, 5, -59, -117, 127, -95, -53, -3, 26, 8, 78, 12, 1, 7, 66, 1, -48, -127, -35, -21, -71, 18, -33, 103, -32, -34, -33, 75, 9, 100, 15, -3, 4, -36, 10, 8, 13, -114, 22, -17, -62, 18, -6, -80, -91, 21, 13, -33, 14, 83, 39, -32, 26, 52, -77, 43, 8, 39, -5, -13, 122, -58, -58, 3, 23, -18, 39, -127, 33, -63, -50, -57, 70, -76, 0, 7, -57, 52, 59, 6, 24, -2, -9, 40, 0, 42, -25, -127, -19, 3, 60, -26, -6, 67, -44, 35, 70, -95, 0, -22, -8, -19, -25, -37, 19, -30, 20, 20, 19, -40, -43, -22, 52, 63, -127, 25, 78, 68, 49, -45, 42, -30, 12, 18, 31, -30, -5, -28, 12, -7, 10, -49, -55, -81, -39, -56, -65, -34, -39, -3, 0, 1, -5, -77, 0, -6, -40, 127, 3, -33, -15, 36, -27, -10, -29, -55, -71, -41, 31, 19, -20, 29, -75, -1, 7, 16, -127, -66, -95, 20, -24, 22, 18, 51, 14, -19, 40, -65, -34, -70, 52, -28, -127, -6, -107, 24, -104, -108, 37, -52, 16, -93, -82, -73, -11, -120, 19, 39, 33, -20, 114, -10, 9, -62, -25, -127, -79, -2, 1, 17, -72, -90, -5, 0, -30, -11, -48, -56, 38, 20, 28, -127, -96, -16, -45, 45, -114, 31, -42, -71, 27, -108, -6, -59, -71, -39, -16, -75, -32, -44, 75, -56, 75, -63, -20, 57, 65, 5, -69, 0, -32, 45, -1, 25, -26, -71, 5, -7, -49, 127, 41, 46, 91, -2, -13, -59, 12, 7, 12, -11, -90, -89, -15, 58, 123, 47, 117, -11, -97, -7, -92, 83, 4, -19, -10, -110, 58, -36, 25, 127, -66, -6, 33, 4, 3, -2, -30, -85, -28, -11, 34, -34, 27, -6, 1, -34, -54, 24, -21, -20, 127, -31, -62, 63, -28, -7, -64, -15, -15, -32, -3, -21, -80, -59, -39, 19, 55, 95, -115, 63, 45, 17, 49, 30, -127, -33, -7, 36, -5, 61, -53, -33, -31, 19, 36, 4, 10, -32, 70, -47, -45, -64, 10, -91, -20, 94, -127, -24, -30, 14, 64, 31, -15, 31, -29, -43, 8, -61, -76, 26, 14, -15, -71, -42, -15, -80, -28, -106, -41, -34, 22, -31, -56, -34, -50, -22, 54, 26, 110, -68, -127, 31, -7, -41, -1, -125, -86, -81, 38, -22, 19, 84, 2, 8, 17, -58, 56, 67, -38, 36, -34, 29, -17, -36, 16, 22, -57, 63, 127, 19, 22, 15, -52, -13, -62, 13, -27, 0, -36, -8, -46, 13, 19, -33, 37, -11, -68, 6, -57, -56, 37, -4, -46, -6, 15, -62, -44, -49, -11, -85, -3, -26, -22, -71, -40, -27, -71, 25, 60, 127, -77, -107, 20, -83, -77, 43, -77, -84, -37, -127, -12, -5, 61, -90, -75, -104, 53, 70, 61, 26, -45, -121, -42, -26, 117, -82, -41, -1, -30, -17, -41, -25, 80, -56, 16, 86, -9, -2, 48, 82, 70, -28, -49, 4, 51, 1, -65, 70, 28, 127, 74, 122, 79, -8, 10, 78, 74, -19, 127, -8, 3, -106, -22, -13, -33, -77, -20, -20, -10, -9, 12, -11, -18, 3, -85, -12, -17, -7, 69, 46, -24, -46, -9, -18, 2, -8, -45, 14, -70, 23, 24, -30, -41, 43, 50, -48, 48, 23, -127, 40, -39, 58, -25, -90, 47, 28, 83, 6, 13, -51, -26, -15, -4, -72, 11, 1, 39, 7, 43, -10, 20, 7, 23, 15, 67, -3, -8, 7, -11, -4, -23, -29, -43, -32, 6, -127, 0, -3, 17, 5, 14, 10, -70, -5, -42, -22, -13, -6, -7, 75, 36, 40, 2, -79, -42, -10, -19, 28, -38, -6, 43, 87, -31, -127, -18, -62, -78, 10, 17, 9, -99, -68, -52, -83, -58, 6, -68, -15, 116, -26, -25, -119, -93, 103, 40, 126, 38, -127, -16, -25, -45, 93, 54, -7, -64, -11, 40, -19, 26, -2, 71, 6, 125, -15, -11, 7, -8, 44, 42, -32, -11, 66, 27, 102, 91, -15, 14, -6, -45, 127, 25, 74, 17, -39, 35, 24, -43, -49, -9, -69, -24, -40, -126, -10, -24, -5, 66, -7, -28, -126, -53, 2, -127, -20, -59, -56, -95, -19, -49, -101, -38, 28, -31, 44, -10, 36, -5, 77, -4, 73, -40, 4, 9, -26, -32, -127, -80, -28, -58, 30, -11, -46, 29, -65, -5, -9, -13, 6, 116, -44, 9, -12, -15, -70, 36, -3, -43, -33, -75, -16, -6, -53, -50, 27, -61, 47, 7, 127, 60, 46, 44, 25, -44, -22, 32, 22, -4, 10, -36, -17, -47, -75, 14, 4, -127, -26, -47, 100, -22, 9, 86, 8, -75, 27, 74, 85, 7, -58, -65, 23, 46, 50, -43, -23, 0, -46, -30, -31, -127, -41, 54, -73, -11, 77, -84, 31, 56, 43, 17, 7, -39, -61, -13, -30, -19, -75, -10, -42, -13, -31, -4, 28, -64, -16, -24, -38, -29, -18, -2, -27, -25, -9, 19, 8, -16, 5, 29, -37, -44, -1, -4, 13, -73, -58, -4, -26, 26, 127, -32, -30, -26, 11, 11, 10, -62, 5, -41, -111, -78, -66, 62, -114, 20, -80, -28, -103, -46, -26, -35, -3, 96, -56, -11, 20, -100, 14, -34, -60, -127, -41, 47, 5, 68, -65, 58, -127, 27, -69, -119, -4, 47, -88, 80, -70, -55, -104, 55, -41, 14, -17, 43, 14, 81, -103, 15, -25, -41, 45, 19, 47, -8, 77, -41, -29, 12, -51, -2, 21, 7, -38, -7, -98, 21, 45, -54, -13, 22, 89, -46, -13, -44, 12, -47, -46, 44, -127, -82, -38, -80, -127, 31, -85, -48, -95, -63, 62, 0, -16, -15, -85, -14, 3, 12, 15, -47, 0, -34, -20, -83, 13, 4, 7, -41, -53, -38, -72, -112, -38, -30, 6, -45, 42, -19, 42, -77, -13, 38, 29, -44, -6, -5, -12, -21, -34, -13, -23, 24, 127, -21, 64, 69, -26, -34, -45, -40, 23, 18, 127, -5, 55, 70, -33, -23, 96, -47, 85, -101, 39, -35, -15, -87, -22, -16, 33, -24, -11, 11, 13, -56, -92, -1, -123, -30, 14, -47, -2, -3, -67, -127, -16, 48, -44, 17, -38, 124, -97, -38, -12, -30, -16, -19, -47, -9, 7, -32, -1, 8, -18, 12, 83, 26, -127, 39, -45, 119, 12, -90, -35, 79, -87, 21, 7, -21, 81, -19, 57, 5, -2, -68, -101, -83, -48, -86, -51, -83, 49, -8, -31, -20, -83, -42, -8, 3, -8, -57, -37, 48, 17, -61, 36, -27, 13, -54, -8, -97, -57, -78, 40, -31, 20, -127, -123, -59, -37, 9, -63, -54, -17, 38, 16, -102, 31, -17, 127, 11, 0, -15, -14, -64, -63, -2, 67, -43, -17, -28, -60, 67, -68, 4, 29, -75, -3, 53, -41, 65, -50, 28, -47, -63, 6, -40, 33, -10, -58, 66, 25, -1, -10, 15, 15, 7, -127, -33, -38, 98, 80, -4, 50, -24, -58, -105, 15, -4, -16, 14, -23, 64, 18, -3, 127, 27, -1, 6, 9, -41, -18, 10, 17, -66, -81, -25, 14, 12, -13, 16, 75, -20, -18, 6, -3, -127, -12, 33, -10, 2, -68, -59, -30, -40, 23, -33, -47, -17, 89, -23, -28, -25, 13, 88, -25, 16, -5, -16, -30, 48, 1, 1, -14, -1, -53, -2, -11, 27, -97, 127, 14, 19, -47, -67, -68, -61, 40, 3, 123, -21, -1, 23, -34, -91, 13, 71, 19, 4, -59, 10, 72, -25, -36, -11, -17, 3, 1, -1, 34, 34, 34, 27, -23, -42, 7, -8, 58, -23, -7, -3, -15, 8, -127, -1, -27, 13, 2, 11, -10, -24, -51, -20, -56, -10, -127, -1, -13, -55, -32, -5, 4, -3, -33, -27, -11, -48, -17, -59, -27, -25, -6, 13, 9, -4, -32, -45, -29, -92, -26, -28, -29, -22, 11, -22, -4, -34, -15, -11, -8, -44, 6, -14, -41, -54, -23, 66, -127, 10, -15, -44, 75, 3, -16, 2, -60, -11, -127, -119, -39, 59, 31, -39, 34, -8, 35, 22, -53, 76, 106, 11, -76, 34, -2, -73, 101, -82, -46, -27, -112, -35, 37, -18, -39, 32, 24, -33, 51, -42, 127, -99, -21, -7, 54, -78, 98, -29, 21, 39, -63, -83, 24, -73, -58, -33, -11, -33, -47, -117, -2, 33, 15, -18, 40, 85, 19, 32, -27, 8, 83, -18, 81, 54, -42, -92, -2, 43, 29, -75, -13, 0, -9, 108, 127, -3, -17, 117, 8, -23, 48, 43, -29, -123, -14, -84, -9, 37, -104, -61, 23, 37, -66, -20, -87, 33, 53, -4, 79, 67, 16, -3, -34, -73, -127, 12, 84, -41, 17, -2, -21, 13, -7, 28, 12, -16, 44, 22, -4, 34, -6, -35, -5, -1, 31, -31, -12, -30, 14, 22, -127, -18, -26, -16, -9, 25, -20, 86, 19, 34, -22, -36, 12, 99, -57, 67, -53, 17, 8, 19, -72, 36, 9, 50, 59, 1, -59, -127, 23, -11, -123, -52, -43, -23, -35, -59, -118, -42, -86, 105, -45, -82, 2, 51, -11, -22, 0, -61, 34, -19, 117, -40, -26, -69, -11, -127, 13, 6, 94, 32, 23, -4, -101, -79, -81, 46, -2, -42, 14, 40, 127, -14, -23, 9, -67, 38, -18, -34, -109, -41, -43, -46, 118, -64, -24, -35, -71, -61, -36, -63, -56, -48, -127, -26, 10, -19, -57, 24, -32, -11, -50, -27, -82, -33, -17, -9, -4, -32, -78, 23, -25, -15, -13, 16, -13, -12, 66, 52, -26, -1, 34, -24, 44, -44, -55, 8, -72, -94, 10, -125, 15, -79, 33, -12, -98, -37, -93, -60, -85, -52, -76, 15, -13, 127, -7, 35, 36, -100, -22, 26, 16, -127, 2, -28, 88, 62, 11, 8, 8, -42, 27, 18, 18, 55, -19, -33, 79, 51, -2, 56, 5, -54, 80, 80, 51, 15, 53, 74, -49, -9, 73, -89, -30, 110, -6, 4, 28, 42, -61, -77, 31, 22, -37, 36, -63, -28, 7, -127, 45, 48, 23, 83, 10, -64, 19, 65, -9, 127, 1, 1, 62, 45, 9, 106, -57, -74, 16, -1, -77, -22, 25, 34, -33, -51, -45, 36, 95, -29, -52, 35, -8, 21, 115, 40, -25, -17, -41, -41, -31, 85, 8, -48, -76, -25, -87, 31, -25, -69, -30, 32, -13, -38, 4, -7, 0, 41, -115, -112, -127, -48, -28, 57, 1, 36, -63, -25, -45, -6, 26, -19, 127, -55, -53, 13, -2, -76, -104, 3, -65, -19, -13, -95, 61, -29, 45, -61, -118, 36, -15, 11, -4, -15, 12, -50, -76, 10, -50, 32, -21, 33, -1, 5, 20, 17, -15, -34, -5, -82, 127, 13, -68, -4, -9, 10, -10, -84, -69, 83, -16, -39, -83, 16, 71, -57, -9, 17, -56, -72, -65, -29, 53, -30, -37, -93, -95, 46, -20, -18, 127, 32, -1, 6, -72, -9, 0, -58, 25, -17, -43, -10, -36, 97, 6, -16, -51, 2, -10, 18, 26, -56, -127, 10, -6, 19, -23, 38, 74, -16, -37, 35, -31, 127, 18, -59, -16, 10, -42, -12, -79, 19, 9, 9, -37, 34, 14, -35, -82, -54, 7, -10, 29, 13, -46, 4, -98, -30, 42, 1, 22, -35, -41, -57, -102, 39, 4, 12, 4, 56, 4, -83, 43, -8, -60, 19, 29, 19, 25, 25, -59, -46, 19, 45, -122, -127, -40, -74, -102, -17, -39, -25, 6, 56, -49, 17, 44, 60, 36, 28, -37, -60, -5, -10, 55, -45, -1, 35, -12, 17, -127, -14, -28, 13, -17, 36, -17, -52, 53, 56, -38, 15, -47, 30, 127, 38, 32, 23, -50, -36, -34, 16, 69, -37, 80, 52, 2, 60, 48, 59, -16, -66, -6, 43, 33, -43, -125, 127, 20, 71, 33, -38, 62, 23, -25, 119, -89, -116, 23, 41, -5, -58, -48, 39, 76, 26, 38, 10, 48, -17, 52, -12, -92, -37, -24, 78, -58, 33, -7, -50, 40, 4, -1, 31, -31, -82, 65, 14, 121, -36, -42, -2, -6, 33, -49, 33, 70, 74, 11, 66, -127, -67, -50, 123, 22, 7, 84, -85, 35, 16, 23, 42, 64, -74, -51, 6, -55, 49, -51, 45, -15, 46, 17, -53, 103, -46, 48, 42, -127, 13, -1, -17, -62, -42, 12, 31, -127, -20, 26, -60, -19, 36, -42, -41, -8, 37, 43, -26, -3, -29, 0, 33, -61, -17, -17, 24, 4, -50, 24, 53, 62, 5, 84, -127, 5, 20, -4, 124, -95, 37, 74, 25, 35, -24, -15, 45, -78, 28, 60, -41, 36, -119, 64, -35, -58, 119, 3, -47, -22, -35, 49, -127, -91, -25, 97, -16, -27, 98, -12, 16, 56, -20, -49, -70, 19, -40, 4, -81, -68, -106, 47, -22, 63, 21, 28, 10, -29, -77, -127, 79, -4, -46, 6, 20, -59, 5, -54, 5, -87, -37, -61, 110, 55, 33, -41, -47, 77, 67, 37, -5, -75, -115, 2, 46, 55, -43, 34, -107, 27, 24, -57, -14, 47, 58, -48, -2, -40, 127, 87, -52, 99, 32, -122, 56, -120, 60, -38, -20, -35, 15, 13, 36, -4, -48, 26, -7, 127, 17, 16, 2, 26, -36, 5, -1, 31, 17, 3, 20, 5, 47, -42, -10, 45, -2, 7, 27, 50, -96, 31, -67, 25, 15, 23, -10, 117, 17, 0, 47, 18, -127, 52, 16, 84, -31, -49, 95, 94, 127, -10, -81, -19, 54, -48, -8, 60, -23, -103, 119, 23, 2, -7, -48, 110, -70, -49, -44, -79, -94, 34, 9, -20, 39, -25, 0, 36, 48, 106, 55, 0, -47, -72, -51, -127, -31, -1, 47, 34, -30, 14, 3, -1, 14, -31, -6, 22, -69, 37, -6, -12, 12, -56, 17, 5, -31, 127, -13, -2, -2, -45, -4, -28, -2, 19, -54, 0, -13, -22, 18, -66, 49, -30, -26, -41, 17, -58, -2, 9, 8, -127, -13, 1, -55, -21, 49, 40, -25, -29, 60, -48, -24, -88, 88, -32, -88, 2, -34, 31, -7, -31, 82, 30, -103, 127, 2, 48, 37, -51, -44, -75, -11, -6, -23, 47, 87, -14, 11, -57, -30, 41, 27, -60, 127, 3, 69, 22, 33, -22, 54, -46, -77, -5, 42, -32, -77, -99, 23, -93, -83, 12, -2, -43, -7, -30, 9, 50, -1, -51, -45, -43, 8, 24, 12, -22, -5, 9, -12, 127, -20, 8, 9, 2, -15, -38, -29, -12, 9, -7, 1, -23, -44, 6, 20, -6, -7, 4, 10, -9, -75, 19, -34, -22, 43, 2, 25, -18, -7, -37, 2, -33, 9, -28, -32, -4, -53, -18, -51, -9, -9, 37, -56, -127, -68, -87, 127, -23, -2, -45, -4, 0, 17, -28, -12, 5, -16, 68, -11, 4, -20, -58, -5, 5, 5, -18, 15, 58, 10, -23, 95, -67, -90, -26, 74, -28, 127, -25, -77, 60, 30, -58, 69, -6, -86, 90, -41, -125, 48, -27, 26, -38, -82, -49, 33, 34, -50, -21, 10, -6, 24, 13, -44, 9, -15, -117, 12, -83, 74, -12, -17, -47, -56, -36, 1, -42, 17, -127, -20, 2, 11, 13, 62, -21, 38, -5, 20, 8, -45, -90, 124, -54, -13, -53, -93, 10, 97, -26, 12, 26, -105, 96, 37, -46, -17, -127, -65, -52, 28, -31, -37, 62, 3, 42, 78, -94, -11, 52, -83, 15, 35, -6, -23, -51, -41, -78, 17, 0, -4, 27, 16, -79, -45, -127, -20, -35, 42, -2, -53, 29, -18, -73, -58, -72, -40, -24, 33, -29, -76, -55, 57, 16, 14, -52, -16, -99, 47, -68, -60, -94, -127, 63, -57, -82, 0, 12, -29, -19, -60, -31, -44, 77, 31, -29, 94, 15, -4, -34, -114, -15, 48, 1, 43, -39, 44, -27, -78, -30, -29, 2, -127, -29, -49, 57, 10, -29, -89, 9, -24, 52, 127, -8, 40, -34, -47, 24, -85, -13, -6, -11, 39, -43, 5, 86, 10, 69, 91, -54, 3, 5, 21, 4, 8, -80, 16, 25, 13, -76, -67, 0, -71, -6, -11, 72, 58, 80, 28, 60, -64, 36, -24, 21, -44, 28, 14, -88, 76, 54, 117, -48, -45, 17, 127, 104, 96, -41, 10, 48, 44, -115, -15, 33, -127, -19, -62, 67, -46, -56, -19, 19, 27, 38, -40, -43, 55, 5, -5, 14, 28, 45, -37, -84, -38, -52, -11, 6, 34, 7, 35, -24, -43, 14, 27, -19, -23, -45, -127, 18, -17, 15, -53, -6, -25, -14, -48, 17, -55, -6, 58, -3, -35, -5, 17, -32, -31, 20, 29, 5, -42, 12, 37, -14, 45, -25, -56, -19, 14, 17, -41, -14, -50, 7, 3, -127, 33, -11, -16, -20, 15, -3, -16, 46, -47, -15, 127, -116, 18, -34, -36, 16, 62, 37, -88, -31, 6, 118, -6, 14, -63, 36, 77, 45, 9, 33, 31, 20, 30, 31, -94, -19, -22, -67, 127, -16, -15, -25, 14, -56, 10, -83, 14, -5, -6, -23, 22, -58, 68, 47, -31, 30, 20, 68, 0, -1, -3, -40, 16, -31, 59, 31, -46, -38, 67, 112, -115, -53, -20, -71, -49, 112, 44, 16, 20, -4, 39, 99, -28, -61, 127, -104, 43, -17, 54, 64, -46, -28, 58, -46, 37, 40, 1, 79, 31, -24, 127, -21, -121, 5, 29, -25, -48, -5, -30, -32, -65, -18, 0, -47, 21, -51, 21, -3, -28, 127, -73, -34, 12, -47, -51, 26, 33, -14, -20, -67, -10, 14, 25, 17, -21, 26, -58, -21, -15, 32, -6, -8, 28, -93, -38, -112, 33, -86, -84, 93, 4, -48, -67, 22, -44, -49, 93, 62, -103, -39, 18, 127, -42, -96, -123, 19, -36, 36, 114, -19, -18, -15, -12, -13, -7, -17, 36, -19, 2, -94, -10, -127, 69, -1, 48, 19, -16, -6, -44, 18, -40, -3, -31, -105, 31, -23, 25, -12, 108, 34, 19, -23, -31, 6, 60, 18, -104, -7, 26, -20, 60, -4, -15, -18, -37, -97, 24, 73, -30, 4, -57, 28, 40, -127, 39, -8, -77, -5, 9, 38, -70, 14, 78, 0, -127, -7, -40, 125, 63, -19, 82, 25, -35, -60, -3, 47, -12, -27, -6, 29, 94, 63, 91, -39, -39, -18, -111, 39, -28, -127, 97, -2, -12, 51, -10, 39, 14, 12, 117, -13, 34, 80, 63, 11, 16, -55, -51, -22, -40, 10, 87, -77, -114, -61, 6, 4, 79, -22, -23, -21, 38, -9, -52, -4, -15, 13, -43, 32, -84, 21, 62, 127, 25, -70, 1, -49, 13, 19, -17, -73, 45, 14, 1, -63, 16, -56, 41, -13, 93, -126, -32, -33, 45, -41, 124, -106, -71, 31, 9, 51, -47, 21, 25, -90, -127, -73, -22, 57, 2, -37, 4, 1, -82, -50, 62, -8, -57, 39, -80, 39, 25, -8, -59, 10, -49, -6, -37, -7, 0, -127, -66, 76, 58, 72, 62, 50, 15, -4, 33, -76, 119, -31, 14, 15, 45, -75, -24, -127, -45, -39, 15, -32, -51, -7, -43, -107, -29, -41, -19, 2, -75, -10, -3, -16, 82, 2, -30, -43, 45, -26, -81, 17, 109, -37, -82, -82, -6, 41, 17, -10, 27, 49, 44, -69, -61, -96, -48, -25, -75, 62, 28, -52, 127, -75, 22, -41, 81, 43, -43, -9, 127, -45, 2, -111, -14, 17, 10, 33, 45, -40, 36, 21, 16, 4, -3, -42, -37, -13, -31, -33, 1, 13, -2, 6, 82, 14, -127, -47, -127, -25, 126, -31, 62, 36, -44, -118, 66, 51, -34, -8, -78, -24, 61, 103, -34, -20, -20, -6, 40, 6, 24, 67, -127, -46, 6, 10, 1, 15, -12, 36, -13, 25, 41, -64, -61, -26, -27, -103, 8, 66, -16, 12, 6, -20, -10, -14, -29, 19, -9, -9, 9, -21, -25, -7, 25, -22, -64, -78, -39, -7, 34, -2, -32, -27, 21, 88, -8, -46, -82, -27, 1, -17, 11, 127, -5, -5, 12, 16, 74, 10, -52, -22, 76, -81, 26, -68, 41, -61, 52, -13, -115, -9, -26, 5, -56, 29, -102, -127, -24, -76, 46, -59, 30, 41, 49, 14, -48, -44, -14, 1, 43, 51, 25, 43, -59, 27, -127, 44, -44, -14, -56, -78, 23, -20, -31, 31, 90, -15, -41, -20, 18, -12, 39, -43, 85, 33, -127, -29, -122, -67, 19, -104, 45, -28, 23, -65, 25, -106, 33, 36, -98, 7, 58, 52, -3, -74, 25, -6, 3, -37, -18, 72, -91, -21, 28, -40, 127, -14, -25, 98, 37, -30, 69, -6, -81, 53, -13, -31, -26, -69, -35, -82, -28, 24, -9, 6, 20, -12, 0, -62, -47, -64, -81, -14, -17, -1, -36, 27, 1, 11, -59, -45, -71, -15, -35, -7, 0, 39, 42, -23, -21, 23, 42, 30, 12, -127, -99, -55, -119, -92, 81, -15, 15, -89, -46, 27, 66, -26, -42, -85, -105, 66, -9, 21, 65, -115, 16, -81, 42, 42, 103, 73, -127, -104, 70, 18, -45, -11, -6, -10, -47, -72, 25, 127, 28, -57, 84, -17, -69, -49, -38, -1, -17, 4, -54, -13, 26, -20, 3, 40, -67, 4, -11, -42, -110, -67, -21, 53, -78, -54, -28, 101, -47, -28, -65, -3, -51, 3, 4, 42, 16, -127, 91, -54, 32, -34, 47, 24, 74, 6, -27, -90, 2, 27, -127, 22, 78, 73, 50, -53, 31, 71, 57, -64, -51, 39, 58, -98, 38, -31, 54, 42, 7, 14, 2, -109, 100, -7, 103, -61, -32, -31, -20, 9, -127, -34, -19, 1, -42, -15, 54, -26, -31, 21, 5, 28, -116, -23, 30, 22, 24, -13, 52, -36, 3, -22, -19, -26, -110, -52, -5, -32, 81, -23, 10, -31, -87, -59, -35, -87, 74, -127, 32, 10, -11, 62, 6, -13, 49, 79, -60, -48, -56, -17, 34, 16, -34, 10, -5, -13, 4, -20, -30, 13, 2, -14, 25, 18, -33, 8, -3, 9, -18, -60, 6, -50, -17, -15, -10, 127, -8, -14, -7, -15, -44, 7, 4, 34, -46, -74, -32, 83, 70, 12, 50, -7, 52, 23, -29, 27, -53, -127, 48, -2, 114, -9, -35, 75, 14, -36, -35, -61, 84, -16, -83, -7, -93, -8, -9, -102, -56, 5, -12, -24, -50, 9, 13, 1, -52, 15, -9, -72, 0, -108, -127, -69, 3, -44, 2, -41, -17, -74, 41, 9, -40, -72, 6, -36, 25, -33, -20, -127, 14, 3, -41, -91, 59, -124, 90, -19, -30, -58, 43, -51, -108, -85, -10, 69, -41, 10, 54, 14, 19, 35, -20, 60, 20, 60, -26, 32, -74, 4, 14, 9, 25, -107, 18, 23, 24, 127, 87, 11, 16, 9, -1, -30, -31, -22, 0, 13, -15, -46, -23, 4, -5, 6, 4, -2, 14, -23, 2, -12, -127, -17, 22, -72, 3, -7, 91, -2, 4, 10, 3, -7, 11, -83, 24, 25, -24, -1, 47, 70, 10, 50, -8, -127, 28, -18, -34, 55, -72, 90, 35, 113, 92, 81, -52, -57, -33, -63, 15, 50, -27, -93, -37, -71, 11, 49, 34, 42, 36, 42, 9, -92, -12, 39, -45, 97, -127, 99, -72, 64, 39, 69, 46, -47, -24, 104, 58, -119, 7, -4, -39, 5, -12, -33, 14, -81, 69, 17, -4, -44, 21, -13, 3, -15, -15, -127, 15, 11, -25, -47, 21, 61, 10, -19, 46, 9, -28, -39, -48, -67, 43, 56, 8, -5, 33, -11, 7, 26, 30, -9, 21, 76, 5, 30, -2, -69, -52, 35, -34, -89, -17, -127, 9, -45, 127, -20, -88, -45, -20, -61, -34, 5, 32, -45, 7, 0, 71, -43, 37, -20, -30, -63, -27, -37, 23, -23, -43, 31, 6, -39, 1, -21, 34, 50, -32, 57, 14, -45, -54, -3, -20, -48, -12, -14, -39, -77, -14, -1, -36, -34, -30, -68, -34, -11, 4, -26, -97, -7, 30, 127, -44, 2, 17, -43, 114, 17, 16, 32, 41, 1, 69, 1, -49, 42, -27, -79, -2, -53, -26, -127, -62, -2, 5, 12, -59, -10, 9, -5, 66, -20, 39, -78, -35, -108, 3, -62, 127, 44, -56, -107, -16, 28, 56, 45, 6, -52, 47, -119, -62, 70, 75, 13, 72, -46, -49, -39, 2, -97, 11, 4, 71, 2, -99, 60, 36, -5, 127, 11, -88, 105, 1, -38, 53, -86, 4, 9, 59, 63, 42, -70, -14, 19, 17, -87, -29, -20, 53, -48, 127, -9, -27, 46, 32, 4, 95, -6, -49, 53, -29, -32, -9, 6, -81, -42, -73, -86, 24, -76, -77, 15, -15, 8, -35, 44, 100, 0, -9, 7, -58, -25, -14, 23, -27, 127, -93, -47, -27, -3, -46, -26, 38, -34, -26, 0, -31, 57, -17, -64, -48, -56, -50, -68, -1, -44, 2, -65, -44, -19, -110, 16, -97, -83, -120, -30, -4, 107, -62, -60, 23, -127, -9, 9, 50, 47, -12, -15, -12, -57, -64, -82, 25, 62, -19, -37, -101, 33, 7, -43, 17, -66, -78, 23, -34, 9, -65, -13, -63, -112, -33, 8, -31, 23, -24, -5, -24, -127, -9, -14, -15, -69, -13, 12, 56, -31, 3, 16, -13, -45, -75, -23, -10, -10, 127, -23, -65, 76, -29, -7, -62, -7, -12, -19, -2, -9, 10, -71, 118, 5, 105, -24, -88, 50, -1, 43, 127, 19, -121, 24, -17, -37, -21, -14, -68, -17, -16, 62, 38, -29, 4, -2, 10, -5, 11, -17, -96, -47, 8, 19, -6, -1, 27, 31, -46, 60, -75, -14, -18, 0, -48, -54, 1, -29, 12, 0, -30, -15, -127, -18, -28, -5, -53, 0, 28, 12, -28, 18, -16, 9, -35, 3, -108, -74, -127, -44, -116, -2, -66, -95, -41, 8, 4, 12, -56, -11, 103, 4, -14, -7, 13, -87, 50, -69, -49, -25, 103, -19, 46, 22, 127, 21, -105, 91, 14, 38, -16, 21, 30, -22, 34, 28, -39, -80, 110, -85, 43, 45, 43, 20, -107, -38, 42, -69, -69, -31, 45, -21, -19, -11, 18, -127, 55, 40, -24, -81, -70, -51, -99, 14, -92, -48, 41, 2, -41, -87, -4, 23, -26, -3, -7, -43, 12, -21, 11, -36, -5, -35, -23, -10, -1, -27, 32, 3, -14, -35, 7, 3, 0, 4, 9, 55, -64, -127, -100, -39, 30, -36, -46, -9, -19, 61, -19, 32, -22, -1, -67, 30, 14, 8, 16, -127, -3, 13, 19, -32, 32, 57, 4, 15, 3, -36, -16, -127, 51, 17, -36, 39, -118, 67, -12, -40, 27, -55, -126, 22, 10, 8, -18, 11, -87, -36, -67, 73, 8, -85, -124, 54, -26, -107, -99, -22, 76, -34, -89, -7, 38, 127, 29, 26, -4, 1, -100, -14, -1, 81, -12, -79, -73, -96, 62, -37, 32, 49, -59, -68, 9, 14, -29, -15, 27, -19, -80, 62, 14, -13, -50, 10, -73, 8, 0, 29, -9, -26, -9, -6, -34, 27, -6, 24, -17, -109, -127, 28, 3, -95, -70, 18, -11, 23, 4, -40, 7, -4, 85, -11, -3, -8, 3, -35, 8, 29, 45, -127, 8, -2, 22, -7, 41, 110, -7, -35, 11, -50, -17, 55, -120, -19, -41, -9, 81, -53, 127, 5, -64, -45, -21, -66, 53, 60, -88, -45, 26, -40, -52, 48, -52, -5, -57, -18, 80, 42, -64, 0, 23, -24, 90, -42, -31, -74, 40, 24, 90, 11, -33, 27, 9, 1, 37, -11, -25, -127, 18, 85, 10, -57, 10, -68, 47, -54, -48, -9, 96, -33, -61, -98, -71, 107, 17, 55, 29, -40, -58, 48, 26, -30, 15, -25, -66, -53, 26, 27, 2, -33, -50, 3, 127, -40, -1, -1, -30, 33, -112, -117, -105, -109, 1, 8, 20, -27, -69, 45, 46, 4, -23, -13, 23, 27, 29, 83, -127, 35, 63, 92, 98, -113, -18, 72, 127, -36, -8, -22, -45, -21, -12, -48, 27, 8, 23, 2, -1, 21, -61, -62, -22, -82, -24, -8, -42, -18, -32, 41, -11, -107, -35, -8, 42, 54, -48, -42, 20, 102, 11, 30, 46, -19, 44, -66, 4, -118, 95, 25, 24, -30, 30, 26, 127, 64, -47, -62, -7, 24, 37, -50, 78, 13, -119, -46, 5, -63, 0, -29, 102, -39, -92, 67, -6, 17, -36, 37, 11, 17, 127, 25, 104, -47, -16, 10, 121, 23, 35, -67, -29, 8, -127, -57, 5, -34, 75, 10, -36, -36, -80, -22, -3, 47, 42, -37, -25, -38, 17, -44, -14, -19, -96, -71, -24, 32, -29, -92, 15, 16, -11, -4, -46, -39, 0, 14, -27, 4, 30, -48, 14, -19, -46, -5, -8, 4, -17, 1, -52, -30, 32, -116, 127, 17, -74, -73, -121, -35, 9, -92, -86, -63, -63, 70, -26, -94, -67, 42, -86, -104, -89, -127, -17, -84, 13, -54, -31, -59, 37, -7, -73, 26, -8, -71, -42, -47, -16, 36, 0, -31, 69, -4, 35, -127, -52, -31, -15, 12, -76, 42, -44, 3, -27, 19, 31, 37, -42, -4, 36, 28, -8, -11, 103, -60, 95, 28, 0, -56, 102, 41, 78, -7, -116, 93, -18, 42, -4, -77, -102, -45, 24, 127, 59, -11, -6, 25, 83, -39, 25, 57, 65, 37, 62, 24, 73, 101, 63, 43, 59, -28, 27, 93, 47, 30, 10, 32, 21, 99, 70, 127, 50, 10, 70, -6, 48, 18, 13, -58, -90, 73, -70, 15, -83, -75, -25, -28, 37, -20, 18, -89, 5, -49, -127, -80, 2, -8, -41, -69, -10, 6, 5, -53, -28, -12, 35, -69, -52, -21, 127, -56, -56, -85, 79, -77, 110, -9, -68, -40, 49, 71, -23, -63, -46, -68, -62, -30, -37, 62, 40, -23, 16, -8, -6, -15, 0, -62, 8, 92, -61, 8, -6, 53, 121, -127, -108, -14, 5, -57, -36, 81, 18, -34, -5, 96, 14, -114, -104, 47, 11, 23, -25, -30, -44, -6, 59, 10, -121, -15, -70, -127, -63, 61, 12, -125, -28, 61, -41, -82, -89, 23, -52, -14, -45, 12, 89, 57, -10, -23, 45, 29, 61, 14, 63, 42, 50, 63, 28, 34, 56, 40, 24, 96, 45, -14, 21, 16, -40, 79, 105, 127, 51, -17, 83, 35, 38, 34, 10, 17, -51, 34, -38, -7, 68, -127, -35, -1, -4, -37, 18, -20, 1, 35, -47, -41, 118, 94, 55, 41, 0, -31, 87, 47, -13, 33, 14, -27, -47, -31, -45, -44, 16, -27, 52, -47, -13, -60, -7, -127, 18, 13, -99, -3, -50, -31, -14, -14, 10, -113, -52, -53, 0, 19, -17, 44, -81, -17, -20, -60, -58, -30, 20, -56, -9, -92, -8, -109, 7, -39, -32, 20, 0, -41, -7, -14, 6, -60, -47, -40, 23, 127, -40, -11, 56, 6, -32, -77, 74, 17, 7, -38, -54, -9, 7, -9, -12, 4, 14, -127, 40, -15, 5, 102, 7, 81, 81, -92, -4, -11, -80, -73, -73, -32, 11, 25, -25, -60, -51, 57, 14, -82, -13, -51, -26, -6, -60, 78, 36, 54, 14, 127, 47, 61, -110, 64, 10, -21, -25, -29, 55, -41, 16, -31, -36, -4, 72, -45, 70, -58, -78, 127, 13, 3, 7, -42, -2, -58, -43, 18, 45, 12, 61, 13, 41, -1, 13, 20, 17, 13, -5, 2, 127, -63, -41, -18, 122, -64, 14, -107, -41, -23, -59, -63, 6, -2, 50, 51, 32, -19, -27, -3, -22, -8, -5, 99, -12, 43, -127, -35, 25, -107, -35, 24, 6, -37, 75, 1, 23, 52, 1, 12, 61, -8, -17, -36, -21, 72, 27, -1, 25, -20, -76, 30, 36, 30, -104, 12, 0, 127, -11, 10, 25, 10, -44, -96, 29, 90, 37, 28, 2, -32, 61, -14, 37, 19, -50, -44, 50, 49, -2, -5, -9, -127, 3, -43, -7, 4, 16, -18, -2, 44, -2, -1, -3, -5, -8, -3, 1, 25, 9, 6, -6, -2, 11, -13, 5, 25, -51, -56, 94, -38, -66, 13, -30, 65, 52, -58, 42, -39, 38, 39, 30, -36, 18, -120, -127, 32, 78, 80, 54, 87, -121, -3, 84, -18, 23, 12, -84, -41, 14, 15, -29, 55, 59, 89, -127, -2, 1, -27, -27, -11, 89, -2, -14, -71, -39, 7, -19, -50, -14, -43, 40, 4, 28, 31, 59, -12, -38, 127, 45, -52, -95, -28, 71, -105, -71, -108, -14, -39, 23, 34, 71, 96, -81, 44, 73, -13, 20, 45, -75, -117, -13, 18, 112, -4, -46, -11, -61, -45, 7, -30, -16, 10, -44, -38, 21, 8, -68, -67, -22, -54, -24, -11, -45, 27, -39, 114, -43, -127, 80, 64, -15, -2, 33, -28, -10, 28, -40, 127, 1, -76, -57, -28, -3, 12, -56, -45, 11, 123, -32, 17, 65, 12, 89, 25, 22, 30, -44, -122, 28, -6, -6, 53, -29, -58, 127, -53, -30, -22, -91, 14, 3, 18, -64, -24, -56, -36, -31, -15, -96, -54, -10, 1, 48, -78, 6, -48, 0, 14, 31, -41, -31, -17, 42, 93, -19, -12, -108, 48, -1, -17, -5, -127, -37, -91, 9, -44, -22, -63, 22, 0, 11, -33, -51, 23, -50, -64, 7, 41, 80, 51, -83, 35, 105, -48, -120, -75, -6, -29, -77, -99, -1, -90, 61, 127, -22, -72, -8, 36, 51, 40, -91, 0, 49, 29, 18, -8, -42, -3, 48, -22, -40, 19, -127, -47, -31, 85, 23, -26, -20, -47, 56, 22, 64, 103, -77, -30, 92, -19, 18, 46, -40, -6, -9, 26, -21, 9, -127, 17, -100, -17, -40, 69, -20, -24, -40, -33, -26, 69, -10, 12, -13, -81, 7, -27, -27, 10, -47, -57, 121, -23, 45, -22, -35, 4, -22, 42, 113, -75, -84, 127, 19, 17, 7, -100, -52, -83, -95, 29, 45, 5, 2, 4, 16, 6, -43, -62, -31, -25, -39, 15, -18, -35, 127, 0, -78, -22, -42, 0, 11, -59, -19, -7, -3, -35, -17, 1, -42, -6, 1, -44, 22, -21, 56, -76, -100, 0, -127, -53, -3, 33, 50, 60, -25, 1, -26, -57, -75, -30, -51, -15, 27, 49, -70, 28, 107, -32, -42, -75, 4, -13, -6, -43, 127, -120, 47, 50, -8, 69, 59, -70, 36, -69, 31, -34, 74, -26, -46, -35, -1, 24, 70, -39, 92, 54, -7, 8, 58, -71, 45, 17, -103, 7, 41, -19, 9, -76, -88, -43, 1, 91, 37, -25, -52, 1, -21, 46, 12, 86, -32, -19, -127, 57, -51, 51, -7, 84, -14, -85, -1, -26, -29, -27, -75, 119, 78, 18, 63, -3, -127, -25, 0, -63, -30, -14, 47, 117, 41, -10, 22, -64, 8, -45, 6, -45, -115, -38, -29, 13, -92, -112, -113, -70, 87, 92, -39, 33, -26, -75, -27, -31, -71, -127, -97, -122, -12, -74, -64, 34, 89, -39, 2, -66, 77, 14, -62, 16, -5, -23, 34, -30, 27, 5, 32, -61, -2, -48, -1, -34, -53, -127, -32, -10, 13, -12, -11, -73, 61, -20, 11, 4, -44, -42, 70, -16, -6, -49, -87, 36, -85, -23, -15, -33, -78, -11, -30, -14, -75, -62, 10, -67, 17, 28, 6, -96, -41, -80, -47, -127, -14, -50, -99, -44, 26, 2, -62, 31, 12, 66, -73, -46, -28, -88, -23, -20, -26, -39, 127, -14, 26, 2, 10, -54, -66, -33, -14, -33, 8, 11, -14, 36, 35, -67, 18, 6, 10, 111, -61, -80, 8, -5, -8, -6, 91, -55, -40, -68, 30, 7, -1, -127, 27, 4, 7, -28, 23, 21, -38, 19, -53, 25, 62, -87, 63, 19, 4, -32, 37, -26, 51, 76, 59, -26, 29, -2, 127, -27, -2, -24, 28, -46, 72, -16, 18, 25, 0, 66, 19, 18, 39, -64, 5, -6, -26, -56, 34, 79, 8, 17, 35, -36, 67, 33, 31, -81, 127, -7, 9, -20, -13, 26, 20, -23, -70, -19, -51, -38, -15, -37, 38, 5, -16, -19, -18, 64, -45, 10, 3, -59, 39, 23, 2, 26, 1, -127, 16, 23, 7, -51, -74, -25, 73, -51, -10, 17, -20, 127, 11, -83, 18, -50, -99, 18, -50, 0, -15, -32, -31, -41, -47, -15, 66, 90, -40, 15, 37, -1, -79, -68, 85, -39, 107, -27, 72, -7, 49, 52, 67, -9, -66, -2, -57, 125, -109, -83, 6, -51, -127, 44, 27, -16, -95, 90, -100, -29, -26, -72, -55, -3, -27, 36, -38, -16, -42, -32, -26, -52, -49, -69, 38, -57, -12, 45, 14, -15, -43, 13, -10, -127, -103, -33, -4, -56, -96, -52, 80, -48, -48, 17, -90, 49, -28, 17, 90, -96, -102, 24, 103, 23, -20, -127, -53, -35, 92, 24, -105, 45, 68, -22, -72, -46, -48, -17, -87, -30, -6, -78, -38, -38, 3, -48, -29, -24, -89, -127, 24, 10, 7, 11, 27, -25, -10, -58, 10, -28, -100, 18, 22, -37, -105, -19, 103, -18, 26, -69, -80, 127, 54, 4, 20, 8, -110, 81, -2, -38, 38, -69, -4, -100, -39, -3, 26, 34, 11, 16, 17, -38, -40, -13, 28, -11, 127, -5, -53, 12, 12, -22, 46, -10, -43, 32, -17, -91, 37, -21, 5, -25, -64, -36, 35, 28, -20, 2, -10, 1, 127, -82, -75, 31, -19, -23, -8, -38, -46, -28, 33, -107, -52, -13, -31, -41, -114, -16, -62, -37, -51, 14, -9, -61, -54, 42, -14, 111, 78, 2, -57, 12, -11, 3, 76, -93, -17, 57, 12, -47, 76, 84, -14, -26, -27, 38, 59, 70, 21, 30, 1, 82, 101, 22, 22, -127, -86, -51, 64, -38, -42, -13, -98, 112, 73, -69, 68, -65, -59, 74, 4, -127, 33, -22, -63, -100, -38, 35, 40, -8, -104, -76, 6, 3, 1, 18, 79, -60, 126, 19, -19, 96, 33, 14, 81, -38, -102, 68, -21, -33, 21, -1, 2, -127, -82, -77, 23, 32, -37, -40, 6, 67, -63, -57, 127, -49, 66, -61, -120, 26, 63, -21, 26, -34, -73, 48, -34, -27, 6, 46, -7, -47, -49, -39, 31, 25, -110, -75, 34, -58, 91, -26, -76, -28, -35, 36, 34, -127, -59, 6, 47, -5, 18, -14, -15, -11, -75, 36, -21, 3, -10, 13, -45, -106, 12, -17, 2, 45, -49, -91, 59, 0, -98, -51, -30, 2, 1, -33, 5, -26, 27, -8, -10, 127, -49, -76, -77, -49, 62, -26, -12, 34, 72, -25, 12, -23, -5, -19, -62, -4, 127, 1, -10, -109, 19, 11, 9, -24, 47, -17, 35, -111, -27, 62, 17, 29, -37, 6, 3, -24, -26, -2, -26, 5, 53, 2, 0, -11, -45, -13, 2, -62, -16, -53, -34, -97, -13, 35, 13, -21, -8, -127, 11, 9, 12, 4, -105, -14, -20, -9, 9, -3, -38, 21, 24, -8, -103, -25, 25, 77, 127, 23, 44, -37, -12, -8, -15, 76, 20, -46, -74, -22, 94, -77, 77, 1, 61, -63, 25, -41, -9, -40, 48, -41, 98, 4, -11, 127, -3, 38, 83, 21, -103, 42, -10, -71, 16, -21, 2, -78, -52, -29, 7, -7, -29, -20, 5, 0, -9, -47, -17, -72, -30, -38, -5, -117, -85, 88, 74, 17, -108, 6, -24, -13, 35, -127, 10, -115, 23, -17, -24, -65, 89, -48, -4, -83, -77, -31, 27, -32, -1, -127, -3, 32, 54, 27, 40, -18, -1, 32, 21, -24, -80, -37, 28, -28, 18, -19, -28, 26, -4, -25, -11, -14, -45, -75, 50, -23, -56, -117, -111, 95, -35, -98, 13, 1, -105, 99, 80, 97, 41, -127, -9, -66, 78, 59, -25, 10, 46, 8, -30, 60, 74, -7, 23, -64, 62, -14, 2, 23, -8, 52, 117, -59, -119, -13, 43, 81, -107, 14, -44, -127, -89, -98, 40, 67, -36, -20, 19, -64, 12, 23, -127, 18, 6, 69, 4, -14, 78, -9, -13, -12, -77, 40, -45, 18, -65, -45, -65, -32, 16, 2, 38, -51, 34, -69, 13, -10, -60, -12, 0, 53, 18, 127, -103, 6, 30, 46, -29, 44, -5, 4, 30, -36, 13, -6, -35, 44, 48, -18, 21, 10, -56, -15, 23, -23, 90, 127, -46, 8, 81, -31, -20, 1, 86, 34, -10, 19, 52, 111, 51, 54, 44, -75, 5, 16, -45, 78, -121, -53, 70, 74, 100, -109, 9, -22, 1, -51, -35, -17, -35, 2, 24, 34, -98, 62, -29, 28, 38, 82, 16, -127, -28, -60, -6, -18, 95, 63, 6, -26, 18, 6, 8, 42, -1, -66, 54, 22, -26, 63, 110, -54, 26, 26, 15, -21, 68, -12, 60, -78, 2, -127, -94, -25, -3, -28, 10, -29, 60, -23, 32, -19, -77, 3, -127, -44, -83, -81, -41, -44, 17, -57, -47, 90, -88, 9, 67, -82, -4, 4, 5, -2, -100, -72, 71, 39, -33, -109, 104, 54, 32, -35, 55, -47, -48, 4, 33, 22, -15, -31, 16, 28, 20, 127, -11, -10, -45, 13, -45, 7, -48, -39, 2, -92, -46, 32, -13, -23, -22, 32, -18, 127, -34, -5, 17, -26, -16, -73, -22, -25, 3, -15, 15, -31, 9, -22, -9, -13, -59, 8, 11, -27, 25, -8, -119, -127, -30, 11, -44, 43, -75, -54, -56, -18, -73, 21, -22, -89, -23, -77, -74, -37, -42, 16, -39, 7, 14, 23, 66, -49, 46, -7, -88, 25, 31, 2, -79, 1, -9, 97, -37, -16, 17, 63, -41, -48, 31, 66, -12, -9, 27, -77, 127, 37, -73, 35, -7, 11, -34, 62, 72, -84, 40, 94, 17, -127, -58, -73, 93, 17, -40, -81, -20, -62, -57, -33, 21, -120, 43, -36, 13, -4, -45, -37, 43, -41, -14, -5, -120, -1, -52, 12, 21, 10, 3, 9, -19, 81, 26, -59, 38, -48, 22, 49, 15, -127, 7, -31, 24, 2, -29, 29, 53, 12, 14, -74, 127, -32, -45, -35, -26, -99, -17, 26, -9, -61, 7, 10, -62, -41, -40, -44, -51, 23, -25, -49, -77, -1, -13, -68, -84, -35, 28, 31, -73, 29, -35, -68, -4, -79, -15, -37, 31, -3, -53, -93, -43, -100, 53, 76, 51, -64, 63, -50, 13, -64, 28, 127, -79, -44, -18, -20, -50, -38, 4, -22, -21, -16, -37, -41, 127, -28, -29, -2, -30, 21, 15, -21, -25, -11, -45, -55, -24, 0, -40, 20, -25, 4, 47, -38, 42, -12, -93, 17, -2, -48, 56, -2, 13, -90, 21, 6, 82, -92, -38, 41, 87, -18, 54, 105, -67, -29, -101, -38, -127, -48, -14, -123, 62, 21, -9, 32, 119, -59, -55, -35, -9, -14, -78, -34, 110, 24, 31, -59, -1, -127, -44, -32, 33, -24, 111, -12, 84, -60, 8, -47, 1, -49, -55, -78, -23, 102, -58, -22, 127, 28, 37, 65, -51, -18, 120, 57, 79, -26, 14, 104, -8, 63, -43, -24, -34, 8, 96, -60, -31, 56, 32, 54, 15, -16, -76, -25, 17, 11, 92, 47, -79, 32, 0, 68, 20, -30, 97, -62, -2, -27, 127, -36, -114, -2, 17, -7, -68, -26, 74, -70, 109, 19, -71, 97, 12, -4, 59, -29, -89, 26, -22, -127, 0, -77, -38, -96, -94, -18, 26, 12, -46, -24, 10, 31, 5, -19, 101, -9, 28, -27, -46, 64, 21, -7, 48, -15, -56, 25, 24, -127, 41, -31, 45, -63, -60, -6, 62, 33, -26, -30, 19, -4, 12, 3, 14, -127, -25, 41, -26, -39, 25, -38, -56, -87, -1, 6, 45, 7, -74, 67, 102, 85, 50, -35, -22, 47, 10, 8, 19, 21, 24, -3, 0, -127, -49, -11, 50, -86, 2, -11, -33, -89, -83, -47, -6, -6, 101, -21, -96, 108, -27, -15, -30, 15, -31, -45, -1, 17, 2, -95, -4, -93, -39, -22, 33, 60, -8, -29, 64, -17, -70, 19, -7, 35, 51, -37, 30, -39, 27, 29, -58, -53, -3, -27, -40, -127, 53, 127, 8, -69, -28, -54, 9, -29, -24, -39, 8, -67, -93, 27, -85, 35, -58, -37, 21, -40, -5, -34, -13, -49, -44, 110, -116, -95, 68, -49, -59, -33, -8, 40, 72, -70, -31, -50, 83, -28, -67, 9, 22, -3, -53, 56, 75, 96, 5, 3, -9, -34, -127, -53, -77, -26, -71, -35, 3, -97, -1, -22, -105, 58, -70, -5, 98, 25, -82, 46, 25, 87, -127, -87, -92, -47, -71, -33, 26, 65, -4, -86, -3, -69, -23, -20, -8, 32, -21, -30, -59, -24, 20, 7, 1, -33, -25, -68, -30, 2, -127, 22, -57, -99, -38, 21, 91, -52, -33, 16, -39, -58, -86, -38, 78, -52, -91, 19, 11, 65, 3, 11, -14, -83, -25, -84, -5, 127, -31, -105, -63, -17, 53, 47, 56, 58, 44, -51, 72, 19, -59, 27, 46, -72, 62, 11, -76, 127, 47, -11, 38, 36, -86, -28, -18, -122, 62, -124, -15, -46, 11, -65, 84, 92, -2, 16, 10, -37, -56, -27, 29, -81, 55, 13, -79, 90, 9, 26, 2, -8, -105, 45, -3, 6, -9, -90, -20, -127, 35, -8, 12, 101, -7, -16, -6, -43, 44, 2, 1, -14, 127, 37, 7, 55, 48, 116, 105, -12, -47, 41, -4, 82, -21, -101, -32, -14, 2, 30, -28, -75, 32, -17, -12, 65, -43, 10, -54, -76, -46, -18, -68, -56, -73, 3, -102, -75, 42, -39, -54, -21, -10, 22, 56, -28, -52, 18, -18, 14, -25, -127, -94, -6, 106, -17, -31, -26, 42, 20, -8, 113, 5, 57, 99, 28, -61, 4, 10, 23, -106, 112, -127, -61, 124, -25, 68, -61, -6, 0, 14, 41, -89, -39, -26, -52, -98, 86, -32, 38, 127, -79, -61, -80, -27, -2, -44, -56, -52, -100, 54, 87, 20, -87, -33, -20, -29, -86, -8, -49, -127, -122, -59, -62, 114, -55, -6, 29, 40, -61, 53, -6, -68, 27, -27, 9, -89, -15, -72, 22, 4, 16, -17, -52, 67, -34, -44, -59, -71, 30, 38, 5, -18, 47, 43, 84, -4, -103, 33, -63, 63, -63, 65, -46, -33, 92, 88, 105, -55, -26, -9, 30, -127, 29, -6, 64, -78, -41, -17, -127, -16, -73, 9, -11, -36, 3, 33, 51, -35, -53, 21, 15, -23, -71, -71, 27, -39, -63, 56, 46, -21, -89, -25, -23, 57, 16, 49, -60, -6, 32, -127, -30, 50, -21, 3, 16, -102, -28, -5, -2, 6, -21, -50, -11, 28, 1, 36, -62, -44, -6, -47, 10, -19, -1, 17, 11, -6, 30, 14, 5, 30, -127, 45, 17, -8, 9, 21, -5, -59, -69, -58, 15, 2, -14, -68, -47, 9, -107, -14, 25, -61, -27, 75, -17, -44, -5, -98, 2, -1, -79, 19, 30, -39, 39, 36, -21, 57, -48, -21, -12, -20, 127, 26, -4, 8, -44, 17, 2, -2, 22, 24, 13, 16, -8, -90, -44, 31, 68, -30, -28, 97, 65, 127, -23, 62, 62, 19, 42, 17, 10, 63, -70, -35, 96, -22, 20, -34, -78, -127, -59, -47, -52, -16, 22, -70, 61, 23, -97, 60, 69, -44, 25, -17, 17, -27, -27, -10, 14, -17, -22, -12, 32, -42, -82, -20, -17, 28, -32, -18, -25, 25, 31, -27, -5, 12, 11, -40, 28, 127, -19, -29, -23, -28, -32, -17, -12, -4, 19, -1, 17, -16, -21, -52, -35, 4, -41, -16, 54, 24, -16, 3, 9, -9, -48, -21, 1, -9, 3, 127, -32, -106, 57, -11, 30, -43, -52, 6, -32, 9, -27, -127, -4, 45, 3, -14, 3, 27, 21, -10, 3, -3, -32, -41, 57, -17, -21, -57, 41, -66, 25, 44, -64, 44, -76, -27, -9, 41, 2, -122, 36, 112, -56, 71, -8, 0, 127, 50, 65, -32, 38, -65, 44, 10, -18, 9, -18, 67, -101, -13, 54, 79, -86, -53, -69, 37, 13, -115, -17, 34, -38, -8, 127, -31, -4, 35, -68, 70, -4, -127, 67, -22, -10, 98, 37, -73, -117, -17, 42, -24, -18, -69, -38, 19, -100, -3, -44, 15, -29, -9, 2, -13, -29, -75, -41, 30, -18, 15, -38, 65, 5, -23, -21, -18, -28, -28, -11, 1, 11, 0, -42, 127, -24, -39, -35, 46, -21, -34, 15, -37, 60, -28, 98, 127, 5, -70, 86, 24, 20, -84, -41, -22, 37, -60, 29, -21, 20, 9, 17, -31, 2, -26, -12, 4, -20, -49, -54, 5, -15, -47, 47, 23, -9, -54, -15, -6, -86, -31, -127, -4, -75, -21, 4, -65, -40, -21, 22, -63, -72, 19, -51, -57, -32, 4, 53, 29, -127, 8, -15, -9, -17, 22, 13, -25, -45, 28, 40, -39, 7, -31, -15, -32, -73, 34, -11, -15, -53, 54, -29, 30, -39, -28, 31, -127, -13, -8, -52, -22, 0, -28, 102, 1, -31, -8, -44, 37, 4, -16, -6, 34, 11, -33, 11, -14, -16, 2, -28, 36, -31, 55, -92, -13, 15, 50, -32, 49, -14, -90, 48, -12, -5, -22, -43, -27, -62, -7, 127, 27, -36, 102, -12, 32, -55, 26, -47, -78, 41, -56, -105, -38, -43, 58, 43, -127, 33, -53, -32, 123, -66, 40, -61, -10, -78, 30, -25, -93, -70, -42, 16, 15, -12, -80, -37, 38, -43, 40, -21, -3, 117, 24, -77, 24, -3, -127, 49, 14, 0, 2, -13, 22, -107, 38, 5, -45, 91, -46, -35, -39, -65, 60, 45, 36, 34, -24, 78, -27, 37, 88, 59, 5, 62, 87, -38, 15, 4, 4, -114, -70, -113, 0, 59, 41, 127, 24, 26, 42, -12, 0, -25, 55, -42, 18, 21, -90, 58, 29, -39, 28, 36, -57, 17, -25, 35, 0, -6, -72, -48, 14, 25, 47, 127, -96, -97, 47, -66, -127, 71, 28, -38, 51, 52, -95, 96, -3, 42, -30, 25, -64, -54, -38, -16, 56, -27, 21, -49, 29, -49, 66, -5, -56, -83, -54, 84, -58, -52, 65, -88, 29, 82, -41, 39, 26, 43, 97, -59, -57, -2, 21, -4, -15, 11, -121, -36, -4, -127, 50, 81, -43, -35, 37, -39, -118, -60, 14, -78, 89, 81, -71, 110, -52, -42, -16, -24, -80, -33, -46, -64, -30, -5, -127, 100, 9, 37, 10, -72, -61, 47, 13, -34, -48, -43, 39, -41, 127, 35, -3, 27, 31, -2, 52, -32, -96, 18, -45, 4, -31, 4, -45, -17, -28, -107, 41, 35, 5, -15, -27, 22, -18, -32, 38, -22, -63, -2, -1, -10, -26, -22, -4, -1, -25, 37, -15, 2, 1, -43, -16, -23, -3, 127, 6, -14, 15, 5, -6, -2, 48, -6, -5, 15, -45, -29, -32, 64, 127, -93, 49, 11, 36, -18, 31, 46, -110, -17, 34, -29, 63, -85, 10, 27, -1, -22, -18, 10, -37, -24, -60, 80, -62, -62, 29, 34, 28, -35, 19, -54, -33, -17, 6, 36, -63, -33, 127, -55, -1, -5, 59, 15, 43, -7, 42, -33, -4, -25, 0, -95, -19, -18, -25, -15, 93, -10, -14, -3, -76, -50, 84, -80, -31, -71, -42, 26, 21, -41, -86, 127, -9, -24, 64, -29, 57, -3, -77, -16, -59, 75, 4, -58, 2, 26, 20, -17, -59, -23, -19, -43, 13, 127, 20, 75, -78, -61, -67, 10, -109, -12, -18, 78, -119, 44, -5, -56, -26, -41, -48, -21, 27, -46, -78, -76, -35, -103, -15, 6, 77, -112, 13, -6, -94, -79, -32, -113, -30, -127, 16, 32, -89, -127, 6, -50, 17, -17, 122, -45, 38, -72, 36, -38, -17, -25, -52, -107, 14, -31, 106, 59, 7, -10, 41, -11, -102, -10, -35, -37, 19, 14, -19, 22, -36, -30, 26, -51, 48, 37, 32, -43, 46, -23, -13, 33, -14, -127, -19, 57, -26, -41, 19, 37, 29, -1, 18, -27, -125, -78, 63, -11, -107, 21, -113, 41, -43, 5, -28, -22, 6, 6, -25, 55, -17, -123, -86, 11, 48, 85, 62, -127, -20, 67, 2, -60, 74, -24, 26, 57, 23, 32, -5, -6, 127, -14, 73, -20, -45, -35, 34, -8, 7, -60, 42, 1, -93, 12, 119, -73, 51, -46, 41, -89, -58, -19, -120, -117, -31, -73, -127, -29, -12, -72, 20, -34, -27, 56, 22, 114, -10, -66, -91, 24, -10, 42, 46, 20, 117, -40, 12, -101, 47, 17, -127, -8, 81, 109, 62, 57, 2, -71, -40, -75, -23, -42, -31, 8, -62, 28, -29, 61, -18, -58, -123, -17, 10, 19, -13, 51, -49, -112, 50, 26, 81, 42, -34, 5, 95, 55, 127, -35, -73, -10, -8, 72, -51, -31, -28, 15, 73, 45, 19, -30, -13, -29, 83, 34, -65, -127, -13, -7, -47, -27, -18, 19, 59, 18, 92, -41, -107, 57, 24, 111, -64, -70, 24, -3, -55, -7, 39, -77, -22, 38, 77, 4, -14, -41, -25, -20, 3, 15, -13, -28, 4, -103, -114, 5, -37, -127, 69, -66, -23, -112, 28, -116, -45, -15, 10, 0, -52, 21, -111, -69, -127, -55, 127, 5, -9, -35, -51, 111, -40, -19, 2, 8, -106, 36, -73, -19, 41, -71, -18, -69, -21, -21, 3, 20, -27, 18, -15, -50, 55, 12, -51, -13, -53, -21, 92, 16, -21, 52, -54, -45, -85, -54, -43, -68, -13, 47, -127, -38, -38, -12, 64, -61, -43, -17, -34, -17, -52, -104, 11, -38, 58, -21, -10, -59, -44, -81, -1, -80, -71, 63, -40, 33, -5, -16, 71, 30, -26, 127, 1, 18, 27, 37, -8, -92, -55, -41, 41, 42, -59, -61, -27, -8, 49, -23, -10, -23, -31, -23, 20, 76, 33, -76, 38, -62, 2, -8, 12, 127, 13, -43, 40, -68, -28, -78, 15, -43, 127, -75, -75, 38, 43, -17, 5, -8, -47, 16, -22, 53, -39, 3, -50, 3, -102, 0, -27, 33, 16, 0, -1, -41, -45, 44, 22, -9, -95, 45, 38, 123, 67, -6, 60, 23, 57, -127, 59, 30, 0, 5, -14, -28, 51, -1, -10, -19, -15, -39, 26, -3, 0, 66, 104, -67, 42, 79, -23, -3, 76, -114, 73, -100, -47, 127, -5, 71, -17, 2, -64, -14, 32, 77, 5, -30, -22, -23, 6, -99, 36, 38, -14, -48, -127, 12, -34, -55, -94, 22, -40, 23, -63, -99, -52, 23, -33, 5, 40, -117, 25, -77, 63, -102, -18, 6, -79, 101, -63, -63, 127, 2, -3, -25, -23, 51, 75, -30, 12, 18, -33, 77, 44, -32, 60, -15, 43, -4, 30, 100, 43, -46, 109, -44, 39, -25, -110, 7, -30, -2, 0, -46, 97, -5, 38, 57, 10, -65, 20, -127, 4, 0, -43, -49, 1, -28, 0, -14, -6, -40, -37, -40, -19, -36, 58, -50, 67, -79, -127, 44, 33, 77, 110, 14, -49, -59, -6, 82, -18, 19, 0, -23, -36, -97, 100, -29, 37, -32, -100, -82, 50, -53, 46, -2, 55, -18, -32, 44, 67, -3, -42, -18, 127, -52, -123, 48, 94, -7, -94, -28, 23, 20, -16, 35, -1, -53, -17, -2, 84, -78, 30, -69, -6, 6, -76, -62, -31, 43, -53, -66, 46, -75, -57, -4, -29, -79, -58, -127, -34, -5, 14, -18, 14, -37, -37, -37, -2, -7, -55, 31, 94, -33, -127, -10, -86, 35, 90, 36, -63, 37, -108, -11, -19, 94, 18, 21, -99, -17, 11, -27, -70, 48, -60, 58, 38, -5, 9, 42, 24, -27, 59, 19, 52, 104, 20, -94, 33, -76, 37, -53, -51, -2, -5, 44, 127, 52, -38, 19, -38, -92, 53, 59, 41, 66, -46, 4, -3, -41, 19, -14, 41, -2, 43, 7, 7, -5, -24, -5, -29, -12, 127, -42, -93, 57, -42, 2, -75, -29, 2, -45, -6, -23, 15, 42, 53, -58, -84, -43, -12, 86, 26, -1, 12, 10, -49, 32, 15, 11, 0, -8, -1, -84, 11, 127, 35, -9, -41, -62, 51, 66, -55, -36, 66, -21, -49, -29, 41, -31, 20, -94, 7, -14, -127, 8, -7, 4, 4, 24, -103, 42, -66, -25, 29, -90, -52, -8, 28, -74, -42, 15, -92, -4, -97, -101, 95, 45, -56, -37, -2, 59, 24, -116, -23, 3, -72, -74, 43, 23, 12, -53, -80, 62, 127, 21, -61, -22, 59, 21, -22, 53, -121, -21, -112, -118, 45, -127, 52, -8, -82, 8, -6, 68, -69, 104, -60, 84, 108, 16, 112, -65, -21, 20, 113, 50, 42, -18, -3, -127, -1, 34, 30, -3, 17, 51, -63, -66, 75, 10, -81, -53, 7, 71, -19, -87, -33, -31, -29, -13, 65, -75, 19, 52, -20, 16, -56, -33, -53, -56, -14, -53, -41, -43, -44, -41, 1, -79, 32, 1, 63, -40, 127, -45, 11, -45, -21, -33, -88, -23, 4, 15, -72, -4, 15, 8, -26, -57, -49, -20, -48, 127, -52, -24, -16, 40, -3, -6, -13, -35, -47, -39, 19, -43, -33, -76, 23, -15, -41, 31, 95, -5, -1, -35, 13, -25, 31, -60, -5, 110, 92, -53, 27, -52, -71, 127, -43, -62, -27, -25, 75, -120, 48, -18, -16, -20, -28, 49, 121, -31, 16, 13, -13, -53, 23, -67, 38, -51, 117, 1, -24, 68, 18, -115, -72, 35, -35, -110, -103, -32, 44, -127, -71, 9, -42, 27, 23, 31, -9, 22, 13, -42, -4, -127, 5, -7, 47, -21, 45, -2, 7, 21, 68, 8, 62, -50, -18, 0, -40, 13, 73, -6, -17, -1, 1, -42, 42, -23, 64, -7, -25, 29, 10, -54, 127, -49, -37, -14, -30, 10, 15, 25, -19, 41, -76, 17, -17, 20, -11, -61, 50, 26, -54, 5, 54, 34, -30, -13, 9, 40, -127, 6, 26, 50, -111, -39, -49, 53, 29, -35, 87, -94, 104, 76, 38, 0, -17, -78, -34, 7, -36, -45, 54, -6, 47, -32, -2, 29, 0, -56, -9, -4, -88, 50, -6, -11, -9, -102, -44, -6, 29, 127, 3, -71, 1, 17, -20, 32, -58, -19, -36, 12, -38, -63, -15, 51, 67, 24, 84, -47, -127, 15, 31, 57, 45, -51, -46, -87, 110, 113, -18, 55, 21, -89, 31, -25, -7, -90, 71, 4, 58, -62, -59, 110, 2, -60, -7, -75, -72, 127, 5, 27, 36, 15, 56, 68, -92, 41, 59, -68, -1, 1, 11, 17, 37, -43, 3, -62, 127, -29, -127, 11, 58, 15, 57, -89, -47, 40, 62, 20, -27, -77, -29, -22, -41, -56, -32, -32, 16, -18, 0, 16, 75, -30, -127, -53, -6, 94, 29, -110, 1, 18, 39, -51, -5, -4, -27, -57, -55, 0, -10, -19, -34, 16, -97, -101, -28, 0, -13, 3, -7, 124, 116, -40, 15, -34, -60, -59, -38, -22, 0, 18, -32, 16, -18, 40, -63, 29, -38, -2, -5, 5, -54, -14, 20, -22, -127, -72, -54, -89, -89, 44, 43, -4, 54, -86, -32, -71, 106, 23, 3, -102, -46, -68, -89, -106, -127, -31, 88, -36, -67, 59, 62, -37, -19, -5, -50, -64, -127, -23, 80, -21, 65, 32, -46, 36, -69, -50, -104, -28, -64, -1, -84, -13, 37, -47, -14, -2, -36, 10, -37, -72, -74, -40, 84, 34, 10, -12, -47, 1, 49, 64, -10, -42, 0, 44, -19, -90, 69, -22, -27, 127, 67, 48, 5, -1, -10, -23, 6, 63, -9, 61, 34, -52, -19, -49, 23, 33, -16, 31, 12, 83, -2, -123, 26, -42, 8, -81, -56, 49, -127, 6, -37, 35, -18, -88, -102, 17, 29, 61, 19, 72, 73, -42, -17, -66, -11, 4, 18, -53, -21, -7, -31, -27, 4, -9, -79, -27, -29, -123, -24, 8, -61, -10, -49, 127, -57, -115, -60, -22, 9, -72, -85, 8, -5, -14, 59, 103, 13, -89, -6, -127, 63, 41, -46, -3, -52, -15, -8, -127, 69, -50, -80, -61, -1, 11, -35, -53, -27, -7, -28, -7, 10, -41, 13, -2, -63, -31, -127, -35, -21, 16, 55, 32, -35, 15, -33, -12, -8, -106, -91, -55, -44, -5, -24, 50, 79, 127, 9, 41, -21, 8, -38, -27, -94, 37, 35, -24, -87, 71, 103, 69, 10, 43, 71, 5, 39, -9, -81, 15, -16, -46, -79, -65, -94, -61, 63, -86, -66, 17, -26, 31, 17, 0, -3, -20, 4, 57, -39, -7, -103, 71, -18, 64, 13, -49, -30, 24, -16, -127, -56, -56, 110, -42, 26, -101, 46, -85, 48, 44, -19, -36, -2, 18, -3, 80, -9, 127, 69, -12, -116, -39, 55, 103, 16, -63, -30, -13, 67, -127, -75, -35, -18, -25, -55, -1, -38, -47, -2, -73, 90, 50, -18, -27, -77, -6, -32, -65, -11, 41, 102, 11, -101, 32, 3, 55, -2, 16, -11, 4, -18, -25, -21, 1, -19, 15, -23, 33, 5, -12, -28, -40, -94, -23, -4, -127, -20, -11, 127, 27, 10, 1, -46, 8, -98, -74, 101, -51, 59, 46, -97, 81, 43, -44, 19, 16, -78, 41, 36, 8, -30, -127, -19, -46, 0, -21, 11, 93, -33, -20, 15, -98, 12, 37, 42, -23, -8, -63, -19, -33, -14, -10, -4, 5, -21, -22, 38, 29, -45, -18, -3, -63, -34, -2, -39, 19, -31, 67, -32, -127, -80, -36, 18, -65, 31, -56, -43, 17, 23, 41, 15, -37, -127, 61, 8, -22, -16, 17, 20, -12, -94, -2, -24, 9, -29, 4, 21, -24, 55, -51, -8, 27, -9, -64, -13, -13, 16, 46, -20, -50, -39, 56, 25, 77, -12, -127, -73, -50, 85, 16, -85, 74, -34, -59, 16, -16, -78, -64, -51, -16, -15, -82, -86, 39, 88, -17, 42, -46, -55, 93, 0, -32, 14, -127, 117, 45, -36, 21, -47, 38, 42, -23, -8, -4, 8, -127, -7, -16, -113, -2, 14, 101, 38, 32, 106, -27, -83, 94, -50, 24, -28, -28, -54, 1, 77, 8, -126, 4, 15, -87, 27, -49, -69, -3, 66, 25, -9, -40, -75, 67, 0, -29, 14, -5, -84, 37, -11, -66, 46, -43, -61, -76, 11, 127, 9, 44, -35, 6, 27, 46, -55, -11, 62, -52, 34, -68, -85, 87, 22, -42, -18, -33, -67, 59, 4, 99, -35, -89, -23, -127, -7, 10, -6, 51, -29, -39, 39, 46, -106, -5, 85, 7, -42, -54, 35, 127, 13, 2, 29, 27, -45, 43, -45, -75, 30, -78, -32, -91, -7, -45, -20, 61, -38, 26, -20, -21, -8, 71, 43, 27, 76, -35, 40, -24, 41, 39, 43, 0, -40, -25, 127, 63, 4, 39, -10, -9, -27, 25, 70, -16, -4, 27, 126, -61, -127, -20, 15, 5, 77, -29, -47, -10, 20, -38, 8, 12, -65, 113, -40, -15, 26, -95, 0, -28, -11, 64, 38, 49, -7, -9, 27, -65, -2, 10, 61, -24, -56, -49, 31, 9, -42, -70, 66, -6, -3, 13, 23, 37, -3, -3, -29, -74, -6, -21, -12, 14, -21, 62, -35, -127, -43, -18, 111, 1, 0, -40, -64, -64, -102, -60, 127, -69, -70, 20, 6, 23, -75, -56, -69, -73, 22, -43, -60, -4, 9, -86, -14, -95, -127, 30, 30, -62, 4, 41, 25, 42, -53, -92, -41, -43, -55, -27, 44, -50, 25, -40, 38, 52, 22, -15, 23, -88, -93, -21, -88, -49, 44, -31, 22, 0, -12, 29, 23, -23, -38, -46, -69, -24, -75, -48, -12, -29, -40, -35, 6, 20, -26, -31, -19, -14, 127, 1, -42, -34, -62, -39, 3, 37, -127, -116, 56, 51, 66, -20, 17, -76, -10, -64, 10, 55, -54, 66, 2, -23, 53, -73, -2, -35, 18, -3, 78, 48, -64, 1, 88, 9, -51, 42, 35, -69, -15, 51, 120, -25, -36, -26, 27, 109, -31, -50, -71, -44, 101, -127, 85, -23, 25, 4, 25, -67, -10, -13, 32, -39, 25, 36, 41, 61, -38, 20, 44, -57, -127, 49, 104, 76, -31, -65, 68, -4, -41, 10, 45, 58, -84, -12, -55, 61, 127, -6, -65, -14, -21, -49, -33, -24, -4, -31, -28, -44, 60, 10, -34, -91, -10, 33, -33, -92, -33, -20, -6, -65, -86, -54, -57, 55, 127, -37, 7, 19, -16, 2, 89, -51, -50, -77, 9, -32, -81, -72, -97, 49, -18, 36, 22, 43, -86, 16, -14, -56, -83, -63, -20, 31, 114, 32, -36, 43, -78, -9, 5, -127, -40, 23, -21, -97, 41, -56, 10, -3, -15, 13, 16, -64, 2, -22, 55, 5, -40, -4, -13, 115, 4, -56, 30, -3, -112, -33, 5, -68, 12, 78, 72, -23, -38, 62, -1, 88, -68, -13, 53, -70, 127, -21, -2, -122, -68, -38, 2, -78, 19, 9, -45, -5, -16, -31, 21, -68, 68, 7, 0, -41, 21, -7, 2, -7, -7, -127, 2, 2, -4, -22, 21, 75, 12, -22, 45, 5, -36, -28, 16, 1, 52, 5, 6, -62, -18, -20, -80, -51, 9, -32, -3, -30, -9, -38, -18, -1, 127, 5, -29, -35, -19, -40, -7, -8, 127, -27, -57, 13, 22, -37, -20, -33, -1, 55, -104, -33, 106, -125, 50, 37, 1, 16, -86, -95, 44, 1, 84, -20, -93, -59, 30, -42, 44, -49, 116, -5, 65, -51, -127, -32, 45, 63, 58, 35, 34, -40, 113, 34, 83, -23, -46, -62, -37, 84, 79, 31, -21, 12, 33, -59, -80, -5, 22, -46, -68, 43, 32, 97, 43, 57, 55, -46, -77, -66, -13, 125, 20, -78, -100, -89, 127, 101, 13, 23, -58, -47, 32, -35, -38, -127, 19, -45, 66, 18, -18, 57, 42, 5, -45, -62, -106, 31, 6, 32, -77, -58, -15, 31, -21, 37, 47, -27, 20, -57, -16, -40, -85, -35, 2, -67, -15, -8, -10, -79, -52, -26, -34, -22, -74, 35, -4, 66, 48, -70, 25, -53, -22, 1, 46, -127, 0, -31, -22, -1, -74, 25, -97, -34, -17, 78, -22, 43, 107, 32, 10, 12, 27, 56, -10, 9, -32, -23, 117, 75, 86, 62, 62, -127, -6, 4, -48, 7, -15, -46, 89, 2, 51, -25, -47, -43, 51, -102, 1, 5, -66, 123, -42, 29, 19, -83, 42, -53, 10, 127, 40, 9, 8, -24, 6, -16, 17, -54, 127, -77, -15, -57, 30, -44, 52, -70, 89, -65, -120, 80, 38, 37, -75, 16, -91, 18, 7, -75, 95, 45, -6, -30, 22, -34, -55, 11, -56, 1, -77, -89, 58, -4, 23, 8, -15, -60, 9, -106, -5, -1, 127, -89, 96, 53, -46, -12, 47, -23, -57, -56, -17, -35, -3, -15, 4, -81, 9, 51, -37, -84, -46, 35, -60, 39, -48, 29, -16, 103, 3, -42, -23, 49, -17, 39, 127, -94, -1, -53, -56, -54, 41, -56, -29, 27, -30, 29, -99, 14, 112, 15, 5, 83, -120, 34, -51, 22, 4, -125, -127, -49, 73, 4, 44, 100, 59, -119, 38, -123, 47, 40, -81, 23, 126, -115, 4, 23, 76, 57, 23, -31, -73, 62, 19, 127, -76, -14, 24, -14, 23, 93, -10, 46, -22, 57, 58, 40, -75, -26, -3, -22, -2, -29, 56, 42, -6, -64, 38, -17, -106, -28, -24, 18, -1, -80, -41, 17, -3, -36, -110, 127, 8, 33, 18, -61, 121, 27, -81, -58, -66, -18, 27, -67, -28, 12, 12, -47, 29, -29, -40, -11, -127, 36, -92, -19, -63, 17, -87, -63, 3, -35, -5, 111, 23, -32, -127, -43, -56, 16, -32, -117, 2, 23, 57, -14, -105, -110, 23, -60, -16, 20, 32, -21, 52, -2, -24, -34, 31, -3, 1, -35, -44, -21, 92, -32, -13, -9, -49, 23, 27, -73, 34, -54, -22, -49, 0, 32, -9, 10, -18, -41, -36, -21, -29, -127, -43, -17, 32, -26, -18, -39, -88, -127, 0, -43, -68, -38, 36, 76, 54, -5, -68, 55, -10, 30, 54, -118, -72, -47, 8, 30, -4, -58, 45, -30, 5, -93, -28, -41, 67, 64, -67, -17, 55, -43, 28, 31, 61, -49, -127, -24, 11, 70, 8, 115, 101, 43, 58, -34, 43, 26, 3, 23, 38, 12, -12, -1, 40, -33, 127, 10, -45, 82, 1, -7, 53, 7, -67, 30, -8, -67, 62, 6, 11, -38, -34, -6, 40, 25, -44, -18, 16, 28, 12, 8, 6, 15, -8, 57, -2, -18, 9, 19, -18, -26, 4, -26, -127, -31, -9, 15, 3, 29, 2, 19, 8, -66, 14, 0, 58, 20, 22, -13, 1, 21, 44, -118, 81, -33, -40, 21, 73, -41, 5, -123, 22, 96, -72, -4, 41, -127, 36, -42, -44, -70, 43, -63, 12, 16, -20, -23, -12, -12, -92, 12, -22, 5, -3, 12, 45, 23, -15, 59, 15, 14, 0, -27, 6, 4, -127, -16, 5, -15, 26, 12, -9, 0, 11, 28, -8, -11, 19, -27, 40, -74, 33, -27, 19, -44, 57, -10, 31, 35, 33, -127, 27, 26, -17, -10, -15, -27, 1, -11, 10, 12, -101, -92, 1, -127, 19, -121, -20, -8, -58, -41, -62, -39, 10, -41, -79, -44, 50, -38, 24, -61, -38, -22, 38, 78, 45, -45, 60, -78, -54, -91, -61, -82, -13, 51, -97, -32, -3, 85, 1, -50, -104, -30, 21, 71, -36, 72, -5, 41, 0, 53, 30, -62, -127, -1, -35, -96, -52, -107, -30, 48, -31, 127, 17, 1, -8, 14, -29, -27, -31, 75, 40, -66, -82, 10, -18, 67, 16, 21, -48, -17, -93, -68, -22, -53, -4, 12, -50, 21, -6, 16, -2, -37, 25, -14, -77, -44, 71, -38, 9, -48, 3, -127, -7, 66, 7, -31, 53, -22, 12, -16, 30, -22, 1, 22, -18, -23, -77, -45, 25, -102, -5, -24, 43, -72, 127, -62, 17, -5, -37, -40, 43, 50, -16, 70, 17, -59, 89, 15, 24, 33, -5, -99, 65, -75, 49, -70, -3, 20, -1, 63, -1, -118, -55, 110, 45, -20, -49, -127, -16, -32, -78, 23, -17, 30, -48, -39, -3, -60, -47, -34, -67, 5, -65, -127, -14, 34, 45, -39, 11, -9, 46, -18, 13, -61, 23, -22, 12, -77, -2, -19, 35, 11, 95, -13, 30, -43, 67, -107, 109, 3, 46, -36, -67, 12, 116, -34, 111, -45, -127, 46, 8, 15, -3, -13, -99, 5, -27, -47, 65, -105, -35, -20, -54, -9, 19, -12, -75, 92, -40, -63, 99, -79, -117, -71, 56, -127, -77, -93, 39, 25, -32, 42, -24, -18, -90, 14, 20, -60, 76, -24, -31, 59, -37, -119, 85, -58, -67, -14, 47, -32, 61, 48, -3, -64, 127, 27, 29, -50, 127, 76, 5, -11, -62, -42, 108, 45, -40, -2, 40, -102, 118, -22, -67, -25, -43, -10, 9, -127, -55, 9, 37, -3, 27, -54, -2, 0, -49, -78, -42, -2, 56, -30, 4, -98, 18, 46, -17, 49, 26, -41, -83, 30, -63, -127, 30, -70, 18, -103, -78, -91, 46, -83, 28, -26, -19, 12, 42, -77, -1, -28, -117, -36, -73, -110, 40, 15, -19, 72, -62, -9, 27, -4, 27, -90, 0, 9, 19, 76, 33, 42, -22, -71, -23, -98, 25, 92, 33, -24, 15, 127, 7, 6, 33, -25, -69, -30, 40, -14, 58, 5, -64, 50, -4, 17, 44, -26, -45, 27, 0, -18, -10, 33, -24, -19, -17, -127, 11, 8, -12, 20, 12, 20, -27, -47, 47, -24, 14, 6, 32, 75, 22, 37, 44, -10, -28, 18, -50, -50, 47, -44, -21, -47, -32, 127, 3, -48, -13, -41, 15, 79, 6, -20, 8, -82, -11, -19, 49, -23, 10, -25, 20, -33, -100, 10, -26, 2, 127, -22, -99, 92, -23, -4, -70, -11, -31, -9, 13, -13, -72, -41, 16, -69, 43, -63, -30, 64, 24, 20, 12, 10, -89, -14, -4, -9, 43, -127, -22, -120, -46, -26, 26, 95, -42, -5, 68, -71, -62, -67, -20, -57, 9, -47, -69, 14, 30, 16, 21, 5, -32, -35, 40, 127, -12, 66, -65, -1, -24, 23, -34, -1, -27, 30, -28, -58, 20, -4, -11, 26, -21, -25, -30, -95, -13, -42, -9, 5, -37, 11, -15, -45, 57, -41, -21, -29, -29, -34, -127, -8, 4, -38, -4, 22, -21, -38, 86, -53, 78, 2, -2, -24, 55, 11, 70, 17, -100, 71, -25, 17, -5, -61, -89, -48, 32, 127, 40, 2, -22, 12, 64, -13, -12, -9, -1, -22, -14, 31, 2, -12, 26, 2, 10, 6, -66, 42, 5, -2, -9, -60, -57, -29, 43, 127, 1, -13, 65, -10, -5, -46, 65, 11, 90, 24, -116, -18, -127, -2, 11, -30, -13, -75, -2, -17, -5, -38, -11, -52, 56, 118, -34, -32, 32, 24, 46, -24, 12, -9, -12, 28, -23, 3, -58, -102, -50, -127, 51, -4, -5, -35, -44, 49, 62, -89, -28, -12, 63, 7, -1, 37, -77, 12, 18, 115, 68, -88, -17, 38, 14, 26, 14, -14, -19, -24, 29, 127, 114, -76, -62, -48, 10, -3, -73, -48, -84, 8, -25, 17, 52, 38, 64, 43, 49, -26, 127, -59, -34, -3, -8, -35, -14, 46, -13, -43, -48, -28, 22, 46, 2, -84, 5, -38, -18, -5, 25, -3, 9, -14, -30, -56, -5, -102, 15, -127, -58, -4, -18, 38, 14, -4, 37, 42, -14, 0, -16, -12, -3, -11, 13, -10, 11, -11, -7, -1, 18, -7, -15, -3, 50, 69, 30, -74, 48, -15, 99, -126, -61, -21, 66, -41, 120, -102, -105, 43, 5, 37, -51, 43, 30, -117, -127, -122, -39, 48, 7, -48, 16, 15, -109, 10, 78, -18, -62, 54, -2, -2, -29, 107, 40, -74, -82, 78, -52, 61, 45, 22, -104, 23, 6, -45, 1, -127, -87, -41, -12, -51, -42, -95, -33, -92, -47, -48, -36, 12, -42, -27, -17, -6, -34, -28, 106, 8, -17, -29, -25, -97, -48, -18, -34, 42, -31, 15, 127, -79, 39, -21, 8, -47, -21, -10, 53, 11, 24, 45, 40, -35, -69, -46, 49, 69, -23, -34, 48, -4, 41, 127, 56, 20, 46, 48, 104, 24, -50, 18, 22, -28, 77, 90, 74, 75, -1, 4, 107, 12, -90, 41, 15, 18, -39, -127, 4, -82, 25, 49, -21, -81, -39, -21, 80, 20, -6, -41, 47, 23, -7, 27, -4, 21, -13, 4, -24, -39, -24, 11, 21, 34, 7, -20, 28, -32, 58, 48, 8, -127, -7, 0, 8, -27, -61, -9, -17, -33, 25, -14, -24, -22, -9, -23, -12, -21, -49, -24, 1, 26, -22, -28, -36, -17, -26, -1, -11, 127, -36, 2, -25, -44, -122, -3, -26, -52, -8, -44, -126, -12, 39, 77, -70, 41, -111, -41, -6, -6, -12, -63, 91, -110, 30, 48, 50, -58, -42, 5, 52, -127, 7, -16, 80, 40, 108, 1, 0, 0, 12, -5, 105, -91, -70, -95, 73, -10, -81, 75, -9, -23, -47, 114, 74, -63, -38, -12, 127, -35, -62, 2, 110, -38, 111, -81, 0, 50, -20, -2, 56, 5, -127, 58, -74, 21, 76, -23, 27, 24, -9, 10, 23, -79, -46, 33, -60, -27, -16, -41, -8, 15, -8, 22, -78, -50, -42, 24, 59, 22, 21, -28, -108, 6, -24, 5, 9, 31, -40, 10, -44, -27, 65, -87, 127, -8, -127, 12, 6, -34, 17, 57, -67, 79, -34, -43, -36, -74, 10, -30, 25, -65, 20, 14, -53, -33, -26, 3, 7, -103, -59, -9, -2, -66, 53, -59, -48, -78, 1, 7, -70, 16, -127, 7, -62, -9, -75, -5, 52, -29, 14, 110, -50, -96, -24, -61, -68, 14, 35, 46, 14, -79, -5, -59, 84, -1, -113, -52, 14, -37, 127, -63, 87, -3, 20, -103, 20, 18, -53, -81, 33, -21, 52, -88, 54, 34, -9, -59, 31, -77, -34, -69, 86, -27, 46, -84, -84, 70, 127, -67, 9, 15, -120, 4, 49, 100, -18, -57, -60, -76, 18, -54, 95, 85, -11, -10, 17, -83, -106, 20, -81, 4, 34, -42, 79, -59, 42, -13, -38, -98, 7, -45, -110, -68, -36, -42, 25, 23, 19, -127, 47, 103, -53, -45, -52, 8, -116, -5, -41, -15, -34, 22, 23, 5, -3, 32, 5, 47, -30, -71, -9, 127, 68, -43, -41, -55, 54, -15, -22, -95, 2, -29, -25, -16, 47, -77, 0, 42, -71, -25, -48, -44, 34, -3, 49, 4, -127, -26, 2, 26, -44, -52, -99, -27, -51, -13, 5, -77, -88, -7, 16, -57, -37, -5, -2, 14, -43, -16, 19, 47, 74, 68, 26, 13, -70, -34, 15, 127, -32, -19, -57, -21, 78, 32, 16, -31, -6, -16, 21, -8, -94, -47, 13, 60, -27, -15, 45, 12, 77, 118, -26, -125, -11, -121, -25, 23, -9, -127, -16, -31, -17, -4, 78, 69, -112, -44, 76, 0, -55, -3, -9, 43, -38, 28, 10, -20, -26, -12, -59, -52, 34, -51, 57, 3, 19, -84, 14, 42, -17, -4, 127, 13, 29, -52, 23, 32, -18, 24, -43, 29, -127, -14, -9, -5, -35, 60, 20, 3, 5, -69, -4, -33, -28, 84, 38, 31, 127, -45, -11, 39, 45, 34, -6, -39, -69, -94, 93, -63, -61, 62, -35, 23, 127, -56, 37, 3, -48, 16, 36, -27, -4, -59, -12, -76, -50, -12, 1, -47, 11, -9, 64, -54, 18, -42, 29, 9, 127, 16, 2, -26, -6, -4, 23, 3, -10, 21, 13, -86, 17, 35, -3, 31, -25, -107, 9, -28, -1, 7, 13, -41, 34, -8, 39, 27, 123, 15, -14, -116, 70, 5, 127, -19, -10, -3, 92, -2, -43, -29, -18, 60, -60, -66, 66, 13, -28, -16, 3, -25, -100, 48, 66, 29, 59, -33, -56, 124, 38, -18, -17, 63, -102, 59, 30, 40, 44, -127, 47, -19, 25, 77, 58, 42, 55, -5, 12, -70, 14, 4, -56, -35, 12, -55, 39, -127, 9, -21, -60, -52, -127, -85, 50, 48, -9, -29, -66, -58, 100, 17, 69, -35, -7, -23, 45, 81, 31, 36, -25, 2, -51, -8, 24, -70, -25, 90, 83, -78, -119, 24, -3, -58, -28, -92, -38, -105, 57, -2, -28, -26, 127, -26, -61, 41, -12, 4, -46, -33, 29, -9, -8, -40, -33, -85, 0, -47, -127, 52, -22, -58, -78, 23, -55, 119, 88, 46, -70, -102, 55, 23, -24, 37, 3, -22, -23, 5, 112, 18, -76, 1, 82, 56, 62, -115, -88, -36, -12, 64, 60, -49, -9, 58, -35, 127, 10, -122, 62, 42, -1, -65, -8, -65, -10, -97, 83, 127, 18, -75, 121, 2, 71, -28, -12, -47, 9, 1, -76, 78, -4, 0, -22, -20, 40, -66, -14, -64, 8, -28, -127, 34, -55, -24, -33, -43, 41, -10, 38, 13, 37, -7, 6, -77, -1, -51, -91, 13, 4, 25, 55, -39, -37, -21, 7, 6, 9, 0, 49, 2, -60, 93, -43, 58, 1, -10, -39, 94, -78, -67, -35, -31, 90, 7, -127, -80, 17, 20, 2, -43, 41, -58, -119, 10, 53, -61, -87, 10, 32, -33, 32, 1, -127, -25, 42, 93, -55, -11, 49, 25, 7, -21, -14, -74, -104, -125, 116, -18, 37, 114, -17, -6, 19, -29, -21, -23, -64, -35, -18, 28, -35, -43, 127, -28, -51, -34, -63, 4, 49, -46, -28, -1, -22, -47, 25, -10, -27, -18, 7, 29, -26, -23, 127, -2, -40, -9, -26, -42, -24, -58, -6, -40, -11, -20, 58, -14, -20, -16, -40, 2, -33, -31, -18, -1, 2, 32, -28, 3, -21, 56, -31, -11, 60, -22, 111, 30, -9, 2, 39, 16, -6, -18, -74, 42, -35, -127, -5, -74, -5, -48, -64, 0, 25, 8, -9, 25, -34, 1, -6, 2, -8, 20, 124, 43, -10, -24, -12, 60, -15, 15, -126, 127, -47, 16, -40, -19, 26, -20, -55, 41, 10, 0, 15, 1, -32, -8, 34, 0, 4, 127, -5, 51, -35, -8, 27, -17, 30, -46, -21, -21, -5, -3, -3, -64, 10, 11, -17, 14, -34, -3, -55, -40, -11, -13, -23, -33, 66, 7, -25, -8, -40, 49, 42, 34, 127, -24, -88, 92, 52, -30, -52, -66, -34, 24, 24, 11, 46, 57, -7, 24, 2, -28, -96, 34, 66, 47, 98, -43, 18, 105, 36, 15, -3, -36, -36, 25, -56, -127, 30, -79, 20, -47, -42, 19, 50, 54, 30, 72, 20, 29, 69, 72, -62, -47, -27, 30, 12, 9, 127, 63, -73, -6, -45, -6, -43, -33, -47, 16, -41, 2, -60, 39, -40, -13, 1, -52, -50, -33, -127, -28, 44, 15, 64, -34, -63, 93, 14, -13, -62, -13, -3, -13, 58, -10, -23, -50, -100, -43, -105, 0, 73, -12, 30, 6, -22, -110, 91, -40, -69, -16, -72, -27, 54, -50, 20, -96, -55, -92, 67, -105, -22, -24, -121, 40, 37, -88, -53, 34, -33, -42, -127, -76, -24, 110, -119, -35, 102, -44, 127, -36, -33, 90, 12, -69, 41, -2, -93, 35, -49, -49, -45, -61, -77, -107, -23, 36, 7, -19, -48, 12, -41, -8, 9, -67, -108, -61, -24, -27, 63, 48, -6, -27, -26, 39, -102, 16, 127, -33, -41, 19, -7, -63, -108, -4, 31, 5, 69, -122, 45, -64, -33, -3, 0, 6, -20, -19, -33, 29, -1, -13, -1, -4, 47, -1, 1, -21, -127, -10, 67, -34, 31, 9, 12, 28, 25, -7, -15, 21, -9, -24, 3, -58, 8, 17, -79, -58, -4, -76, -32, 27, -79, 31, -3, -22, -78, 44, 16, -18, -83, 112, 89, -127, -42, -54, 1, 7, 60, 33, 75, -31, -32, -47, -14, -8, -4, 88, -15, 41, -102, -30, 40, -35, -103, -96, -10, -24, -56, -20, -100, 26, 61, 68, -74, -127, -7, -127, -31, -13, -10, 41, 0, -4, 3, 31, -7, -4, 1, -5, 12, -13, 21, -22, 20, -5, -5, 3, 13, 10, 14, 11, -68, 34, 104, 9, 31, 14, -41, 88, 71, -28, 54, 85, 52, 13, 2, -14, 23, 127, -47, -83, -29, 11, 111, -32, 27, -81, -1, 19, 65, -31, 35, 10, -54, 16, 12, -73, 49, 109, -99, -2, -24, -36, -46, 127, 16, 5, -25, -45, -6, 45, -36, 9, -9, 26, 31, 14, -3, -17, -120, -12, 25, -19, -127, 29, 14, 125, 21, 78, 37, 51, -126, -44, 0, 96, -123, -21, -52, -48, 110, -16, 20, -54, 24, -24, -53, 2, -24, 22, 8, -41, -7, -23, -20, 27, 40, -17, -13, 25, -62, -25, 32, -11, -17, -71, -29, -40, -46, 14, -75, 127, 11, 18, -63, -124, 11, -51, -3, -21, -84, -5, -16, 127, 51, 50, 26, -12, -117, -31, -17, 78, 10, -74, -71, 9, 68, -5, 39, 52, -46, -35, 41, -15, 72, 2, 10, -7, -22, -110, 53, -62, 24, -51, 67, 72, -29, 60, 29, 20, 5, -9, -25, -127, -51, 69, -16, -61, 23, 19, 12, -1, -57, -36, -9, -17, -17, -6, -52, 53, -39, 33, 15, -4, -52, -41, 8, 38, -17, -37, -14, -3, 25, -15, 34, -127, -31, 2, 16, -11, -62, -29, 26, -35, 28, 25, -115, 83, -18, -53, 23, 34, -111, 82, -8, -75, 55, -59, -57, -44, -22, 127, 21, 24, -18, -31, 20, -40, 1, -34, 64, -7, 127, -9, -93, 18, 17, 12, 97, -38, -80, 19, 15, -97, 27, -20, -82, -66, -40, 20, 14, -42, -72, 21, 10, -25, 2, -59, 37, -90, 18, -12, -76, 7, 33, 4, 127, -35, -23, -54, 15, -13, -23, -120, 42, -58, -18, 6, 17, -6, -3, 6, -26, -42, 2, 3, -30, -15, -10, -47, -5, -12, -10, -22, 37, -28, -29, -25, -10, -6, -25, -67, 9, 8, -34, -18, -33, 127, -42, -8, -20, 11, -73, -46, -77, -53, 8, 77, -32, 30, -50, -12, -31, -31, -44, -127, 31, -32, -41, -23, 21, 103, -17, 2, 31, -17, -83, 18, 32, -42, 109, 2, -47, 20, -68, 55, 71, -122, -11, -13, 18, -33, 8, -84, -71, -1, -36, 106, -30, 33, -41, 10, -32, -127, -1, -80, 17, 78, 88, 10, -32, 12, 127, 23, 40, -70, -24, -61, 22, -12, 2, -58, 3, -71, -51, 10, -4, 25, -21, -22, -21, 57, 22, -37, -42, -18, -16, -41, 38, 31, 126, -72, -89, -115, 41, 2, 70, -11, -18, -37, 30, 109, 74, 68, 36, -127, -24, 47, 4, -112, -31, -18, -18, 52, -65, -10, -46, 55, 19, -17, 12, -16, -79, -35, -30, 24, 1, -127, 20, 34, -13, -41, -91, -36, -24, -24, -33, 3, -21, -26, -47, 0, 15, -29, 1, 6, -127, -33, 3, 4, -29, -25, -60, 3, 2, 0, -9, 26, -3, -15, -1, -10, 25, 109, -4, 12, 5, -37, -10, -13, -41, -66, 37, -25, 55, -13, -15, 33, 50, -10, 127, -27, -100, 36, 7, 43, -61, -11, -29, -19, -13, 2, 6, -3, -8, -17, -4, -14, -27, -35, 53, -37, 19, 99, 51, -53, 38, -28, 0, 9, -32, -127, 26, -9, -25, -23, 26, -21, -2, 12, 26, 9, -65, 13, -55, -31, -85, 26, -7, 18, 20, 37, -55, 114, -127, -60, 94, -11, -34, -21, -54, 30, 81, -87, -40, 54, 88, -10, 73, 9, 32, -26, -53, 50, -24, 23, -56, -2, -2, -27, -76, 17, -21, 1, 8, -65, -65, 16, -93, -27, -21, -65, -8, -5, 19, -52, -30, 98, -13, 2, -127, -45, 75, -6, 12, -12, -36, 14, -14, -51, 40, 56, 88, 9, -85, -37, 0, -1, 55, -85, -51, -65, 76, 39, -14, -127, -1, -11, 39, -9, -10, 3, 13, -5, 83, -8, -69, 19, 63, 45, 80, -127, -74, -10, 14, 71, 53, -24, -25, 66, 9, 66, 20, -98, 48, 23, 14, -41, -13, -53, -96, 22, -20, -37, 44, -74, -127, -73, 12, -76, 80, -83, -86, 3, -62, 14, -53, 37, -26, -33, 41, -74, -35, -10, 38, 97, -41, -41, -36, 13, 7, -6, 14, -57, 91, 63, 104, -127, -76, -18, 34, 53, 24, -53, -16, -27, 55, -70, -85, -74, -63, -15, 65, 95, -127, -8, 16, 39, -5, -10, 41, 89, 4, -48, -7, 17, -28, -5, 14, 5, 24, 7, -2, 5, 37, 21, -28, -34, 2, -9, 1, -40, 47, -3, 48, -9, -67, -42, 67, -74, 21, 87, -27, -100, 9, 27, 1, 127, -110, -44, -9, 22, 7, 70, -4, -14, 78, 21, 4, 15, -47, 1, 127, 43, -58, -27, 17, 8, 116, -18, -10, -51, -11, 0, -25, -15, -51, -9, -50, -89, -123, 31, 34, 7, 4, -9, 52, -37, 23, 55, -69, 43, 127, -14, 25, 48, -42, 71, 59, -73, -41, -19, 41, -22, 0, 72, 12, 4, 43, -116, 44, -17, 3, 2, -5, 47, 6, 25, -63, -15, 24, -8, 10, -56, 54, 19, 28, -47, 29, -13, 31, -1, -21, -127, 0, 10, -35, -18, -16, 1, -1, -26, 30, 3, -4, -42, -27, -12, -4, -52, -80, -2, 54, -50, -19, 31, -64, -17, -3, 65, -7, -8, -52, -20, -1, -24, 38, -127, -49, -32, 5, 8, -83, -61, 22, -41, -28, -31, -70, 127, 22, -9, -9, 39, -80, 3, 37, 98, -34, -105, -28, -57, 36, -14, -36, 46, 15, 40, -30, -60, 127, -44, 1, -19, -32, -96, -58, -63, -28, -74, 10, -52, -70, -22, -9, -1, -21, -98, -59, -74, -41, 76, 42, -5, -100, -65, -5, -11, -18, -41, -15, 0, -61, -56, -22, -6, 14, -94, 28, -58, -5, -7, 6, -34, -127, -42, 37, 41, 26, -10, 41, -85, -31, 15, 42, -47, -44, -19, 53, -55, 127, 3, -24, 73, 11, 7, 61, -29, -79, 35, -24, -104, 3, -45, -18, -99, -64, 1, 25, -6, -25, -25, 21, 22, 28, -8, -19, 32, -81, -12, 70, -127, -13, -116, 8, 29, -21, 7, 48, -24, -8, 25, 101, 10, 26, -69, -60, 41, -20, -4, 75, 56, -2, -29, 14, -1, 93, -8, -51, 30, 24, 24, 79, -69, -87, 34, -36, 20, -36, 0, 6, -21, -91, -24, -127, -21, 16, -70, -23, 0, 90, -21, -37, -7, -78, 33, -43, -127, 14, -32, -34, -7, 28, 18, 46, -6, 63, -27, -4, -10, -57, 74, 0, -48, -15, -3, 23, 45, 14, -53, -17, -109, -97, -23, 23, -70, 83, -74, 37, -26, -42, 22, -15, 127, 18, -35, -99, 87, 111, -109, -24, 71, -50, -2, -74, 20, 23, 35, 17, -11, 17, -25, -90, 20, -34, -32, 1, -57, -108, 127, 24, 20, -41, -92, 18, 75, -34, -12, -34, -26, 24, 17, -57, -4, -35, -42, 38, -40, 59, 27, -49, -2, 15, -21, 41, 28, -67, 71, 8, -11, 34, -105, 1, -59, -23, 127, 52, -41, 4, -24, 37, 37, 13, 4, 89, 25, -13, -58, -14, 73, -71, -32, -41, -25, -49, -67, 26, 49, 55, -37, -64, -48, 127, -60, 21, -57, -33, -1, 22, -25, 39, 26, 21, 90, 61, -67, -56, -15, -54, 31, -16, 19, 46, -13, 50, 14, -127, 3, 3, -121, 1, -23, 47, 7, 21, 35, -34, 1, 96, -34, -13, 21, -43, -25, 28, -22, -28, 16, 13, -8, -54, 20, 20, -18, 9, -26, 111, 55, 61, 127, 0, 2, 69, 19, 6, 25, 36, -120, -4, -45, -41, 56, -55, -106, -7, 32, -70, 112, -122, -17, -6, 28, -71, -83, -71, -41, -31, -35, -106, 90, -69, 91, -127, -21, -9, -10, -1, -12, -16, -7, -10, -17, -21, -16, -9, 1, -10, -4, -7, -23, 8, -30, 3, -33, -18, 127, -14, -11, -6, 2, -2, -3, -40, -70, 37, -43, 27, -14, -20, -3, 9, -38, -13, -41, -92, 127, -11, -19, 14, -14, -68, -15, -76, -49, -19, 82, -23, -24, -40, -82, -31, -127, 95, -4, 92, -64, -106, 118, 27, -60, 7, -77, -81, 99, 8, -12, 9, 20, 57, 63, -111, 48, 60, -93, 44, 7, 21, -40, -30, -65, -62, -29, 46, -79, -58, 28, 32, -1, 33, -66, -66, 105, 41, -91, -71, -26, 82, -127, -63, 48, -28, 10, -63, 21, 72, -4, -34, -49, 57, -74, 76, -34, -113, 79, 81, -110, 112, 61, -120, 66, 24, -19, -7, -127, 16, 9, 1, -32, -75, -25, -11, -53, 6, -6, 47, 35, -69, -19, -59, 30, 4, -127, 27, 39, 0, -41, 38, -25, -31, 19, -39, 16, 29, 47, 7, -38, 32, -86, 12, 41, 3, 39, 9, 15, 28, -30, 54, 40, -29, -23, 20, 30, -30, -21, 0, 21, -42, 97, 16, -10, -69, -37, 5, 127, -42, 36, 8, 2, -44, -32, 37, 12, -29, 48, -10, -73, -45, -12, -9, -32, -5, 15, -63, -42, 8, -26, -26, -2, -24, -33, -10, -17, -36, -25, -52, -29, -29, 127, 12, -86, 20, -83, -49, -127, -38, 28, 51, 10, 24, -15, -113, 123, 51, 71, -12, -62, -24, -60, 28, 39, -21, -32, -86, 19, 72, -44, -52, -65, 16, -103, 121, 4, -8, 30, -5, -30, 90, -39, -127, 102, -50, -9, -84, -114, -20, -71, -49, -38, 14, -27, 21, -4, 43, 2, -35, -41, -70, 30, -76, -127, 105, 18, -12, 53, -21, -75, 35, -68, -84, 21, -23, -69, -57, -76, 58, 10, 25, 45, 74, -28, 10, -50, -1, -117, 5, -96, 54, 24, -44, 40, -41, 60, 20, -12, -12, 127, -32, -75, -120, -38, -30, 73, -4, 86, -15, 10, 31, -12, -5, -114, -127, -89, 34, -9, -24, -103, 26, -6, -7, -32, 10, -59, -10, 41, -21, 17, -32, -57, 1, 30, -11, 70, 54, 84, 16, 45, 57, -19, -64, 11, 1, 7, 2, -22, 7, -35, 41, 14, -31, -20, -4, -47, 26, 14, 23, -76, -18, 2, -23, -8, 19, 127, -35, -28, 6, -48, 114, -24, -60, 29, -113, 8, 52, -67, 28, 3, 9, 33, -27, -77, 3, 93, -27, -17, -127, 29, 90, -111, 79, -2, -63, -14, 29, -43, -20, -13, -3, -14, -22, 8, -22, -28, 23, -22, -15, -17, -15, 4, 127, -28, -11, -8, -27, -29, 5, -10, -25, 13, 0, -20, 14, 6, -22, -4, -86, 39, 109, -37, 16, 112, 45, 26, 51, -10, -83, 36, -35, -24, 8, -122, 127, -4, 11, -38, -75, -21, 42, -21, 13, 5, -23, -84, 10, -15, -65, 25, 127, -57, 119, 15, -63, -76, 10, -50, 18, 3, -12, -68, -29, -17, 67, 64, -25, -73, -47, -2, 50, -76, -50, 3, 50, -28, -28, -12, -30, 127, 2, 5, 13, -3, -64, -10, -31, 43, -21, -80, -13, -42, 31, 24, 8, -5, 8, 5, -7, -11, -53, -5, -41, -66, 17, 39, 70, 78, -27, 49, 95, -54, -90, -51, -26, -24, -52, -69, 0, -91, 53, 127, -57, -54, -55, 4, 5, 2, 45, -11, -84, 127, -20, 50, -91, 43, 21, -114, 10, 10, 47, -2, 29, 3, -25, -16, -36, -124, -30, -11, -31, 5, 3, -9, 23, 38, -127, -44, 39, -102, -15, 75, -39, 24, 14, 20, 29, 0, -38, 118, 13, 22, 70, -37, -73, -88, -29, -41, -34, -6, -6, -46, -3, -22, -63, -105, -8, 67, -39, -14, 127, -100, -8, 70, -97, 3, -78, -34, 31, -50, -44, 12, -57, 77, -84, -4, 12, -11, -120, -41, -21, -108, -40, -17, 4, -2, 2, -5, 2, 12, -19, 26, -6, -10, -41, 3, 1, -31, -26, -32, -55, -8, -17, 127, -17, 6, 36, 8, 16, 9, -61, -66, 77, -14, 126, 21, 2, 17, 12, -29, 100, -29, -127, 80, -71, 41, -14, -30, 16, -15, 21, 76, 36, 102, 92, -21, 52, -14, -31, -43, 127, -69, -51, -53, -64, 92, 64, 44, -91, 14, -40, 74, -17, 30, 24, -56, -38, -72, -2, 46, 69, -110, -49, -15, 56, 18, -82, -5, 48, 14, -8, -20, 21, -2, -31, 7, -71, -96, 9, -37, 53, -2, 88, 36, 101, -35, -64, 8, 81, -127, -29, -47, -41, -40, -73, -53, 10, -48, 10, -16, 6, 6, 127, 20, 10, -11, 11, -24, 16, -35, 3, -80, -43, -23, -68, -14, -52, 34, 36, -38, 71, -60, 120, -8, -93, -58, -46, -60, -9, -127, 20, -13, -30, -52, 30, -53, -9, 35, -55, 38, -52, 15, 21, 41, -27, -62, 10, -58, -26, 53, -17, -98, 81, -67, -16, 62, -46, 19, 77, 14, 107, 1, -101, 100, 85, 16, -39, -127, -68, -1, -6, 63, 32, 31, -2, -17, 39, -4, -15, -40, -14, 66, -63, 13, 44, -53, 10, 88, 3, -127, -28, -73, 4, 12, -30, -69, -7, -8, 29, -41, 29, 62, -36, -8, 90, 54, -111, -60, 69, 2, 33, 28, -69, 127, -34, -52, -21, 32, -55, -7, -36, 6, 55, -84, 5, -24, -46, -120, 8, 101, -48, -31, 15, -54, -49, -30, 12, 5, -122, -26, -7, 127, 72, 14, 106, 21, -84, 61, -13, -27, -38, -61, -31, -40, -4, -32, 0, -40, 53, -76, 9, -18, -73, -36, 79, -11, -8, 42, -28, 7, -49, 66, -21, -127, -83, 68, -5, 37, -28, 49, 10, 44, -11, 28, -43, -88, -5, -27, -17, -43, -51, -25, -62, -87, -6, -6, -23, -18, 22, 55, -127, 25, -107, 34, 2, 12, -17, 69, -81, -7, -12, 21, -20, -39, -74, 20, -12, -101, 19, -24, -11, -26, -35, -58, -5, -70, 11, 5, 32, 22, -13, -19, -9, -2, -81, -22, -59, -109, -25, -5, 127, -9, 6, -34, -50, -39, -40, 9, -23, -18, -127, 8, -4, 102, -28, 50, 9, -40, -84, 2, -26, 40, -37, -9, -15, -45, 82, -19, -47, 22, 30, 30, -40, -29, -22, 24, -50, -43, 10, 80, 77, 31, -40, 39, -75, 11, 5, -46, -13, -3, -127, 15, 61, 66, 29, 29, -113, -7, -32, -1, 41, -39, -30, 13, -2, -29, -15, 4, 57, -11, -90, -127, -11, 1, -88, 109, -6, 9, -72, 22, 53, -19, -18, -37, -26, -27, -17, -39, 18, 43, -28, -48, -126, 113, -28, -69, -55, -54, -55, -50, -21, -61, -80, -49, -48, 38, -78, -39, 2, -45, -53, 25, 7, -82, -127, 23, -25, -3, 14, 7, -11, -17, -127, 14, -22, -43, 20, 34, 23, -14, 14, -13, 21, 12, 32, -42, 21, -30, 103, 34, 14, -46, 1, -9, 24, -33, 51, -56, -60, 17, -25, -39, -32, -19, 4, 9, 56, -44, -122, -21, 87, 5, -69, -91, -85, 6, -72, -11, 127, -5, -68, -36, -22, -24, 93, 12, -48, 37, -57, 23, -52, -7, -59, 37, 8, 7, 94, 14, -2, 21, -41, -12, -2, 1, 66, -99, -83, -127, -37, 5, 12, -3, -63, -2, -75, -51, -14, 2, -72, -48, -34, 7, -59, 2, -38, -53, -62, -9, 33, -104, -2, -59, -29, -56, -55, -127, 13, -14, -55, -35, 39, -1, 18, -15, -28, -27, 9, -27, 7, -79, 58, -80, -9, 1, 16, -2, -36, -8, 84, -37, 46, -13, 13, -61, -127, -59, 30, 11, -11, 69, 17, -3, 19, -68, -3, -57, -75, -81, -18, -14, -19, 0, 17, -1, 0, 18, 29, 19, 6, 127, -4, 2, -2, 36, 19, -19, -12, -63, -14, 59, -67, -77, 52, 127, 45, -61, -20, -101, -102, -16, 54, -54, 114, 71, -91, 93, 16, -42, -42, 42, 63, -7, 17, -117, -127, -41, 60, -24, -4, -13, -28, -5, 26, -5, 8, 17, -72, 36, -10, 27, 33, -43, 20, -59, 35, -38, 56, -37, 59, 4, 33, -35, 64, 48, 11, 15, 94, -68, -39, -12, 77, 127, 58, -52, -44, 19, 15, 106, -54, -5, -3, -53, 31, 100, 5, 55, -62, 73, 118, 16, -37, -59, -35, 8, -4, 73, 2, -39, -15, 39, 29, -11, -28, -7, 43, -32, -41, 3, 27, 66, 69, 93, -9, 15, -24, -127, 111, -112, 31, -127, 1, -15, 42, -108, -6, -34, -56, -52, 43, -124, -65, 7, -103, 117, -87, 37, 105, -64, -76, 3, -117, 56, 8, 32, -12, -15, -19, -66, 78, -44, 67, -51, -67, 29, 7, -23, -1, -79, 56, 39, -44, 53, -9, -5, -58, -127, 21, 19, -68, 3, 49, 3, -49, -23, -50, 28, -93, -118, -55, -125, -16, -70, 55, -37, -115, 54, -48, 27, 95, -115, 33, -127, -23, -75, 0, -75, -13, 36, -40, -64, 47, -58, 63, 4, -47, -3, -23, 18, 9, -26, 26, 8, 37, -63, 7, -10, 1, 43, -72, 71, -32, 38, 2, -127, -43, -50, -11, -9, -7, 7, -21, -127, -44, 6, 8, -33, -7, 48, 0, 5, -46, -47, -30, 8, 36, 21, -30, -59, 20, -29, -29, 13, -49, 25, 47, -80, -66, -20, 11, 0, -25, 127, -14, -25, 11, -14, 5, -21, -2, -9, -32, 8, 21, -10, -15, -16, -7, -19, -14, -14, -19, -20, -73, -44, -12, 13, -12, -67, 3, 55, 93, -83, 18, -127, 70, 11, 4, -35, 90, -36, 26, -21, -11, 20, -90, 13, -66, 3, -7, 22, -27, -30, -7, -40, 69, -2, -88, 24, 99, 17, -24, -69, -18, 46, -64, -127, -89, -41, 0, -31, 24, 9, -56, 66, -24, 71, 31, -14, -22, -26, 6, -47, -17, -95, 17, -67, -4, -127, -53, -15, -12, 9, 34, 12, -73, 103, 18, 30, 30, -66, 25, -104, 17, 48, -8, -10, -99, 6, 3, -65, 41, 23, -22, -85, 9, 18, -127, 8, 62, -4, 35, -24, -56, 24, 17, 22, 74, 36, -45, -67, 26, 23, 17, -39, -48, 18, 112, 8, 14, 58, -19, -7, 8, -27, -19, -30, -27, 38, -4, -89, 16, -79, 15, 29, -2, 0, -35, -92, -54, -34, -16, 45, 87, 105, -127, -63, -15, -32, 49, -7, 19, 116, -96, 38, -4, 127, 80, 13, -29, 68, 6, 53, 18, -30, -19, -52, 53, 47, -50, -88, -41, 16, -40, -90, -7, -20, -9, -13, 31, 0, -29, 9, 12, 12, 17, -15, -17, 1, 0, 10, -26, 8, -9, 1, -4, -127, -9, -17, -15, -2, -2, -11, -52, -1, 8, 1, -21, -44, -45, -7, 11, -60, -2, 4, 14, 82, -36, -7, -127, 10, -36, -46, -20, -7, -127, -9, 44, 16, -50, 27, 127, 92, 26, -30, 87, -71, -30, 41, -14, 113, -33, -88, -89, -28, -24, -1, -8, -77, 1, 40, -45, -49, 74, 78, 87, 0, 39, 62, 35, -56, -44, -17, -22, 16, -31, -20, -5, 43, -22, 5, 38, -1, -127, 6, -8, 22, 8, -13, 2, 9, -39, -32, -5, -17, 30, 20, -75, -17, 45, -30, -69, 14, -5, 127, 29, 41, -8, -12, -50, -23, -15, 71, 18, -59, -44, -42, 1, -21, 20, 16, -14, -41, 15, -27, -9, -20, -59, -24, -38, 38, -19, -12, 40, -89, 5, -12, -105, -87, 127, -3, -40, -39, -27, -32, -49, -22, 76, -49, -37, 26, -22, -78, -10, -51, 4, -26, 8, -14, 11, 23, -34, -9, 14, 1, -18, -15, 127, 15, -23, 3, -21, -26, -43, 3, 20, -14, -34, -22, 9, -63, -115, -12, -30, 5, 44, -5, -7, 127, 57, -48, -18, -63, -54, -12, 21, -9, 1, 23, -72, 13, 16, 28, -56, -49, 22, 24, 9, 12, -127, -53, -48, -38, -1, 91, -34, -9, -4, -7, -47, 13, -33, 33, -44, -7, -30, 74, 11, -115, -2, 6, 10, -76, 18, -24, -12, -53, 8, 5, 3, -95, -24, -84, -50, -10, 40, -10, -13, 53, 115, -26, -12, -23, -127, -49, 17, -126, -4, -14, -48, 24, 109, 1, -26, 28, 24, -26, 49, -18, -71, -5, -28, 23, -36, 0, -2, 4, -95, 12, -17, 47, 9, -18, 4, -43, -42, 127, 23, -36, 5, 22, -35, 33, -12, -14, 1, -19, -49, 30, 9, 51, -5, 50, -3, -18, 4, -14, -7, 63, -43, 21, -47, -13, 59, -127, 0, 13, 3, 9, 36, -11, 64, 3, 58, 48, -62, -112, 3, 57, 21, -12, 55, 7, 30, -59, 17, -26, -96, -33, -127, -76, -124, 27, -26, -2, 65, -4, 50, 66, 21, 10, -42, 26, 50, 2, -60, -2, -44, 127, 21, -20, -43, -33, -6, 5, -6, -63, 5, -17, 25, -39, 11, -40, 0, -12, 51, 60, -53, -21, 39, 30, 127, -19, 20, 10, -58, 91, 83, -35, -87, 43, -70, -56, 54, 16, 19, -14, 32, 48, 43, -64, 6, 4, 15, 6, -77, -52, 7, -47, -75, -51, -4, -50, -48, 127, 75, -12, 21, 54, -4, 112, 3, -67, -80, -12, 41, 7, -46, -58, 69, -111, -79, -71, -103, 9, 73, -5, 55, -33, 73, 127, -7, 32, -88, 24, 14, 101, -49, 5, 21, -53, 24, -126, -39, -80, -12, 60, 8, 0, 22, 2, -41, -15, -5, -33, -18, -19, 10, 12, -19, -28, -33, -81, -65, -16, -30, -33, -9, -11, -20, -10, 10, -2, 8, 1, 127, -17, 7, -1, -37, -26, 6, -20, 127, -19, -58, 25, 49, -4, 95, -20, -104, 43, -10, -21, -50, -51, -4, 12, -5, -20, -1, -4, 12, 6, 54, -66, 17, -15, -2, 9, 40, -47, 46, 15, 51, -30, -53, -60, -38, 20, 24, 127, 8, -32, 29, -39, -72, 30, 16, 5, 5, -51, 38, 27, 38, -15, -12, -43, 96, 32, -64, -33, 23, -42, 127, 7, -38, -35, -4, -64, 36, 32, 38, -21, -65, -43, 33, 11, 24, -33, -69, 9, -28, -90, -43, -57, 13, -53, 32, 26, -34, -59, 31, -127, 49, 18, 42, 35, 35, -1, -27, -11, 8, 1, 65, -32, 40, 20, -1, -79, -42, -32, -7, 5, 18, 12, 52, 58, 35, 0, 82, 59, -127, -28, 12, 19, -19, -62, -44, -38, 21, 88, 18, 3, 44, 4, 35, -27, -124, -41, 37, -12, 120, -8, 92, -61, 40, 14, 39, -34, -50, -29, 10, -59, 46, -81, -6, 18, 26, 127, 23, -81, -34, 102, 93, -22, 36, -91, 21, 9, -46, 12, -49, -127, -36, -60, 34, -24, -98, -97, 14, -16, 51, -63, 111, 41, 95, -39, -120, -46, -80, -26, 28, 40, -65, -20, -20, 41, 22, -1, -39, 51, -53, -17, -34, -92, -31, -9, 76, -35, -59, 28, -1, 49, -20, -9, -86, -127, 16, -57, 8, 74, 38, 44, 13, -48, -14, 7, 17, 29, 85, 24, -57, 10, 2, 14, -69, -2, 17, -11, -8, -127, -34, -25, 23, -104, -37, -41, -82, 29, 61, 8, 53, -12, -12, 0, 45, -63, 28, 94, 0, -127, 24, 3, -13, 123, -123, -69, -50, 6, -7, 103, 39, 15, 91, 8, 41, -7, -55, -27, -4, 11, -14, -29, 15, -20, 14, -9, -21, -53, 86, -19, -14, -52, -36, -116, -40, 54, -59, -30, 0, 127, -19, -27, 6, -23, -127, -27, 76, -43, -38, -41, -43, 116, 23, -4, -10, -29, -88, -6, -9, 18, 52, -100, -87, -21, -10, 8, 9, 77, -35, -25, 4, -38, -42, -1, -94, -12, -25, 83, 16, -127, -8, -22, 24, -99, -12, -5, -59, 51, -126, -73, 13, 83, -5, -28, -26, -4, -60, 1, -25, 28, -46, -24, 67, -53, 25, 0, -34, -32, 55, 55, 95, -32, 34, -24, 40, 21, -5, -42, 20, -17, -24, 20, 59, 26, 20, -16, 45, -127, 59, -16, 9, -39, 47, -124, -21, -67, -38, 27, 34, -127, -7, -38, -19, 45, -9, -45, -28, -104, -63, -4, 25, 2, 11, 18, 15, -13, -24, -39, 23, -24, 127, 0, -12, 46, 24, -19, 83, -23, -70, 48, -41, -111, 10, -23, -21, -62, -76, -30, 0, -17, -7, -11, -6, -3, -46, -59, 16, -61, -19, -106, 25, -31, 10, 45, 25, -18, -79, 127, 15, 48, -4, -29, 40, -49, 45, 37, -18, -39, -46, -20, 0, -79, 84, -43, -97, -15, 31, 68, -15, -115, 62, 52, -43, 3, 46, 23, 31, -76, -68, -58, 39, 86, -71, 127, 14, -38, 41, 76, 35, -67, -33, 29, -25, 127, 29, -54, 39, -53, 21, -10, 85, 33, -31, 40, -38, 46, 27, -96, -14, -75, 34, 28, 73, -2, -39, -23, 63, -47, -38, -15, 22, 21, -33, 5, 34, -103, -3, 5, -2, 15, 48, 61, 1, 6, 19, -127, -6, -8, -22, -42, -15, 44, 65, -36, 2, -52, 36, 10, 4, -15, -2, 1, -18, 2, -58, 38, 17, -126, 93, -31, 10, -16, 10, 11, 41, 59, 4, -21, -11, 127, -76, 0, -48, 75, -52, -18, -87, 13, -6, -64, -127, 14, -56, -57, 19, -35, 55, 15, -52, 22, -9, -48, -68, -20, 0, -13, 14, -20, 80, 13, 19, 25, 1, -15, 30, -2, -39, -77, 29, -13, -45, 42, 0, -73, -24, 29, -80, -44, 65, -127, -42, -19, 22, -27, -77, 107, -5, -33, -60, -40, 4, -14, -3, -11, -16, 8, -10, 22, 14, 18, 8, -21, -20, -14, -4, 32, -26, 5, -19, 15, 17, -127, -10, -21, -10, 2, -2, 0, -1, -42, 28, -44, 15, -26, -23, -51, -32, 73, 70, -63, 11, 7, 31, 41, 36, -54, 60, -13, -10, 17, -39, -32, -2, -127, 3, -18, -12, -23, 3, -17, 52, 15, -41, -17, 29, -20, -7, -29, -37, 13, 10, 20, 10, -66, 1, 6, 24, -127, -34, -48, 30, -1, 24, -46, -127, -35, 63, -33, 20, -31, -31, 29, 51, 41, -40, 41, -126, -12, 16, -40, 55, -93, -29, -52, 28, -8, 55, 55, -76, -30, 60, -56, -13, 15, -45, 9, -14, -20, 8, 3, 5, -4, 14, -23, -15, 7, 0, -22, -55, -49, -3, 3, -17, -4, -9, 127, 0, -3, -14, -11, -1, -26, -49, -54, -56, 84, 48, -76, -19, -16, -23, -104, -5, -95, 3, 2, -127, -15, 17, -7, -43, -99, -28, -95, -42, -31, 5, -9, 127, -3, -85, -1, -30, 34, 34, -91, -36, -4, 29, -19, 19, -60, -10, -14, -15, 16, 18, -1, 26, 33, -37, -93, 1, -24, -5, 16, -127, -21, 59, -22, 68, -29, -48, 24, -10, -32, -21, -46, -59, 40, -35, -10, -38, 88, -42, -5, -49, 27, -29, 1, -28, 54, -25, 4, 8, 22, -127, 88, -46, -85, -20, -94, 108, -43, 50, -21, -72, 0, 57, -16, -114, -37, -55, -19, -29, -6, 123, 10, -39, -35, -7, -28, -19, -18, -83, -47, -64, -41, -8, -65, -9, -127, -19, -22, -68, -97, -37, -56, -97, -35, -13, -38, 52, -37, -40, -51, 80, -59, -55, -16, -44, -15, 120, 48, 116, -50, -16, 16, 67, 102, 59, -17, -112, 40, 21, -38, 42, 89, -39, -5, -1, -109, 127, -19, -58, 44, 69, 30, -38, -34, -46, 0, 22, -8, 83, -7, -42, -65, 48, 59, -53, -101, -14, -8, -21, -61, -104, -85, -108, -24, -117, 56, 127, -49, -16, -51, -9, -10, 17, -7, -6, -8, -19, -23, -21, -44, -1, 5, -9, 18, -13, -30, 12, -37, -15, -32, -17, 127, 1, 0, -11, -3, -11, -4, 17, -49, -5, -41, 28, 14, -9, 12, 0, -26, 127, -9, -37, 23, 9, 56, 48, -4, -9, 25, -24, -61, 44, -36, 8, -41, -22, -54, 67, -25, 26, -26, -7, -102, -23, -86, -13, 6, 114, 35, -73, 24, -12, -3, -13, -75, -16, -127, -64, -45, 65, 19, 117, -1, 36, -12, 8, 20, -76, 38, 14, 20, 8, -87, 63, 68, 20, -41, -8, 37, 65, 16, 84, 42, -108, 76, -24, -35, 127, -76, -61, 9, 34, -64, -127, -21, -43, -1, -47, -22, -52, -64, -8, 30, -61, 52, 13, 22, -14, -18, 21, 15, -63, 67, -53, -8, 6, 35, 61, -27, -43, -47, 88, -40, -17, -68, -61, 31, 24, -51, 21, 31, 31, -50, -103, -48, 27, -12, -57, 127, -84, -61, 0, -3, -20, -42, -81, -42, 38, 56, -77, -84, -9, 12, -127, -63, -26, 56, 54, -43, 7, 34, 19, 34, -40, 35, -41, -24, 122, 36, 64, -5, -18, 17, 14, -64, -61, 17, 15, 46, 8, 127, -10, -84, -16, -53, -5, 76, -52, 7, 86, 24, 63, -44, 40, 79, 38, 73, 7, 56, 28, 4, 62, -5, 57, 98, -55, -31, 30, 32, -34, -59, -19, 26, -17, 1, -25, 7, -56, 22, 9, -19, 13, -22, -2, -38, 25, 127, 19, -8, 14, 17, -16, 16, -63, -2, -27, 9, 6, 14, -127, -34, -20, -15, -72, 42, -13, 8, -60, 57, -44, -52, -8, -12, -64, 38, -100, -16, 16, -15, 32, -43, -39, -15, -108, 27, -12, -67, -50, -33, -27, 9, -52, -64, -5, -46, -55, -49, -70, -19, -89, -100, -33, -9, 127, -25, 58, -5, -12, -19, -90, -35, 30, -52, 15, 15, -93, 71, -57, -31, -18, -18, -127, -15, -37, 24, -36, -47, -17, -37, -7, -35, -38, 115, -71, 65, -7, -27, 3, 8, -58, 1, 4, -39, 23, -32, 38, 24, -5, -12, 118, -55, -35, -51, 30, -127, -43, 13, 1, -18, 13, 14, 25, -44, -24, -11, -47, 5, -81, -32, -22, -28, -12, 18, 30, 5, 16, -43, 104, -9, 13, 8, -19, -127, -21, -11, 41, 16, -30, 45, 11, -60, 22, -31, -6, -3, 12, 26, -54, -25, 8, -7, -24, -17, -21, 1, -6, -23, -2, -17, 6, -29, 0, 2, 3, 127, -4, 14, 4, 6, -16, 0, -22, -33, -2, -55, -81, 66, -26, 45, 84, -105, 83, 91, -40, 21, 30, 76, -16, -123, -24, 22, 56, -70, 60, -35, -19, -127, -30, 5, 19, -48, -8, -66, -4, -29, -5, 17, -70, -13, -13, -51, 4, -28, 21, -9, -56, -76, -8, -127, -35, -17, 30, -15, -33, 22, 121, -101, 86, -45, 0, 9, 23, -37, -76, -48, -42, 51, 31, 2, 37, -33, -107, -38, -63, -22, -66, -32, -16, 62, 18, -99, 30, 1, -127, -15, -127, 123, -44, 10, 40, -33, 2, -66, 1, -46, -15, 53, 68, 26, -28, 52, 43, -39, 61, 18, -57, 1, 1, 101, 6, 16, 42, 16, 1, 4, -5, -6, 6, 3, 23, 48, 19, -70, 27, -14, -8, -77, -19, -20, -13, 9, 74, 42, -9, 0, -10, -12, -127, 16, -10, 45, -21, -1, -8, -4, 12, -3, 4, -4, -5, -17, 7, -17, -13, -13, -13, -29, -18, -40, -10, 28, -6, -4, 6, 127, 10, -8, -10, -12, -56, -36, 23, -30, -63, 52, -63, 73, 39, -27, 33, -19, 2, -50, 22, 1, -127, 15, -6, -3, 52, -3, 17, 26, -57, -14, -16, -26, 100, 29, -47, 7, -112, 127, -45, -16, 27, 10, -28, 10, 24, -35, 5, 11, -37, -31, -48, -62, -87, 3, -2, -71, 23, 8, 64, 24, 11, 9, 74, -18, 26, -110, -26, 88, 26, -127, 80, -89, -91, 78, 90, 80, 14, -71, -6, -121, -71, -47, 42, -20, -8, -104, -91, -92, -1, 2, -28, -66, -127, -4, -34, 92, 50, 55, 25, -23, -56, -49, 32, 26, -41, 13, -26, 19, 30, -48, 17, -56, -38, -30, 39, 36};

float bias_raw[672]={-0.009470412507653236, 0.0057729571126401424, 0.03479359298944473, -0.03011322021484375, -0.04944207891821861, 0.014579814858734608, -0.005586948245763779, 0.03375738486647606, -0.030970077961683273, -0.006432623602449894, -0.01157637033611536, 0.004199327435344458, -0.12164205312728882, -0.04384396970272064, -0.02688612788915634, -0.007406949531286955, 0.01143938023597002, 0.0009973590495064855, -0.024618323892354965, -0.018276862800121307, -0.061610832810401917, -0.031821444630622864, -0.04023102670907974, 0.0061081149615347385, -0.060161516070365906, 0.04433688893914223, -0.05153042823076248, -0.007068544626235962, 0.029589863494038582, 0.013151122257113457, -0.017759600654244423, 0.008731091395020485, -0.047306984663009644, -0.049715060740709305, -0.004866450559347868, 0.0005811494193039834, 0.001422422705218196, -0.04489366337656975, -0.01071756798774004, -0.014125990681350231, 0.006440985016524792, -0.026861784979701042, -0.012340663000941277, -0.03738187998533249, -0.008058280684053898, -0.03243470937013626, -0.028934240341186523, 0.019069189205765724, -0.00828544981777668, 0.0022711651399731636, -0.006909550633281469, 0.05339296534657478, -0.019909299910068512, -0.04431702569127083, -0.03429807350039482, -0.031008528545498848, -0.05214102938771248, 0.01050137635320425, -0.0052625383250415325, -0.018055729568004608, -0.008946256712079048, -0.0341293029487133, -0.03410648554563522, -0.07912337779998779, -0.06115429103374481, 0.03115832433104515, -0.003163352143019438, 0.017067020758986473, -0.003622132819145918, -0.024719515815377235, -0.003402282251045108, -0.0249306783080101, -0.0024337766226381063, -0.0006018357817083597, -0.003467510687187314, 0.00426335446536541, 0.026767725124955177, 0.0021825393196195364, 0.0012148519745096564, 0.009262323379516602, -0.012921781279146671, 0.002197708934545517, -0.09035184234380722, -0.018415028229355812, -0.008145329542458057, 0.0460994727909565, 0.006177876610308886, -0.0012478140415623784, 0.014020129106938839, -0.026112966239452362, -0.013891144655644894, 0.004579449072480202, -0.08074149489402771, -0.009384125471115112, 0.0167675893753767, 0.008719313889741898, 0.013923272490501404, -0.05659268796443939, -0.021301351487636566, -0.027053620666265488, 0.004048474133014679, 0.006841219495981932, 0.03468978777527809, 0.01231263019144535, -0.007076677866280079, -0.0471685416996479, 0.03467458486557007, 0.01862560398876667, 0.03274992108345032, -0.006280867382884026, -0.0435468927025795, -0.0276056956499815, -0.02964160591363907, -0.006377228070050478, 0.031144125387072563, 0.014259672723710537, 0.009382970631122589, -0.011071962304413319, -0.035235900431871414, -0.0023707589134573936, 0.00044168709428049624, 0.003850687528029084, 0.021216290071606636, -0.019228549674153328, -0.038382481783628464, 0.016916688531637192, 0.008662432432174683, 0.0008056684164330363, 0.0026570039335638285, -0.03987744078040123, 0.0122798727825284, 0.028137164190411568, -0.02163948118686676, -0.011660914868116379, -0.011859072372317314, -0.02532741241157055, -0.024602079764008522, 0.0020178500562906265, -0.042968813329935074, -0.009660907089710236, 0.02410319820046425, -0.04952538013458252, 0.026024436578154564, 0.014459941536188126, -0.003419021377339959, -0.028135286644101143, -0.020764535292983055, 0.019643018022179604, -0.01400283444672823, 0.012944171205163002, -0.005252974573522806, -0.018865471705794334, 0.02153165638446808, -0.03158088028430939, -0.048905521631240845, -0.06603190302848816, -0.006517827045172453, -0.017955051735043526, -0.004615431651473045, -0.01420511119067669, 0.006876454222947359, -0.014301714487373829, -0.018909651786088943, -0.029273219406604767, 0.0028772566001862288, -0.014200553297996521, 0.02667882852256298, -0.006001695059239864, -0.0063791279681026936, 0.02809954807162285, -0.025059429928660393, -0.030108384788036346, -0.003950555808842182, 0.0037293280474841595, -0.029485922306776047, -0.0011646137572824955, -0.05570658668875694, -0.023104777559638023, 0.007937592454254627, 0.04890863224864006, -0.06103769317269325, -0.022612180560827255, 0.0049827126786112785, -0.007930048741400242, 0.046545423567295074, 0.004565141629427671, -0.0076704081147909164, -0.004536893218755722, 0.004464226774871349, -0.024146856740117073, 0.0027991558890789747, -0.007303956430405378, -0.020974455401301384, 0.020027676597237587, -0.019409939646720886, 0.003307430073618889, -0.07909742742776871, -0.04089675098657608, -0.09673983603715897, 0.016482332721352577, -0.044273849576711655, -0.0253179632127285, 0.011019671335816383, -0.014669337309896946, 0.00632766867056489, -0.015687886625528336, -0.073147252202034, 0.056265488266944885, -0.024076756089925766, 0.03581400215625763, 0.016898056492209435, -0.027061397209763527, 0.005498512182384729, -0.04416612163186073, -0.023222768679261208, -0.010349349118769169, 0.026351764798164368, 0.022407397627830505, -0.02402966096997261, -0.0002163211756851524, 0.028977852314710617, -0.07532718777656555, -0.032356373965740204, -0.018229899927973747, 0.019366197288036346, -0.02046329900622368, -0.04366205260157585, 0.007161902729421854, 0.0010453801369294524, 0.017280394211411476, 0.005051441490650177, -0.03385195508599281, -0.013180815614759922, 0.020374448969960213, 0.002042858861386776, 0.006327498704195023, 0.017411870881915092, -0.003306732978671789, 0.017125165089964867, 0.013408771716058254, -0.001025883830152452, 0.012339437380433083, -4.869549593422562e-05, 0.03273417800664902, 0.016320666298270226, 0.0025163416285067797, -0.0441676490008831, 0.02458256110548973, -0.09841053187847137, -0.07776220887899399, 0.011321627534925938, -0.00546377943828702, -0.03485922887921333, -0.024869104847311974, -0.01712978072464466, -0.0830048993229866, 0.0430389828979969, -0.013471713289618492, -0.010210871696472168, 0.013464996591210365, 0.021975593641400337, 0.03117499127984047, -0.00889642909169197, 8.909267489798367e-05, -0.035308826714754105, -0.06771916896104813, -0.036771729588508606, -0.039421748369932175, -0.04729277268052101, -0.030441973358392715, -0.00741549301892519, -0.03945449739694595, 0.01380111277103424, -0.08182521164417267, -0.0009318871307186782, 0.004786384291946888, -0.014613634906709194, 0.019031189382076263, -0.013370520435273647, -0.0036892257630825043, 0.006687238346785307, -0.00629377318546176, 0.04994850233197212, -0.002650939393788576, -0.10863186419010162, -0.0187675878405571, -0.014908053912222385, 0.02838660404086113, -0.016865726560354233, -0.04748878255486488, -0.002849928103387356, -0.055419787764549255, -0.04238691180944443, -0.030671464279294014, 0.019119352102279663, -0.019174139946699142, -0.004990623332560062, 0.05382570996880531, 0.004233608487993479, 0.004513000603765249, -0.012811784632503986, -0.011373321525752544, -0.0034475529100745916, -0.04438736289739609, -0.003505104221403599, 0.019180018454790115, -0.012553876265883446, -0.02542513608932495, -0.04756084084510803, -0.037437934428453445, 0.007250440772622824, -0.012757321819663048, 0.031706783920526505, -0.04259420558810234, -0.00963767059147358, 0.006831277161836624, -0.0013012333074584603, -0.07087130099534988, -0.019412348046898842, -0.03081752546131611, -0.051983337849378586, -0.01703791320323944, -0.017143642529845238, 0.003852739231660962, -0.007008701097220182, -0.032520536333322525, 0.023974701762199402, -0.08439039438962936, 0.012592826969921589, -0.006192848086357117, -0.03689124062657356, 0.0014839150244370103, 0.020137988030910492, -0.01619007997214794, 0.013011336326599121, -0.05804948881268501, -0.033716168254613876, -0.002291952259838581, 0.013880858197808266, -0.09273947030305862, -0.11860876530408859, -0.0076240855269134045, -0.016137901693582535, -0.014791043475270271, -0.011084958910942078, 0.015326890163123608, -0.017389146611094475, 0.014601211063563824, 0.020982803776860237, -0.006517830304801464, -0.080889992415905, -0.02722257748246193, -0.033306751400232315, -0.032040055841207504, 0.03517456352710724, -0.041984133422374725, -0.02581368386745453, 0.014908860437572002, -0.021742714568972588, 0.04790420085191727, -0.017711618915200233, 0.007609518710523844, 0.007077173329889774, -0.05987546592950821, -0.03453601151704788, -0.011301188729703426, 0.009630859829485416, -0.024792660027742386, -0.02010928839445114, -0.027483461424708366, 0.0025329808704555035, -0.008076812140643597, -0.029884854331612587, 0.028856948018074036, -0.0024118993896991014, 0.00849833246320486, -0.056203726679086685, -0.006286724004894495, -0.05898081511259079, 0.025912422686815262, 0.00023714188137091696, 0.0008679027669131756, -0.027387727051973343, 0.009623568505048752, -0.008228749968111515, -0.019256794825196266, -0.006816658657044172, -0.07036934047937393, -0.022526808083057404, 0.02817864529788494, -0.0010425852378830314, 0.02791396714746952, -0.02765307016670704, 0.04470227658748627, -0.001253635622560978, -0.02907128445804119, -0.011707467958331108, -0.02728615142405033, -0.02765519544482231, 0.056780315935611725, -0.00796059425920248, -0.036867786198854446, -0.02566688321530819, -0.04399382695555687, -0.058531418442726135, 0.011716237291693687, 0.04143712669610977, 0.010529729537665844, -0.029921164736151695, -0.02862057276070118, 0.007809780538082123, -0.004364688415080309, -0.05706482380628586, -0.043647248297929764, -0.03198375925421715, -0.0182266253978014, 0.0012612695572897792, -0.005447331815958023, -0.011528397910296917, -0.04560595378279686, 0.003697018837556243, -0.03852556645870209, -0.055749863386154175, -0.0026273727416992188, -0.0009966437937691808, 0.031883131712675095, -0.021595586091279984, -0.019578861072659492, -0.013057364150881767, -0.06693541258573532, 0.004954240750521421, -0.04519571363925934, -0.02869602106511593, -0.01527123712003231, 0.008229716680943966, -0.03131852671504021, -0.034096796065568924, -0.023203356191515923, -0.01848122477531433, -0.08388736099004745, 0.025720108300447464, 0.002689565299078822, -0.017454441636800766, -0.05815238878130913, 0.011038444936275482, -0.00837252289056778, 0.002612412441521883, -0.10493709146976471, -0.05536643788218498, -0.008883190341293812, -0.02280549705028534, -0.01796889677643776, -0.00930706039071083, -0.005287300329655409, -0.03774836286902428, 0.013263076543807983, 0.010894177481532097, 0.011904232203960419, 0.004294008947908878, -0.036242347210645676, 0.009902996011078358, 0.027139941230416298, 0.02502329833805561, 0.005350183229893446, -0.06113710626959801, -0.061815325170755386, 0.012202991172671318, -0.005393522325903177, -0.054402898997068405, -0.02712642401456833, 0.009713935665786266, -0.1205393522977829, 0.01550293155014515, -0.001194532960653305, -0.0184270478785038, -0.015227017924189568, -0.0004931669682264328, 0.047669630497694016, -0.006513399071991444, -0.004493901506066322, -0.013950285501778126, 0.0070806145668029785, -0.03814069554209709, 0.01675611361861229, -0.011689909733831882, -0.06920675933361053, -0.028914907947182655, 0.01442430354654789, -0.024005308747291565, -0.00953391008079052, -0.069126658141613, -0.026951685547828674, -0.010235903784632683, -0.002667809370905161, -0.011430669575929642, -0.029880190268158913, -0.020734403282403946, -0.030447527766227722, -0.04622599110007286, -0.04374024644494057, 0.010487206280231476, -0.00046483997721225023, 0.012445145286619663, -0.058929357677698135, -0.0043165236711502075, 0.011724896728992462, -0.023201964795589447, -0.07194796204566956, -0.021702753379940987, 0.018952783197164536, -0.010490930639207363, -0.009525329805910587, 0.0038698017597198486, 0.02374759130179882, -0.013444569893181324, 0.03770079463720322, 0.003364092670381069, 0.010193419642746449, -0.02377653494477272, 0.002160079311579466, 0.03346067667007446, 0.02249715104699135, 0.002486439421772957, -0.015209866687655449, -0.024245556443929672, -0.0017628263449296355, -0.03313799202442169, 0.012761474587023258, 0.018083781003952026, -0.01870524138212204, 0.035670340061187744, -0.073641337454319, -0.07979641109704971, -0.014803052879869938, 0.027728252112865448, 0.02124938741326332, -0.01829793117940426, -0.012853854335844517, 0.007167438045144081, -0.04128068685531616, -0.014129476621747017, -0.007752028293907642, -0.027628418058156967, -0.027285009622573853, -0.01465847808867693, -0.0005240006721578538, -0.026841480284929276, -0.1287418156862259, 0.03190222010016441, 0.002359232632443309, -0.009181269444525242, -0.003942311741411686, -0.013794833794236183, -0.03809736296534538, -0.05576010420918465, -0.03449135273694992, 0.01431927178055048, 4.001761772087775e-05, 0.0033780315425246954, 0.010015505366027355, -0.02807936631143093, -0.012297673150897026, 0.005038992501795292, -0.0668317973613739, 0.03228729963302612, 0.02372400090098381, -0.06035761162638664, -0.04345161095261574, -0.030594872310757637, -0.010020106099545956, -0.022342706099152565, -0.000283840112388134, -0.0040901037864387035, -0.05461164191365242, 0.011633333750069141, -0.07720926403999329, -0.009169521741569042, -0.03197100758552551, -0.005026699975132942, 0.012823658995330334, 0.06541818380355835, -0.019040260463953018, 0.006780852563679218, -0.0013164227129891515, -0.029858246445655823, -0.027982808649539948, -0.040228407829999924, -0.02084697224199772, -0.01263925526291132, -0.036241885274648666, -0.013106451369822025, 0.02357455901801586, -0.001984397415071726, 0.0010748865315690637, -0.05345853790640831, -0.06855520606040955, 0.013654159381985664, 0.03562074899673462, -0.011780527420341969, -0.07117532938718796, -0.024131115525960922, 0.011442000046372414, -0.027201402932405472, -0.051890116184949875, 0.011797092854976654, -0.0009605164523236454, 0.014819366857409477, 0.0031825515907257795, 0.0005565455066971481, -0.053578127175569534, 0.06088937446475029, -0.08460449427366257, -0.0004591860924847424, 0.057703711092472076, -0.00540205230936408, -0.06390329450368881, -0.0035453522577881813, 0.013185492716729641, -0.043962277472019196, -0.0021185469813644886, -0.01696121320128441, 0.002472636755555868, -0.05766791105270386, -0.017284255474805832, -0.01945793256163597, -0.03589293360710144, 0.01774260587990284, -0.015309568494558334, -0.0257564689964056, -0.026344073936343193, -0.017350733280181885, -0.07116206735372543, 0.014115029945969582, -0.053439583629369736, -0.03087105043232441, -0.06472723931074142, -0.02379230596125126, -0.06348976492881775, -0.02729945443570614, -0.02669489197432995, -0.054131001234054565, -0.06081448867917061, -0.0159259382635355, -0.028054537251591682, -0.07829727232456207, 0.019413383677601814, -0.06933698058128357, -0.05011836811900139, 0.000212955754250288, -0.018961794674396515, -0.008997166529297829, -0.028293168172240257, -0.024400735273957253, -0.0295172818005085, 0.038635022938251495, -0.005893020890653133, 0.03910191357135773, -0.07527425140142441, -0.03960752487182617, -0.03000379353761673, -0.020668312907218933, -0.010203355923295021, 0.004704331513494253, -0.00016177768702618778, 0.010313084349036217, -0.005340413190424442, 0.016698608174920082, -0.002504024188965559, 0.0021915610413998365, -0.04028880596160889, -0.002919062739238143, 0.004785764496773481};

int8_t* filter_tensor_data=filter_raw;
float* bias_tensor_data=bias_raw;

bool has_conv_bias=true;
const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActNone;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={672,1,1,28};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={672};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteInt8;
const TfLiteType bias_type=kTfLiteFloat32;
const float scale_filter=0.0;
const int32_t zero_point_filter=0;
const float scale_bias=0.0;
const int32_t zero_point_bias=0;
// const float scales_filter=;
// const int32_t zero_points_filter=;
// const float scales_bias=;
// const int32_t zero_points_bias=;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void ExtractConvParams(TfLitePadding padding, int stride_width, int stride_height, 
                               int dilation_width_factor, int dilation_height_factor,
                               TfLiteFusedActivation activation,
                               TfLiteConvParams* data_params) {
  // TfLiteConvParams data_params;
  data_params->padding = padding;
  data_params->stride_width = stride_width;
  data_params->stride_height = stride_height;
  data_params->dilation_width_factor = dilation_width_factor;
  data_params->dilation_height_factor = dilation_height_factor;
  data_params->activation = activation;
  // return data_params;
}

void GetConvTensor(TfLiteType type, const char* name, TfLiteIntArray* tensor_dims_data, 
                       TfLiteQuantizationParams quant_params,
                       char* tensor_data, TfLiteAffineQuantization* quant_struct,
                       size_t bytes_size, TfLiteTensor* tensor) {
  tensor->type = type;
  tensor->name = name;
  tensor->dims = tensor_dims_data;
  tensor->params = quant_params;
  // tensor->data.raw = reinterpret_cast<char*>(tensor_data);
  tensor->data.raw = tensor_data;
  tensor->bytes = bytes_size;
  tensor->allocation_type = kTfLiteMemNone;
  // data_0.allocation = allocation;
  tensor->is_variable = false;
  if (type != kTfLiteFloat32) {
    tensor->quantization.type = kTfLiteAffineQuantization;
    tensor->quantization.params = quant_struct;
  } else {
    tensor->quantization.type = kTfLiteNoQuantization;
  }
  tensor->sparsity = nullptr;
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  const float* input_data = GetTensorData<float>(input);
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  const bool need_non_dilated_im2col =
      params->stride_width != 1 || params->stride_height != 1 ||
      filter->dims->data[2] != 1 || filter->dims->data[1] != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // std::cout << "codes runs here #-1" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #-2" << std::endl;
  bool has_bias = false;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  // const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));
  // TfLiteTensor* filter;
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;
  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data),
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  auto filter_input_channel = filter->dims->data[3];
  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;
  // std::cout << "codes runs here #-3" << std::endl;
  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }
  // std::cout << "codes runs here #-4" << std::endl;
  const TfLiteTensor* bias = nullptr;

  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.
  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }
  // std::cout << "codes runs here #-5" << std::endl;
  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8));

  if (is_hybrid && filter->type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  // std::cout << "codes runs here #-6" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  data->supports_multithreaded_kernel =
      (kernel_type == kMultithreadOptimized) &&
      (context->recommended_num_threads != 1) && !is_hybrid &&
      (params->dilation_width_factor == 1) &&
      (params->dilation_height_factor == 1) &&
      (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);

  int channels_in = filter->dims->data[3];
  int channels_out = filter->dims->data[0];
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  int filter_width = filter->dims->data[2];
  int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  // std::cout << "codes runs here #-7" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = params->padding;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      params->stride_height, params->stride_width,
      params->dilation_height_factor, params->dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);

  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #-8" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    // std::cout << "affine_quantization->scale->size: " << affine_quantization->scale->size << std::endl;
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
        context, input, filter, bias, output, params->activation,
        &data->output_multiplier, &data->output_shift,
        &data->output_activation_min, &data->output_activation_max,
        data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #-9" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter->type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }

  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }

  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #-10" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }

    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }

    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #-11" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = params->stride_height;
  op_params.stride_width = params->stride_width;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {
  // std::cout << "codes runs here #4" << std::endl;
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }
  // std::cout << "codes runs here #5" << std::endl;
  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }
  // std::cout << "codes runs here #6" << std::endl;
  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(params->padding);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), GetTensorShape(filter),
                          GetTensorData<float>(filter), GetTensorShape(bias),
                          GetTensorData<float>(bias), GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // std::cout << "codes runs here #7" << std::endl;
      const float* filter_data;
      if (data->need_hwcn_weights) {
        filter_data = GetTensorData<float>(hwcn_weights);
      } else {
        filter_data = GetTensorData<float>(filter);
      }
      // int index;
      // for (index = 0; index < 432; index++){
      //   // std::cout << "filter_data[" << index << "] = " << filter_data[index] << std::endl;
      //   std::cout << filter_data[index] << ", ";
      // }
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          GetTensorShape(filter), filter_data, GetTensorShape(bias),
          GetTensorData<float>(bias), GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = params->dilation_width_factor;
  op_params.dilation_height_factor = params->dilation_height_factor;
  op_params.stride_width = params->stride_width;
  op_params.stride_height = params->stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(params->activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = params->stride_width;
      op_params.stride_height = params->stride_height;
      op_params.dilation_width_factor = params->dilation_width_factor;
      op_params.dilation_height_factor = params->dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #0" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams data_params;
  ExtractConvParams(paddings, stride_width, stride_height, dilation_width_factor, dilation_height_factor, activation, &data_params);
  TfLiteConvParams* params = &data_params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);
  // std::cout << "codes runs here #1" << std::endl;
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  TfLiteTensor filter_tensor;
  TfLiteIntArray* filter_dims_data = TfLiteIntArrayCreate(filter_dims_size);
  int size_filter = 1;
  for (int i = 0; i < filter_dims_size; i++) {
    // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
    filter_dims_data->data[i] = filter_dims_raw[i];
    size_filter *= filter_dims_raw[i];
  }
  size_t bytes_size_filter = sizeof(float) * size_filter;
  TfLiteQuantizationParams filter_params;
  filter_params.scale=scale_filter;
  filter_params.zero_point=zero_point_filter;

  TfLiteFloatArray* scale_array_filter = TfLiteFloatArrayCreate(1);
  scale_array_filter->data[0] = scale_filter;
  TfLiteIntArray* zero_point_array_filter = TfLiteIntArrayCreate(1);
  zero_point_array_filter->data[0] = zero_point_filter;

  TfLiteAffineQuantization quant_struct_filter;
  quant_struct_filter.scale = scale_array_filter;
  quant_struct_filter.zero_point = zero_point_array_filter;
  quant_struct_filter.quantized_dimension = 0;

  // float* filter_data;
  // filter_tensor_data = filter_raw;
  GetConvTensor(filter_type, "filter", filter_dims_data, filter_params,
                       reinterpret_cast<char*>(filter_tensor_data), 
                       &quant_struct_filter, bytes_size_filter, &filter_tensor);
  const TfLiteTensor* filter = &filter_tensor;

  TfLiteTensor bias_tensor;
  const TfLiteTensor* bias;
  if (has_conv_bias) {
    TfLiteIntArray* bias_dims_data = TfLiteIntArrayCreate(bias_dims_size);
    int size_bias = 1;
    for (int i = 0; i < bias_dims_size; i++) {
      // std::cout << "dims_raw: " << dims_raw[i] << std::endl;
      bias_dims_data->data[i] = bias_dims_raw[i];
      size_bias *= bias_dims_raw[i];
    }
    size_t bytes_size_bias = sizeof(float) * size_bias;
    TfLiteQuantizationParams bias_params;
    bias_params.scale=scale_bias;
    bias_params.zero_point=zero_point_bias;

    TfLiteFloatArray* scale_array_bias = TfLiteFloatArrayCreate(1);
    scale_array_bias->data[0] = scale_bias;
    TfLiteIntArray* zero_point_array_bias = TfLiteIntArrayCreate(1);
    zero_point_array_bias->data[0] = zero_point_bias;

    TfLiteAffineQuantization quant_struct_bias;
    quant_struct_bias.scale = scale_array_bias;
    quant_struct_bias.zero_point = zero_point_array_bias;
    quant_struct_bias.quantized_dimension = 0;
    
    // float* bias_data;
    // bias_tensor_data = bias_raw;
    GetConvTensor(bias_type, "bias", bias_dims_data, bias_params,
                        reinterpret_cast<char*>(bias_tensor_data), 
                        &quant_struct_bias, bytes_size_bias, &bias_tensor);
    bias = &bias_tensor;
  } else {
    bias = nullptr;
  }

  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;
  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  // std::cout << "codes runs here #10" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_gbdckb_REF() {
  static TfLiteRegistration r = {gbdckb::Init, gbdckb::Free,
                                 gbdckb::Prepare<gbdckb::kReference>,
                                 gbdckb::Eval<gbdckb::kReference>};
  return &r;
}

TfLiteRegistration* Register_gbdckb_GENERIC_OPT() {
  static TfLiteRegistration r = {gbdckb::Init, gbdckb::Free,
                                 gbdckb::Prepare<gbdckb::kGenericOptimized>,
                                 gbdckb::Eval<gbdckb::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_gbdckb_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {gbdckb::Init, gbdckb::Free,
                                 gbdckb::Prepare<gbdckb::kMultithreadOptimized>,
                                 gbdckb::Eval<gbdckb::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_gbdckb_CBLAS_OPT() {
//   static TfLiteRegistration r = {gbdckb::Init, gbdckb::Free,
//                                  gbdckb::Prepare<gbdckb::kCblasOptimized>,
//                                  gbdckb::Eval<gbdckb::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_gbdckb() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_gbdckb_MULTITHREADED_OPT();
#else
  return Register_gbdckb_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
